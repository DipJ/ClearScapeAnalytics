{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1378a69-ac58-4d0c-af22-7ef881abac45",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Anomaly Detection in Robot Welding Process</b>\n",
    "</header>\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Anomaly detection is a must-have when dealing with data, and sensor data are no exception. Despite well-known approaches — from engineering rules to graph and deep learning, it is still a challenge: it is hard to capture all the anomalies, minimize false positives, cope with the diversity of sensors and metrology issues, and deliver actionable insights at a business pace. Reactivity suffers from the volume, diversity, and complexity of time series. Sounds familiar? Let’s see how ClearScape Analytics from Teradata Vantage Cloud helps address these problems.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Spot Welding Quality Assessment</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Spot welding is a common technique used for welding car body panels, particularly in the assembly of smaller parts and components. Spot welding involves using a pair of copper electrodes to apply a series of short, high-current welding pulses to the metal, fusing the parts together at specific points or “spots”.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The automotive industry is known for its high level of automation, and spot welding is one of the most automated processes, heavily reliant on robots to improve efficiency, reduce labour costs, and improve the consistency and quality of the finished product. Poor welding quality is rare, but even so, the consequences of poor quality may not be negligible in terms of rework costs and customer satisfaction, especially when quality issues are detected too late.</p>\n",
    "\n",
    "<img  src=\"images/AnomalyWelding.png\"/>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>There are many ways to assess the quality of a spot, like tensile or ultrasonic testing to assess the weld strength or the analysis of the welding current measured and recorded during the welding process to name a few. In this demo, we focus on the analysis of the welding current, and more specifically on the resistance, i.e. the voltage-current ratio. The shape of the resistance curve depends on many factors, like a recipe, the nature of the materials, the geometry, and the quality of the electrodes, …</p>\n",
    "\n",
    "\n",
    "\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33aebf1-80cf-4043-99de-b2ac0356ea64",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>1. Start by connecting to the Vantage system.</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ba4a7-ae62-411d-82b7-381c1febe8d9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the section, we import the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a57efc-4277-4840-8b3f-eb1efdedd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# # '%%capture' suppresses the display of installation steps of the following packages\n",
    "!pip install tdsense\n",
    "!pip install imblearn\n",
    "# !pip install xgboost==1.7.3\n",
    "#!pip install colorlover\n",
    "#!pip install teradataml --upgrade teradataml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93c8af-e523-439c-bf7d-f0d867ad546d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>**RESTART kernel after installing tdsense</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The above statements may need to be uncommented if you run the notebooks on a platform other than ClearScape Analytics Experience that does not have the libraries installed.  If you uncomment those installs, be sure to restart the kernel after executing those lines. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b76c2-b211-452f-949c-676da6da9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from teradataml import *\n",
    "\n",
    "import tdsense\n",
    "from tdsense.plot import plotcurves\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from tdsense.clustering import hierarchy_dendrogram, hierarchy_clustering\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn2pmml.pipeline import PMMLPipeline\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, f1_score\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "\n",
    "import os\n",
    "from jdk4py import JAVA, JAVA_HOME, JAVA_VERSION\n",
    "# Set java path\n",
    "\n",
    "os.environ['PATH'] = os.environ['PATH'] + os.pathsep + str(JAVA_HOME)\n",
    "os.environ['PATH'] = os.environ['PATH'] + os.pathsep + str(JAVA)[:-5]\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from collections import defaultdict\n",
    "import plotly.offline as offline\n",
    "offline.init_notebook_mode()\n",
    "\n",
    "\n",
    "from teradataml.dataframe.sql_functions import case\n",
    "from teradataml import db_drop_table\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "display.max_rows = 5\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c250746-66ba-40aa-b41b-c791786f61a0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, then use down arrow to go to next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff461e-68e6-48b7-ab07-f03ae0765ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "eng.execute('''SET query_band='DEMO=AnomalyDetection.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476f53a-7115-4018-a58f-dd09f7fc8b88",
   "metadata": {},
   "source": [
    "<b style = 'font-size:20px;font-family:Arial;color:#E37C4D'>2. Getting Data for This Demo\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage.  You have the option of either running the demo using foreign tables to access the data without using any storage on your environment or downloading the data to local storage which may yield somewhat faster execution, but there could be considerations of available storage.  There are two statements in the following cell, and one is commented out.  You may switch which mode you choose by changing the comment string.</p>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940cdb5-c88e-425d-9f7a-aff8f6336c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_AnomolyDetection_cloud');\"\n",
    " # Takes about 50 seconds\n",
    "# %run -i ../run_procedure.py \"call get_data('DEMO_AnomolyDetection_local');\"\n",
    " # Takes about 3 minutes 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1653ae8-e6c1-4336-9260-b25024ce1c12",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9616e68-a4a7-42e1-999d-e3b56042a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f47fb4-6f61-4205-b563-2b08bf086cab",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>3. Analyze the raw data set</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a DataFrame to get the data from the table created.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99598e0a-8a6c-4539-a06d-f6723f67134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensor_Data = DataFrame(in_schema('DEMO_AnomolyDetection', 'Sensor_Data'))\n",
    "Sensor_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9b958-737d-41a0-adec-91614fa0fe2e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We create a view with the columns required to get data with proper column names.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cde234-6107-487e-92f2-7f045576cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "REPLACE VIEW DEMO_AnomolyDetection.V_dataset_01 AS\n",
    "SELECT\n",
    "    1 AS PLANT\n",
    ",   {41} AS ROBOT_ID\n",
    ",   CAST(A.PARTITION_ID AS BIGINT) AS WELDING_TYPE\n",
    ",   CAST((DATE '{str(datetime.now()).split(' ')[0]}'  + FLOOR((WELDING_ID-700*WELDING_TYPE)/100))  AS DATE FORMAT 'YYYY-MM-DD') AS WELDING_DAY\n",
    ",   CAST(A.ID AS BIGINT) AS WELDING_ID\n",
    ",   CAST(A.X AS INTEGER) AS TIME_MS\n",
    ",   A.Y AS RESISTANCE\n",
    "FROM DEMO_AnomolyDetection.Sensor_Data A\n",
    "\"\"\"\n",
    "eng.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3a959-c5e0-4039-88f8-846adca6f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_dataset_new = DataFrame(in_schema('DEMO_AnomolyDetection', 'V_dataset_01'))\n",
    "welding_dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09198aa2-6ab7-4339-a01a-365cba02c772",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.1 - Some aggregations and visualization. </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We check the total number of weldings done per day</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5ad77-6b48-4d40-a0aa-687678ba9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_dataset_new. \\\n",
    "    groupby('WELDING_DAY'). \\\n",
    "    count(distinct=True). \\\n",
    "    sort('WELDING_DAY'). \\\n",
    "    to_pandas(). \\\n",
    "    plot(x='WELDING_DAY',y='count_WELDING_ID', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b1b1a-eece-487a-97d7-b4759ea624ce",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We can observe in the above bar chart that the total number of weldings per day are same. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will check the histogram based on the minimum and maximum Time for welding.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>A histogram is a better way to assess distribution, to cope with the scalability, it is recommended to compute the histogram bins in-database to leverage the Massively Parallel Architecture of Teradata Vantage. For that, we use the Histogram function of teradataml that pushes down the computations to Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d38c3-ebb9-47a2-b8ad-f00acd9d769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_duration_ms = welding_dataset_new. \\\n",
    "                        groupby(['PLANT','ROBOT_ID','WELDING_TYPE', 'WELDING_ID']). \\\n",
    "                        agg({'TIME_MS':['min','max','count']})\n",
    "welding_duration_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bf739-a421-4ffd-8fc1-53f273db9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import Histogram\n",
    "obj = Histogram(data=welding_duration_ms,\n",
    "                    target_columns=\"count_TIME_MS\",\n",
    "                    method_type=\"Scott\")\n",
    "obj.result.sort('MinValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0e2a1-4371-4e2c-b4f0-6d06ec208b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = obj.result.sort('MinValue').to_pandas()\n",
    "res['duration_ms'] = [str(row['MinValue'])+'-'+str(row['MaxValue']) for i,row in res.iterrows()]\n",
    "res.plot(x='duration_ms',y='CountOfValues',kind='bar', figsize=(15,10), legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88429a10-aa8b-459f-976a-6276ab121bbc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the above histogram we can see the bins between the Min and the Max value of the durations and the welding counts.</p> \n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.2 - More advanced processing using window functions and delta_t </b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5615026-52eb-4aae-8bb2-146e88ef4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_dataset_new.loc[welding_dataset_new.WELDING_ID == 854]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c72091-f7f3-4ed3-a436-ee5c44335f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotcurves(welding_dataset_new.loc[welding_dataset_new.WELDING_ID == 854],field='RESISTANCE',row_axis='TIME_MS', series_id='WELDING_ID',select_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae924828-6e92-4003-93c9-b66aeec1821f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above graph shows the variation of the resistance of the welding with respect to time. We see that the most interesting part lies between 40 and 400ms from the start of the curve.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Next we apply the window function on the resistance to smooth the resistance and taking the mean value.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d8fd4-ab2c-44cd-89d2-d8075e40cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve smoothing\n",
    "window_for_smoothing = welding_dataset_new.RESISTANCE.window(\n",
    "                            partition_columns   = \"WELDING_ID\",\n",
    "                            order_columns       = 'TIME_MS',\n",
    "                            window_start_point  = -15,\n",
    "                            window_end_point    = 15\n",
    ")\n",
    "welding_dataset_smooth = welding_dataset_new.assign(RESISTANCE_SMOOTHED = window_for_smoothing.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ec8dd-a75d-4089-9c9c-85049d863476",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_curve = 854\n",
    "single_welding_pandas = welding_dataset_smooth[welding_dataset_smooth.WELDING_ID == id_curve].sort('TIME_MS').to_pandas().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,7))\n",
    "single_welding_pandas.plot(x='TIME_MS', y='RESISTANCE', ax=ax)\n",
    "single_welding_pandas.plot(x='TIME_MS', y='RESISTANCE_SMOOTHED', ax=ax,  xlabel='time (ms)', ylabel=r'resistance  $(m\\Omega)$', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bf795-653e-45a4-8f39-5143d81173cf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above graph shows the variation of the resistance of the welding with respect to time and the smoothed resistance, as shown by the Red line, after applying the window function.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Next we calculate the derivative by using the lead function and taking the difference of the lead value and the mean value of the resistance.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb6149-ce72-4601-983b-a87f2bc52417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute the lead\n",
    "window_for_lead = welding_dataset_smooth.RESISTANCE_SMOOTHED.window(\n",
    "                            partition_columns   = \"WELDING_ID\",\n",
    "                            order_columns       = 'TIME_MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9bc90-f330-467f-8765-5a00578c6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_dataset_smooth = welding_dataset_smooth.assign(RESISTANCE_SMOOTHED_AFTER = window_for_lead.lead())\n",
    "welding_dataset_smooth = welding_dataset_smooth.assign(DERIVATIVE = welding_dataset_smooth.RESISTANCE_SMOOTHED_AFTER - welding_dataset_smooth.RESISTANCE_SMOOTHED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b925a57-abf3-4461-bde9-7ad1d6675d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes approx. 20s\n",
    "welding_dataset_smooth.sort(['WELDING_ID','TIME_MS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68f532-dec6-496b-9487-9ac6c75789ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_curve = 854\n",
    "single_welding_pandas = welding_dataset_smooth[welding_dataset_smooth.WELDING_ID == id_curve].sort('TIME_MS').to_pandas().reset_index()\n",
    "single_welding_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163be85c-50e5-416d-85fe-d80c5b2755ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,7))\n",
    "single_welding_pandas.plot(x='TIME_MS', y='RESISTANCE', ax=ax[0])\n",
    "single_welding_pandas.plot(x='TIME_MS', y='RESISTANCE_SMOOTHED', ax=ax[0],  xlabel='time (ms)', ylabel=r'resistance  $(m\\Omega)$', color='r')\n",
    "single_welding_pandas.plot(x='TIME_MS', y='DERIVATIVE', ax=ax[1],  xlabel='time (ms)', ylabel=r'resistance  $(m\\Omega)/ms$', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9f6e0-7b26-4fed-9b43-1d35989affad",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We see that the most interesting part lies between 40 and 400ms from the start of the curve, so we plot only that subset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615d965-6892-4729-81b0-9dd39f7d9411",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>It is hard to assess the diversity of curve shapes in this plot since many of them are superimposed. However, we see in the middle of the picture a sharp drop that looks unusual. Moreover, we guess that there are shifts in time and height.</p>\n",
    "<hr>\n",
    "<b style = 'font-size:20px;font-family:Arial;color:#E37C4D'>4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82ee40-3e38-49af-a6ca-a678ba240ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_dataset_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a4c25-f868-44af-bca3-13b4ca477445",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create a feature table by using different functions on the Resistance column. Valid values for functions are: 'count', 'sum', 'min', 'max', 'mean', 'std', 'percentile', 'unique','median', 'var', 'skew', 'kurtosis'. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37d2af-c185-4a84-9ca5-8628a216aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = welding_dataset_new.loc[welding_dataset_new.TIME_MS > 20,:]. \\\n",
    "        groupby(welding_dataset_new.columns[0:5]). \\\n",
    "        agg({\n",
    "            'TIME_MS':['min','max'],\n",
    "            'RESISTANCE':['count', 'sum', 'min', 'max', 'mean', 'std', 'percentile', 'unique','median', 'var','skew','kurtosis']\n",
    "        })\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2bc26-00d3-4a06-bace-8804485f95ac",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'>Store the features in a table in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60bab0-fddc-4f9a-aa89-33c005ec85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_sql(\n",
    "    table_name  = 'welding_features_01',\n",
    "    schema_name = 'demo_user',\n",
    "    primary_index= 'WELDING_ID',\n",
    "    if_exists = 'replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196e16a-9d9d-4d44-a0ed-e5220c3314e2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>5. Anomaly Detection on Sensor Data</b></p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Let's start by getting the feature columns from the features tables</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdf0f8-e0b3-41b5-b18d-b77cdbc5652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = features.columns[7::]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655f048-ffbd-4785-9e8b-39d192ff7808",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>5.1 Clustering by curve shape</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To cluster time series by shapes, we will use the Dynamic Time Warping (DTW) distance that measures the similarity between two time series. This distance is well adapted to this kind of problem since it provides robustness to shifts in time and height.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Distance Matrix in-database Computations</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ClearScape Analytics DTW function computes at scale distances between one reference curve to a set of curves, a many-to-one approach. ClearScape Analytics offers in database dynamic time warping function, callable in SQL as TD_DTW. This function computes at scale the DTW distances between one reference curve to a set of curves, a many-to-one approach. We want to compute the distance matrix of our subset, i.e. the DTW distance between each curve. The distance matrix is symmetric, since the DTW is, hence we only need to compute the triangular matrix. We wrapped this computation in the tdsense package that calls the TD_DTW function and iterates on the matrix row to compute and store the whole triangular distance matrix in a table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e72c8-41e3-481a-9727-a4c7510f4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = welding_dataset_new.groupby('WELDING_DAY').count(distinct=True)\n",
    "dates = list(overview.to_pandas().reset_index()['WELDING_DAY'].values.astype('str'))\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7180b4-a8b5-450a-96be-8aed93d1199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = welding_dataset_new[ \\\n",
    "                 (welding_dataset_new['PLANT'] == 1) & \\\n",
    "                 (welding_dataset_new['ROBOT_ID'] == 41) & \\\n",
    "                 (welding_dataset_new['WELDING_TYPE'] in (8,9)) & \\\n",
    "                 (welding_dataset_new['WELDING_DAY'].isin(dates)) \\\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda2eca-af26-4741-abeb-b63758f8c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_zoom = subset[(subset.TIME_MS < 400) & (subset.TIME_MS > 40)]\n",
    "subset_zoom.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40f422-886d-48e5-a4ce-03b259523917",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Since this is a very small(4 amps) system, the below computation takes around more than 2 hours for 350k rows and so we have pre calculated it and stored in the table in database.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><i>**In case you still want to compute the matrix please set the If part of the below code to <b>True</b> instead of <b>False</b></i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fd1b7-e057-4c0c-b8b0-4e063d70eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    dtw_matrix = dtw_distance_matrix_computation2(subset_zoom,field='RESISTANCE',\n",
    "                                     table_name=dtw_result_table,\n",
    "                                     schema_name = Param['database'],\n",
    "                                     row_axis='TIME_MS',\n",
    "                                     series_id = 'WELDING_ID')\n",
    "else:\n",
    "    dtw_matrix = DataFrame(in_schema('DEMO_AnomolyDetection','DTW_Matrix'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f770a5-f3b2-4862-8256-b1cc1f969750",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>5.2 Hierarchical clustering with Scipy</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now the distance matrix is available, we can perform the clustering. Here we will use the open-source package Scipy and its cluster.hierarchy modules, that have been used in a tdsense for convenience.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87b35b-c283-42d8-845b-5c9c7851c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_matrix_loc = dtw_matrix.sort(columns=['WELDING_ID_2','WELDING_ID_1']).to_pandas(all_rows=True)\n",
    "dtw_matrix_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f64fd3-1f33-4b7c-9d8f-b0636bffc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linked, labelList = hierarchy_dendrogram(dtw_matrix_loc, cluster_distance = 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e168ff-626b-47b8-bc2b-ecfaac22a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hierarchy_clustering(linked, labelList, n_clusters=6)\n",
    "cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b62135-409c-45a9-b604-6e98ccf059fd",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>And if we plot the curves per cluster, we spot the curves with a sharp drop(cluster 2) and these are the curves we are interested in, i.e. the curve exhibiting the anomaly we are looking for. We note also the other clusters are looking more or less similar.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bafdc-9f43-4083-9677-ef7d94c18eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize=(20,10))\n",
    "colors = cluster[['cluster','leaves_color_list']].copy().drop_duplicates()\n",
    "for k in range(6):\n",
    "    plt.subplot(2,3,k+1)\n",
    "    img = plotcurves( subset_zoom,\n",
    "                      field='RESISTANCE',\n",
    "                      row_axis='TIME_MS',\n",
    "                      series_id='WELDING_ID',\n",
    "                      select_id=list(cluster[cluster.cluster ==k].CURVE_ID.values),\n",
    "                      noplot=True)\n",
    "    plt.imshow(img)\n",
    "    plt.title('cluster : ' +str(k) + '\\n' + str(cluster.groupby('cluster').count()['CURVE_ID'][k]) + ' obs.',fontdict = {'fontsize' : 10, 'color':colors.leaves_color_list.values[k]})\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99a7ac-6a99-4c9e-9ead-0f6d6e5c4759",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>5.3 Create the anomaly dataset</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now we create a table containing the anomaly flag that will be the target of a supervised machine learning model or a relevant KPI to monitor in production dashboards.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5b577-b0dd-45c8-8fad-fee1fb1f952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cluster.copy().drop('leaves_color_list',axis=1)\n",
    "target = target[target.cluster.isin([1,2])]\n",
    "target['WELDING_ID'] = target['CURVE_ID']\n",
    "target['anomaly'] = 0\n",
    "target.loc[target.cluster==2,'anomaly'] = 1\n",
    "target.drop(['cluster','CURVE_ID'],axis=1, inplace=True)\n",
    "target.groupby('anomaly').count().plot(y='WELDING_ID',kind='bar',figsize=(10,10))\n",
    "copy_to_sql( target,\n",
    "                  schema_name='demo_user',\n",
    "                  table_name = 'Anomoly_Target',\n",
    "                  if_exists='replace',\n",
    "                  primary_index='WELDING_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7c451-2fb3-45fa-895d-e881cc88a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = DataFrame(in_schema('demo_user', 'Anomoly_Target'))\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6297fd-6f49-4619-af30-791db2af90da",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above anomaly data has the welding ID and the anomaly flag.</p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>5.4 Build the analytical dataset </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We prepare the analytical dataset by joining the feature table with the anomaly table using the Welding ID so that we get the anomalies for the weldings.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cfcfb-7d91-47e5-a4cc-e44428e51cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS = features[['WELDING_ID']+feature_names].join(other=anomalies, how='inner', on='WELDING_ID=WELDING_ID',rsuffix='r',lsuffix='l')\n",
    "ADS = ADS.assign(WELDING_ID=ADS.l_WELDING_ID).drop(['l_WELDING_ID','r_WELDING_ID'],axis=1).select(['WELDING_ID']+feature_names+['anomaly'])\n",
    "ADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2163c-9fea-4f3d-ab0b-696b3cccaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3168b-8c53-4ffd-ba75-b26f40608654",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>6. Build the model </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have datasets in which different columns have different units – like one column can be in kilograms, while another column can be in centimeters. If we feed these features to the model as is, there is every chance that one feature will influence the result more due to its value than the others. But this doesn’t necessarily mean it is more important as a predictor. So, to give importance to all the features we need feature scaling.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we apply the Standard scale and transform functions which are ScaleFit and ScaleTransform functions in Vantage. ScaleFit() function outputs statistics to input to ScaleTransform() function, which scales specified input DataFrame columns.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0898e-53a7-4aca-9f24-2e2f06ac73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import ScaleFit , ScaleTransform\n",
    "scaler = ScaleFit(\n",
    "                    data=ADS,\n",
    "                    target_columns=feature_names,\n",
    "                    scale_method=\"STD\",\n",
    "                    global_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af7c0a-b1cf-4914-a099-aeaeeb0c4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS_scaled = ScaleTransform(data=ADS,\n",
    "                         object=scaler.output,\n",
    "                         accumulate=\"anomaly\").result\n",
    "ADS_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418761ae-bf4c-4bfc-81f5-17e7b820a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fe2c1-1ebf-4f37-b647-f1e3146796ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.output.to_sql(schema_name='demo_user',table_name='scaler_anomaly',if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1ed77-bd6e-4476-9b76-abb448c7199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ADS_scaled.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a8548-555a-48fd-88e4-795abaff2cc5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>6.1 Create a model file using the python libraries.</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. One way to solve this problem is to oversample the examples in the minority class. the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling Technique, or SMOTE for short. SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Then we use the RandomForestClassifier to create the model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The Random forest classifier creates a set of decision trees from a randomly selected subset of the training set. It is basically a set of decision trees (DT) from a randomly selected subset of the training set and then It collects the votes from different decision trees to decide the final prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847d16a-9735-4482-953d-66c80faf0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[feature_names]\n",
    "y_train = df['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350a66c-2ff9-483c-ae30-8f17c5d375b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the training set using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Create a random forest classifier\n",
    "model = RandomForestClassifier(n_estimators=10,max_depth= 3, random_state=42)\n",
    "\n",
    "# Create a pipeline that includes the SMOTE transformer and the model\n",
    "pipeline = PMMLPipeline([ ('model', model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a3ff5-e8ee-4c9b-909e-3e1a79fa6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "start = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print('duration : ', end-start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff634a-aea7-4966-bf38-30b77547f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the training set\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "\n",
    "# calculate and print the accuracy score\n",
    "acc = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "\n",
    "# calculate and print precision, AUC and F1-score\n",
    "prec = precision_score(y_train, y_train_pred)\n",
    "print(\"Precision: {:.2f}%\".format(prec * 100))\n",
    "\n",
    "# calculate AUC, AUC requires probability for positive class\n",
    "prob = pipeline.predict_proba(X_train)[:, 1]\n",
    "auc = roc_auc_score(y_train, prob)\n",
    "print(\"AUC: {:.2f}%\".format(auc * 100))\n",
    "\n",
    "f1 = f1_score(y_train, y_train_pred)\n",
    "print(\"F1-Score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da084cfa-5c7b-4899-9c9b-41b065546bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn2pmml(pipeline, \"my_model.pmml\", with_repr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b23c2-c4c4-4601-b374-9d021a4845b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_columns = {\"Description\": type(\"RandomForestClassifier model\"),\n",
    "                              \"UserId\": type('demo_user'),\n",
    "                              \"ProductionReady\": False,\n",
    "                              \"ModelAccuracy\": float(acc),\n",
    "                              \"ModelPrecision\": prec,\n",
    "                              \"ModelAUC\": auc,\n",
    "                              \"Modelf1Score\": f1,\n",
    "                              \"ModelSavedTime\": str(datetime.now(tz=pytz.UTC)),\n",
    "                              \"ModelGeneratedTime\": end-start,\n",
    "                              \"sklearnVersion\": sklearn.__version__\n",
    "                             }\n",
    "for k in additional_columns.keys():\n",
    "    print(type(additional_columns[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351d68c-fed5-4034-b00f-fe0379625090",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>6.2 Save the model file</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc1be2-d980-4468-9fc9-58ef30e5cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    save_byom(model_id = 'model_anomaly1',\n",
    "          model_file = 'my_model.pmml',\n",
    "          table_name = 'BYOM_PMMLMODELS_REPOSITORY',\n",
    "          schema_name = 'demo_user',\n",
    "          additional_columns={\"Description\": \"RandomForestClassifier model\",\n",
    "                              \"UserId\": 'demo_user',\n",
    "                              \"ProductionReady\": False,\n",
    "                              \"ModelAccuracy\": float(acc),\n",
    "                              \"ModelPrecision\": float(prec),\n",
    "                              \"ModelAUC\": float(auc),\n",
    "                              \"Modelf1Score\": float(f1),\n",
    "                              \"ModelSavedTime\": str(datetime.now(tz=pytz.UTC)),\n",
    "                              \"ModelGeneratedTime\": float(end-start),\n",
    "                              \"sklearnVersion\": sklearn.__version__\n",
    "                             }\n",
    "            )\n",
    "except Exception as e: \n",
    "    # if our model exists, delete and rewrite if str(e.args).find('TDML_2200') >= 1: \n",
    "    delete_byom(model_id = 'model_anomaly1', table_name = 'BYOM_PMMLMODELS_REPOSITORY',schema_name = 'demo_user') \n",
    "    save_byom(model_id = 'model_anomaly1',\n",
    "          model_file = 'my_model.pmml',\n",
    "          table_name = 'BYOM_PMMLMODELS_REPOSITORY',\n",
    "          schema_name = 'demo_user',\n",
    "          additional_columns={\"Description\": \"RandomForestClassifier model\",\n",
    "                              \"UserId\": 'demo_user',\n",
    "                              \"ProductionReady\": False,\n",
    "                              \"ModelAccuracy\": float(acc),\n",
    "                              \"ModelPrecision\": float(prec),\n",
    "                              \"ModelAUC\": float(auc),\n",
    "                              \"Modelf1Score\": float(f1),\n",
    "                              \"ModelSavedTime\": str(datetime.now(tz=pytz.UTC)),\n",
    "                              \"ModelGeneratedTime\": float(end-start),\n",
    "                              \"sklearnVersion\": sklearn.__version__\n",
    "                             }\n",
    "            )\n",
    "    pass \n",
    "else: \n",
    "    raise    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0f97c-52b2-407e-921c-75a61ca2d3fa",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The model file is saved as can be seen in the left pane(file editor).</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We create new scaled data to apply this model and predict data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe7dff-a0fa-43a6-aa03-d11aeed2904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = features[['WELDING_ID']+feature_names].join(other=anomalies, how='inner', on='WELDING_ID=WELDING_ID',rsuffix='r',lsuffix='l')\n",
    "newdata = newdata.assign(WELDING_ID=newdata.l_WELDING_ID).drop(['l_WELDING_ID','r_WELDING_ID'],axis=1).select(['WELDING_ID']+feature_names+['anomaly'])\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda2ce6-4728-41dc-847a-b6aec79a5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b4d80-3bb8-4e96-ba57-c85c84ae990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_scaled = ScaleTransform(data=newdata,\n",
    "                         object=DataFrame(in_schema('demo_user','scaler_anomaly')),\n",
    "                         accumulate=[\"WELDING_ID\",\"anomaly\"]).result\n",
    "newdata_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb63a9-35eb-40e9-a4d4-d1aa558b19d1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>6.3 Retrieve the model file and use it to predict</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We use the PMMLPredict function from the teradataml library to predict the anomalies.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Predictive Model Markup Language (PMML) is an XML-based standard established by the Data Mining Group (DMG) for defining statistical and data-mining models. PMML models can be shared between PMML-compliant platforms and across organizations so that business analysts and developers are unified in designing, analyzing, and implementing PMML-based assets and services.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0c6bb-3551-4337-a4e3-8c2a79fd55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import PMMLPredict\n",
    "modeldata_anomoly = retrieve_byom(\"model_anomaly1\", table_name=\"BYOM_PMMLMODELS_REPOSITORY\")\n",
    "result=PMMLPredict(\n",
    "                modeldata = modeldata_anomoly,\n",
    "                newdata = newdata_scaled,\n",
    "                accumulate = ['WELDING_ID'],\n",
    "                overwrite_cached_models = '*'\n",
    "                )\n",
    "result.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b14f1b-804a-40cc-9cb7-288e45d6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expand_json = f\"\"\"\n",
    "SELECT \n",
    "    WELDING_ID\n",
    "      ,   \"probability(0)\" as proba_0\n",
    ",   \"probability(1)\" as proba_1\n",
    "FROM TD_JSONSHRED(\n",
    "    ON (SELECT WELDING_ID, json_report FROM  {result.result._table_name})\n",
    "    USING\n",
    "    ROWEXPR('')\n",
    "    COLEXPR('probability(1)','probability(0)')\n",
    "    RETURNTYPES('FLOAT','FLOAT')\n",
    "    ) t\n",
    "\"\"\"\n",
    "sanitized_results = DataFrame.from_query(query_expand_json)\n",
    "sanitized_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220bb477-2d63-4672-98a1-cb50d40f960f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>7. Decision Forest </b></p>\n",
    " \n",
    "<p style = 'font-size:16px;font-family:Arial'>We will now use the DecisionForest model to predict the anomalies. A decision forest is a generic term to describe models made of multiple decision trees. The prediction of a decision forest is the aggregation of the predictions of its decision trees. The implementation of this aggregation depends on the algorithm used to train the decision forest. The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We start by creating a subset for the most interesting part lies between 40 and 400ms from the start of the curve.</p>\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a84c6-2c67-43c7-86e2-1f31c6bd1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_curves_zoom = welding_dataset_new[(welding_dataset_new.TIME_MS > 40) & (welding_dataset_new.TIME_MS < 400) ]\n",
    "DF_curves_zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9f479-f2ff-4863-b969-b9b8a873e6d4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Create the various features by using the window function on the Resistance and taking the difference between the previous and current resistance based on time. Features will be created by using the aggregation function on this resistance and the difference of the resistance.</p>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227337c-3b57-443c-a256-dd5230ed98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_curves_zoom = DF_curves_zoom.assign(\n",
    "    resistance_diff = DF_curves_zoom.RESISTANCE \n",
    "                        - DF_curves_zoom.RESISTANCE.window(\n",
    "                                partition_columns=['WELDING_ID'],\n",
    "                                order_columns=[\"TIME_MS\"]\n",
    "                            ).lag(1)\n",
    ")\n",
    "# DF_curves_zoom[DF_curves_zoom.WELDING_ID==138].sort(\"TIME_MS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c00e7-c465-46ba-99ae-c094969a2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features = DF_curves_zoom.groupby(\"WELDING_ID\").agg({\n",
    "    'RESISTANCE':['sum', 'min', 'max', 'mean', 'std', 'var','skew','kurtosis'],\n",
    "    'resistance_diff':['min']\n",
    "})\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6498373-8b50-49fb-ac0b-b0db7b0cb522",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = DF_features.columns[1:]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57712977-e195-4ce9-9867-a7cdbc772279",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.1 Build the anaytical dataset.</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial'>The anaytical dataset is created by joining the anomaly table created above and the dataset with the features created.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55686241-b413-45eb-a495-9888c946c634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF_target = DataFrame(in_schema('demo_user', 'Anomoly_Target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b595e-d794-4797-9125-b0bd2e9b046a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF_ADS_train = DF_features[['WELDING_ID']+feature_names].join(\n",
    "    other=DF_target, how='inner', on='WELDING_ID=WELDING_ID',rsuffix='r',lsuffix='l')\n",
    "DF_ADS_train = DF_ADS_train.assign(WELDING_ID=DF_ADS_train.l_WELDING_ID\n",
    "                                  ).drop(['l_WELDING_ID','r_WELDING_ID'],axis=1\n",
    "                                        ).select(['WELDING_ID']+feature_names+['anomaly']\n",
    "                                                ).assign(anomaly_int = DF_ADS_train.anomaly.cast(INTEGER()))\n",
    "DF_ADS_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199e5db-a881-4a2e-92df-0fcc0a54158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ADS_score = DF_features[['WELDING_ID']+feature_names]\\\n",
    "                            [DF_features.WELDING_ID>800]\n",
    "DF_ADS_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3865607-6205-43e4-a3be-2142af2dd340",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'>We store these training and scoring datasets into Vantage to be used by the In-DB functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d263-3183-4e37-aa05-6f5ccd61ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ADS_train.to_sql(\n",
    "    table_name  = 'ADS_train_data',\n",
    "    schema_name = 'demo_user',\n",
    "    primary_index= 'WELDING_ID',\n",
    "    if_exists = 'replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8815e7-cdc4-40fb-9160-bfd466d7535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ADS_score.to_sql(\n",
    "    table_name  = 'ADS_test_data',\n",
    "    schema_name = 'demo_user',\n",
    "    primary_index= 'WELDING_ID',\n",
    "    if_exists = 'replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cc3c9-6828-4c65-9b72-53ea02a172cd",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.2 Train Decision Forest</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <a href = 'https://docs.teradata.com/search/all?query=TD_DecisionForest&content-lang=en-US'>TD_DecisionForest</a> is an ensemble algorithm used for classification and regression predictive modeling problems. It is an extension of bootstrap aggregation (bagging) of decision trees. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This function takes the training data as input, as well as the following function parameters</p>\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>\n",
    "        <li>InputColumns; list or range of columns used as features (we used an ordinal reference of columns 2:217)</li>\n",
    "        <li>ResponseColumn; the dependent or target value (we used “class”, the first column)</li>\n",
    "        <li>TreeType; either CLASSIFICATION or REGRESSION</li>\n",
    "    <li>Other hyperparameter values detailed in the documentation</li>\n",
    "        </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfbfad-e6d9-4125-a8d7-fb2320a71150",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''Create table DF_train as (\n",
    "SELECT * FROM TD_DecisionForest (\n",
    "ON ADS_train_data AS INPUTTABLE partition by ANY\n",
    "USING\n",
    "  ResponseColumn('anomaly_int')\n",
    "InputColumns('sum_RESISTANCE', 'min_RESISTANCE', 'max_RESISTANCE', 'mean_RESISTANCE', 'std_RESISTANCE', 'var_RESISTANCE', 'skew_RESISTANCE',\n",
    " 'kurtosis_RESISTANCE', 'min_resistance_diff')\n",
    "MaxDepth(16)\n",
    "MinNodeSize(1)\n",
    "NumTrees(8)\n",
    "ModelType('CLASSIFICATION')\n",
    "Seed(3)\n",
    "Mtry(1)\n",
    "MtrySeed(3)\n",
    ") AS dt\n",
    ") with data;\n",
    "'''\n",
    " \n",
    "try:\n",
    "    eng.execute(query)\n",
    "except:\n",
    "    eng.execute('DROP TABLE DF_train;')\n",
    "    eng.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca1ef54-8f11-48af-9d9f-ffe19a08b050",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.3 Predict and Evaluate Decision Forest model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Execute a testing prediction using the split data above.  Evaluate the model by creating a confusion matrix with the <a href = 'https://docs.teradata.com/search/all?query=TD_ClassificationEvaluator&content-lang=en-US'>TD_ClassificationEvaluator</a> SQL Function.</p>\n",
    "\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Execute <a href = 'https://docs.teradata.com/r/Teradata-VantageTM-Analytics-Database-Analytic-Functions-17.20/Model-Scoring-Functions/DecisionForestPredict'>DecisionForestPredict</a> using the model built above</li>\n",
    "    <li>Execute <a href = 'https://docs.teradata.com/search/all?query=TD_ClassificationEvaluator&content-lang=en-US'>TD_ClassificationEvaluator</a> and pass the actual classification and the predicted value</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a987d-2dfb-4f4c-8a20-503eb0b4d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "Create table DF_Predict as (\n",
    "SELECT * FROM TD_DecisionForestPredict (\n",
    "ON ADS_train_data AS InputTable PARTITION BY ANY\n",
    "ON DF_Train AS ModelTable DIMENSION\n",
    "USING\n",
    "  IdColumn ('WELDING_ID')\n",
    "  Accumulate ('anomaly_int')\n",
    "  Detailed('false')\n",
    "  OutputProb ('true')\n",
    "  Responses ('0','1')\n",
    ") AS dt) with data;'''\n",
    "\n",
    "try:\n",
    "    eng.execute(query)\n",
    "except:\n",
    "    eng.execute('DROP TABLE DF_Predict;')\n",
    "    eng.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0ff60-f38b-40a4-aafe-0d644a284e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict= DataFrame(in_schema('demo_user', 'DF_Predict'))\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d5fdc-43c9-4ee3-b3b4-e68da1302ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * FROM TD_ClassificationEvaluator(\n",
    "   ON DF_predict AS InputTable\n",
    "   --cast(target as VARCHAR(32000) CHARACTER SET UNICODE NOT CASESPECIFIC) as target from DF_predict_test) AS InputTable\n",
    "   OUT VOLATILE TABLE OutputTable(additional_metrics_test)\n",
    "   USING\n",
    "   ObservationColumn('anomaly_int')\n",
    "   PredictionColumn('prediction')\n",
    "   Labels(0,1)\n",
    ") AS dt;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    eng.execute(query)\n",
    "except:\n",
    "    eng.execute('DROP TABLE additional_metrics_test;')\n",
    "    eng.execute(query)\n",
    "\n",
    "# pd.read_sql('SELECT * FROM additional_metrics_test', eng)\n",
    "df_metrics= DataFrame(in_schema('demo_user', 'additional_metrics_test'))\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352bc41-0b92-4e37-9eff-2d87f903159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_eval=DataFrame(in_schema('demo_user', 'DF_Predict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973565d1-d4e5-446b-803a-8d1d2e234238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability can be used to further discriminate!\n",
    "DF_eval.groupby([\"anomaly_int\",\"prediction\"]).agg(\n",
    "    {\"prob_0\":[\"mean\",\"std\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda02bba-235d-4f1a-b2a7-3e2ea619cce2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.4 Show AUC-ROC Curve for DecisionForestPredict</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <a href = 'https://docs.teradata.com/search/all?query=TD_ROC&content-lang=en-US'>ROC</a> curve shows the performance of a binary classification model as its discrimination threshold varies. For a range of thresholds, the curve plots the true positive rate against false-positive rate.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This function accepts a set of prediction-actual pairs as input and calculates the following values for a range of discrimination thresholds.</p>\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>\n",
    "        <li>True-positive rate (TPR)</li>\n",
    "        <li>False-positive rate (FPR)</li>\n",
    "        <li>The area under the ROC curve (AUC)</li>\n",
    "        <li>Gini coefficient</li>\n",
    "        <li>Other details are mentioned in the documentation</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1c9e2-be8c-44da-9e0a-9056a2ec8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import ROC \n",
    "roc_obj = ROC(DF_eval, \n",
    "                    probability_column = \"prob_1\",\n",
    "                    observation_column = \"anomaly_int\",\n",
    "                    positive_class=\"1\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27834036-13cc-49e9-a34e-b2bcb2c192b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data = roc_obj.output_data.to_pandas().sort_values(\"fpr\", ascending=True)\n",
    "roc_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90afd6-b0c1-4edd-9492-c97b16c8d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_obj.result.to_pandas().reset_index().iloc[0,0]\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9044f2d-b66e-4c0d-9478-356cb16541c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of a ROC curve for DecisionForest\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(roc_data.fpr, roc_data.tpr, label='ROC curve (area = %0.2f)' % auc, drawstyle='steps')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate',fontsize=12)\n",
    "plt.ylabel('True Positive Rate',fontsize=12)\n",
    "plt.title('Receiver operating characteristic curve for DecisionForest',fontsize=14)\n",
    "plt.legend(loc=\"lower right\",fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447128d-3b3b-4153-9b86-458af227fdde",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>It looks like the optimal threshold for prob_1 is around 0.2 (see table roc_data).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3f67f-3ee5-4f54-982d-406c62baa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set meaningful threshold, where true positive rate is above 80% and false positive rate below 20%\n",
    "THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd88d6-c12c-40e2-b95c-7d950577909b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.5 Score new Data</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c8ea9-2dd2-4c65-97b7-cb711ca3eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "Create table DF_Predict_test as (\n",
    "SELECT * FROM TD_DecisionForestPredict (\n",
    "ON ADS_test_data AS InputTable PARTITION BY ANY\n",
    "ON DF_Train AS ModelTable DIMENSION\n",
    "USING\n",
    "  IdColumn ('WELDING_ID')\n",
    "  Detailed('false')\n",
    "  OutputProb ('true')\n",
    "  Responses ('0','1')\n",
    ") AS dt) with data;'''\n",
    "\n",
    "try:\n",
    "    eng.execute(query)\n",
    "except:\n",
    "    eng.execute('DROP TABLE DF_Predict_test;')\n",
    "    eng.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58448648-772a-445e-9989-8b174ac9db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_test= DataFrame(in_schema('demo_user', 'DF_Predict_test'))\n",
    "df_predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e6218-5f6b-4957-a2ae-13df739ebbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_scored=DataFrame(in_schema('demo_user', 'DF_Predict_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdb7ee-47b4-44d3-bda3-92066b133ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1 = DF_scored.prob_1\n",
    "DF_scored = DF_scored.assign(above_threshold  = case([(prob_1>THRESHOLD, 1 )],\n",
    "                                         else_ = 0))\n",
    "DF_scored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea71dd2-d060-4964-a9d3-b41e36cd67e4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.6 Validate data visually</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2038fe1-92ec-4a20-bff4-f9402fd2d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "welding_ids_NiO = DF_scored[DF_scored.above_threshold==1]\\\n",
    "    .select([\"WELDING_ID\"]).head(30).to_pandas().reset_index()\\\n",
    "    [\"WELDING_ID\"].values.tolist()\n",
    "welding_ids_iO = DF_scored[DF_scored.above_threshold==0]\\\n",
    "    .select([\"WELDING_ID\"]).head(30).to_pandas().reset_index()\\\n",
    "    [\"WELDING_ID\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36500800-7056-48f3-9fb6-ba9f9696d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_scored_NiO = DF_curves_zoom[DF_curves_zoom.WELDING_ID.isin(welding_ids_NiO)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3900dfe-2b6a-4dcb-b297-c369e3283cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_scored_iO = DF_curves_zoom[DF_curves_zoom.WELDING_ID.isin(welding_ids_iO)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d301b-f451-4aa5-977a-6d765c375c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "img = plotcurves(DF_scored_NiO,field='RESISTANCE',row_axis='TIME_MS', series_id='WELDING_ID',select_id=None,noplot=True, color=\"r\")\n",
    "plt.imshow(img)\n",
    "plt.title('NiO',fontdict = {'fontsize' : 10})\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "img = plotcurves(DF_scored_iO,field='RESISTANCE',row_axis='TIME_MS', series_id='WELDING_ID',select_id=None ,noplot=True, color=\"b\")\n",
    "plt.imshow(img)\n",
    "plt.title('iO',fontdict = {'fontsize' : 10})\n",
    "plt.axis('off')\n",
    "plt.suptitle('Visual Inspection of Scored Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be6263-22d8-43d2-94e2-1f58d730f567",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>8. Conclusion</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have seen an end-to-end exploration process for labeling anomalous time series using ClearScape Analytics on Teradata Vantage. Thanks to the in-database capabilities offered by Teradata Vantage with ClearScape Analytics, we were able to run this exploration with the smallest notebook instance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e90d19-1b71-44e8-b6d5-aa53e3b673c1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>9. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Work Tables</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a959e6-319f-4592-93af-482d391224b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['welding_features_01', 'scaler_anomaly', 'ADS_train_data', 'ADS_test_data',\n",
    "          'DF_train', 'DF_Predict', 'DF_Predict_test',\n",
    "          'additional_metrics_test']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    db_drop_table(table_name=table, schema_name='demo_user')\n",
    "    # Construct the drop table SQL statement\n",
    "    # drop_table_sql = f\"DROP TABLE {table};\"\n",
    "    # Execute the drop table command\n",
    "    # eng.execute(drop_table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603da4b4-fb07-45f1-8c39-8b0e365c2cec",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Databases and Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf7acc-2df8-48fa-a92a-cc871a94fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_AnomolyDetection');\" \n",
    "#Takes 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8f9bc-9f3a-47e9-b2d4-81fd00291bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da48da7-d4de-4693-9365-5d5f63810673",
   "metadata": {
    "tags": []
   },
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Copyright © Teradata Corporation - 2023. All Rights Reserved.</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
