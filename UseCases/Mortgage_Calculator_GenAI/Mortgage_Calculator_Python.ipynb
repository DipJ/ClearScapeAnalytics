{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bc532d-c78c-4c89-b191-242da0733f39",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Mortgage Calculator chatbot using Generative AI with Vantage</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff71661-19b4-423a-867a-7c815b064c81",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Introduction:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the Mortgage Calculator chatbot using Generative AI demo, the combination of <b>RAG, Langchain, and LLM models</b> allows users to ask queries in layman's terms, retrieve relevant information from the Vantage tables, and generate accurate and concise answers based on the retrieved data. This integration of retrieval-based and generative-based approaches provides a powerful tool for extracting knowledge from structured sources and delivering user-friendly responses.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we will build Generative Question-Answering using LangChain, a powerful library for working with LLMs like GPT-3.5, GPT-4, Bloom, etc. and JumpStart in ClearScape notebooks, a system is built where users can ask business questions in natural English and receive answers with data drawn from the relevant databases.</p>\n",
    "\n",
    "\n",
    "<center><img src=\"images/header.png\" alt=\"mortgage calc\"  width=800 height=800/></center>\n",
    "\n",
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before going any farther, let's get a better understanding of RAG, LangChain, and LLM.</p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'><li> <b>Retrieval-Augmented Generation (RAG):</b></li></ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp;RAG is a framework that combines the strengths of retrieval-based and generative-based approaches in question-answering systems.It utilizes both a retrieval model and a generative model to generate high-quality answers to user queries. The retrieval model is responsible for retrieving relevant information from a knowledge source, such as a database or documents. The generative model then takes the retrieved information as input and generates concise and accurate answers in natural language.</p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'><li> <b>Langchain:</b></li></ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp; Langchain is a language model developed for understanding and generating human-like text. It is designed to handle queries and requests expressed in everyday language, enabling users to ask questions in layman's terms. Langchain leverages state-of-the-art deep learning techniques to comprehend the semantics and context of user queries. It can process various types of queries, ranging from simple factual questions to complex and nuanced queries.</p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'><li> <b>LLM Models (Large Language Models):</b></li></ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp; LLM models refer to the large-scale language models that are trained on vast amounts of text data.\n",
    "These models, such as GPT-3 (Generative Pre-trained Transformer 3),  GPT-3.5, GPT-4, HuggingFace BLOOM, LLaMA, Google's FLAN-T5, etc. are capable of generating human-like text responses. LLM models have been pre-trained on diverse sources of text data, enabling them to learn patterns, grammar, and context from a wide range of topics. They can be fine-tuned for specific tasks, such as question-answering, natural language understanding, and text generation.\n",
    "LLM models have achieved impressive results in various natural language processing tasks and are widely used in AI applications for generating human-like text responses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca49b9d-df0b-400a-acf8-0252ab8a2618",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>LLM</li>\n",
    "    <li>Run the query function</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833b5bf-74af-42be-8543-3782e1da95dc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>1. Configuring the environment</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6027a7-888d-441f-abc7-a6ea1c45f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# '%%capture' suppresses the display of installation steps of the following packages\n",
    "\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97cdce-0d5e-4e54-b404-da7bae24ef51",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    <i>The above statements will install the required libraries to run this demo. To gain access to installed libraries after running this, restart the kernel.</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b160ce-5ace-4116-86b6-394d6502553b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>To restart the kernel, press the escape key first, then type 0 0.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61067c88-2e9a-4c92-985b-34dc4ab74a13",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>1.1 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50a4aa-1211-44fc-8166-317c35253207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# teradata lib\n",
    "from teradataml import *\n",
    "\n",
    "# LLM\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "# from langchain import PromptTemplate,SQLDatabase, SQLDatabaseChain, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import panel as pn\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718f8-7af4-4d1a-abc7-a860eb7cbae3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>2. Connection to Vantage and OpenAI</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153b889-0924-4e0f-acc8-6b0d440c77b3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.1 Get the OpenAI API key</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e10add-4b3c-4fc5-9359-0b44737bba0f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In order to utilize this demo, you will need an OpenAI API key. If you do not have one, please refer to the instructions provided in this guide to obtain your OpenAI API key: </p>\n",
    "\n",
    "[Openai_setup_api_key_guide](..//Openai_setup_api_key/Openai_setup_api_key.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd64269-9393-434a-ac26-0b8386fc0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your openai api key\n",
    "api_key = input(prompt = '\\n Please Enter Openai api key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e4d23-3f17-4d43-b9b9-7e125f59b8c7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.2 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cfc91-93ed-45b9-98ba-73b91a50c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "eng.execute('''SET query_band='DEMO= Generative_Question_Answering_Python.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22db506-84a7-406b-be9f-9fe69d268ba4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beec557-411b-4418-acf3-2f28d3c6beac",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.3 Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. You can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. You may switch which mode you choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c91d11-b431-4a0b-8344-a3a71f660bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"Customer\", \"Interest\"]:\n",
    "    try:\n",
    "        eng.execute(f\"DROP TABLE {t}\")\n",
    "    except: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530fc25-7ad8-4669-bd69-85bc8c9c12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/datagen-table_cust.sql', 'r') as f:\n",
    "    # print(f.read())\n",
    "    eng.execute(f.read())\n",
    "    \n",
    "with open('data/datagen_table_int.sql', 'r') as f:\n",
    "    # print(f.read())\n",
    "    eng.execute(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785508b-3a4e-4897-a855-a4e16e316001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/datagen_insert.sql', 'r') as f:\n",
    "    # print(f.read())\n",
    "    eng.execute(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26ca7c-fa9e-4200-8e9e-27f5886aa5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_MarketingCamp_cloud');\"        # Takes 1 minute\n",
    "# %run -i ../run_procedure.py \"call get_data('DEMO_MarketingCamp_local');\"        # Takes 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a16b5-da34-4326-b66b-5f7d5a9d9b04",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79252a5e-b2c1-404a-ae22-6c3d698510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f3fe2-ed63-4838-bb19-4c0d0157453d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>3. Data Exploration</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The goal of the Marketing Campaign Effectiveness prediction is to reduce marketing resources by identifying customers who would purchase the product and thereby directing marketing efforts to them.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The data is from the last marketing campaign, with thousands of rows of customer data like age, job, marital status, education, etc.<p/>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Each row is a snapshot of data taken during the last marketing campaign, and each column is a different variable. The input dataset can be divided into three categories, as below:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'> \n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>customer data i.e. age, profession, eduction, monthly income, etc.</li>\n",
    "    <li>attributes related with the last contact of the current campaign i.e. contact, month, day, etc.</li>\n",
    "    <li>other attributes i.e. campaign, previous outcome, payment methods, etc.</li>\n",
    "   <li>target attribute - purchased.</li>\n",
    "\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The source data from <a href=\"https://www.kaggle.com/datasets/janiobachmann/bank-marketing-dataset\">kaggle</a> is loaded in Vantage and supplemented with information about city, monthly income, family members, etc. The data is loaded into vantage table named <i>Retail_Marketing</i>.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b><i>*Please scroll down to the end of the notebook for detailed column descriptions of the dataset.</i></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8e565-b58b-4cc6-8d9c-50790ca3dcab",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.1 Examine the Customer and Interest table</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Let's look at the sample data in Customer table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9951cf-e2b0-40d7-9a8f-a01c7f48ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(in_schema('demo_user', 'Customer'))\n",
    "\n",
    "print(\"Data information: \\n\",tdf.shape)\n",
    "tdf.sort('CustomerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6d954-b72f-41c3-9f8c-bc15cf74d672",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>There are 5 records in all, and there are 18 variables.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now, let's look at the interest table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad00db-08e5-40d3-a3d2-2be92908a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(in_schema('demo_user', 'Interest'))\n",
    "\n",
    "print(\"Data information: \\n\",tdf.shape)\n",
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b6add-e7fa-4381-8d73-2c6a86daaf7f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>we can see that interest are basis on individuals CreditScore.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72874be1-a21c-4bd3-bbf3-8d346d89a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''\n",
    "    select c.FirstName, c.LastName, c.CreditScore, i.InterestRate from Customer c join Interest i on c.CreditScore between i.MinCreditScore and i.MaxCreditScore\n",
    "    where CustomerID=1\n",
    "'''\n",
    "\n",
    "pd.read_sql(q, eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89367d27-22ca-4a0e-b299-0f8e1a669157",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>4. LLM </b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>4.1 Connect to databases using SQL Alchemy</b></p>    \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Under the hood, LangChain uses SQLAlchemy to connect to SQL databases. The SQLDatabaseChain can therefore be used with any SQL dialect supported by SQLAlchemy, such as Teradata Vantage, MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. Please refer to the <a href=\"https://docs.sqlalchemy.org/en/20/\"> SQLAlchemy documentation</a> for more information about requirements for connecting to your database.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Important: The code below establishes a database connection for data sources and Large Language Models. Please note that the solution will only work if the database connection for your sources is defined in the cell below</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Build a consolidated view of Table Data Catalog by combining metadata stored for the database and table in pipe delimited format.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced43c34-b7d5-4ea2-b91a-94801e80656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(conn):\n",
    "    \"\"\"Return a list of table names.\"\"\"\n",
    "    table_names = ['Customer', 'Interest']\n",
    "    # tables = conn.execute(\"sel TableName from dbc.tables where databasename = 'demo_user' and creatorname='demo_user';\")\n",
    "    # for table in tables.fetchall():\n",
    "    #     table_names.append(table[0].strip())\n",
    "    \n",
    "    return table_names\n",
    "\n",
    "\n",
    "def get_column_names(conn, table_name):\n",
    "    \"\"\"Return a list of column names.\"\"\"\n",
    "    column_names = []\n",
    "    print(f\"table_name: {table_name}\")\n",
    "    qry = f\" SELECT ColumnName FROM dbc.columnsV WHERE DatabaseName = 'demo_user' and TableName = '{table_name}';\"\n",
    "    columns = conn.execute(qry).fetchall()\n",
    "    for col in columns:\n",
    "        column_names.append(col)\n",
    "        \n",
    "    column_names = [list(i)[0].replace('(','').replace(')','') for i in column_names]\n",
    "    print(f\"column_names: {column_names}\")\n",
    "    return column_names\n",
    "\n",
    "\n",
    "def get_database_info(conn):\n",
    "    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n",
    "    table_dicts = []\n",
    "    for table_name in get_table_names(conn):\n",
    "        columns_names = get_column_names(conn, table_name)\n",
    "        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n",
    "    return table_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3ebbb-c82e-4f1b-b3cc-55462462d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema_dict = get_database_info(eng)\n",
    "database_schema_string = \"\\n\".join(\n",
    "    [\n",
    "        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
    "        for table in database_schema_dict\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(database_schema_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781b83c-0907-4bbd-8fe1-1067e587512b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b> 4.2 Format the answer and Display</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To view the answer in proper format with markdown</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c27d29-1bde-4908-b7f1-309dcfd89eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def response_template(query, response):\n",
    "    if \"result\" in response:\n",
    "        return f\"<p style = 'font-size:16px;font-family:Arial'>SQL and response from user query {query}  <br> <b>{response['result']}<b>\"\n",
    "    else:\n",
    "        return f\"<p style = 'font-size:16px;font-family:Arial'>SQL and response from user query {query}  <br> <b>{response}<b>\"\n",
    "\n",
    "def error_template():\n",
    "    return f\"<p style = 'font-size:16px;font-family:Arial'>Sorry, there was an error while generating the SQL query. The GenAI may have made a mistake in the syntax of the query.  <br>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680228a2-3bfa-44f4-8a02-b73fa8bf8e95",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>4.3 Define LLM model</b></p>  \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In OpenAI's language models, the <b>temperature</b> parameter controls the randomness of the generated text. It affects the diversity and creativity of the model's responses.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A higher temperature value, such as 1.0 or above, increases the randomness and diversity of the generated output. This can lead to more varied and surprising responses, but it may also result in less coherence and occasional nonsensical outputs.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>On the other hand, a lower temperature value, such as 0.2 or below, reduces randomness and makes the model's output more focused and deterministic. The generated text is likely to be more conservative, sticking closely to patterns observed in the training data.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Choosing an appropriate temperature value depends on the desired output. Higher temperatures can be useful for creative tasks or brainstorming, while lower temperatures are preferred when you need more control over the output, such as when generating specific responses or following a particular style.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbceafb4-793b-4d62-ab22-c9d49ac9a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# # OpenAI API\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    \n",
    "# # call open AI model - api\n",
    "# llm = OpenAI(temperature=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678df33-4503-44c4-9345-c4fcb00c1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe3f75-c7ff-4142-9848-7aa89530a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tool\n",
    "from langchain.tools import tool\n",
    "\n",
    "db = SQLDatabase(eng)\n",
    "llm = OpenAI(temperature=0, verbose=True)\n",
    "\n",
    "@tool\n",
    "def generate_sql(query: str) -> str:\n",
    "    \"\"\"Given an input question, first create a syntactically correct query to run, then look at the results of the query and return the answer.\"\"\"\n",
    "    agent_executor = create_sql_agent(\n",
    "        llm=OpenAI(temperature=0),\n",
    "        toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n",
    "        verbose=True,\n",
    "        agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    )\n",
    "    return agent_executor.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5db2c5-2e48-4aa9-84e9-f904b8e37ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [generate_sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ffe4a-8ffe-4151-9fe2-0542395beebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_tool_to_openai_function(generate_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc7ea0-9229-4ba9-8dd7-1f55f7426f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11330bf4-8eb6-49ec-ab60-52cd99b52eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"What will be the InterestRate for customer whose CustomerID=1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f437666-621a-45c8-849e-8c993dd527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c6ec0-d20e-41ee-b2d8-680fddec4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e41cf-3a04-4cce-ae96-724783c1d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_sql(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e37f3-0726-45c9-9a7c-ace3c1276f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cda78d-cbf2-40b3-8a44-99cc16d7a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "import panel as pn\n",
    "import param\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "# Current user\n",
    "CustomerID = 1\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    # chat_history = param.List([])\n",
    "    \n",
    "    \n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "\n",
    "        self.functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "        # self.memory= ConversationBufferWindowMemory(return_messages=True,memory_key=\"chat_history\", k=5)\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", '''You will play the role of a mortgage adviser working with a client to create a mortgage quote. I will play the role of the client.'''),\n",
    "                    \n",
    "            (\"system\", '''As a mortgage advisor for the bank you have direct access to the banks data for me\n",
    "                    generate the SQL to get the details  like Balance, Income, CreditScore, etc. if user don't know the answer or Do not enter the answer.'''),\n",
    "                    \n",
    "            (\"system\", '''This information should compliment any answers you gain from the me. \n",
    "                    Ask a series of questions, no more than eight, sequentially - pausing between each question to wait for a response before proceeding to the next. '''),\n",
    "\n",
    "            (\"system\", '''Here's the first question: 'Income: What is your total annual income, including any additional sources of income?' '''),\n",
    "\n",
    "            (\"system\", '''And then follow this format for each subsequent question. Again, wait for my response before moving on to the next. '''),\n",
    "\n",
    "            (\"system\", '''Start the chat with greeting and ask the questions to user as mentioned above. '''),\n",
    "\n",
    "            (\"system\", '''For example:\n",
    "                    First question: What is your total annual income?\n",
    "                    Second question: Do you have any additional income?'''),\n",
    "            \n",
    "            (\"system\", '''Examples of question and expected SQLQuery\n",
    "                    Question: What will be the InterestRate for customer whose CustomerID=1\n",
    "                    SQLQuery: SELECT i.InterestRate from Customer c join Interest i on c.CreditScore between i.MinCreditScore and i.MaxCreditScore where CustomerID=1'''),\n",
    "            \n",
    "            (\"system\", '''Calculate the maximum house price and monthly repayment for principal and interest based on a 5% APR. Assume income tax is 25% and \n",
    "                    that the bank will lend no more than an 80% LTV.'''),\n",
    "            \n",
    "            (\"system\", '''\"Provide a succinct summary paragraph of these details as a response to the client but don't show the method of calculation. Follow with a formatted table of monthly amounts \n",
    "                    (gross income, tax obligation, mortgage obligation, expenses, remaining disposable income)'''),\n",
    "                    \n",
    "             (\"system\", f'''If the user fails to provide a response or says 'I don't know' for a question, automatically call the 'generate_sql' function to retrieve the answer from the database. The function should return the relevant answer for the question asked.\n",
    "                    current user: {CustomerID}'''),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        self.chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | self.prompt | self.model | OpenAIFunctionsAgentOutputParser()\n",
    "        self.qa = AgentExecutor(agent=self.chain, tools=tools, verbose=False, memory=self.memory)\n",
    "        # self.qa = create_sql_agent(llm=OpenAI(temperature=0),agent=self.chain, tools=tools, memory=self.memory,toolkit=toolkit,verbose=True,agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,)\n",
    "        \n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = ''\n",
    "        result = self.qa.invoke({\"input\": query})\n",
    "        self.answer = result['output']\n",
    "        # self.chat_history.extend([(query, result[\"output\"])])\n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=800)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=800, styles={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "    \n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#E37C4D'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef158864-1f54-41e7-a03c-6e1c7db4ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=400),\n",
    "    pn.Row(inp),\n",
    "    \n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "# tab2 = pn.Column(\n",
    "#     pn.panel(cb.get_chats),\n",
    "#     pn.layout.Divider(),\n",
    "# )\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# Mortgate Calculator Chatbot')),\n",
    "    pn.Tabs(('Conversation', tab1))\n",
    "    # pn.Tabs(('Conversation', tab1), ('History', tab2))\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c8e78-6e22-463e-8187-06144ed9c4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3d740-0df0-46a6-a185-5e4bbb163091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a7558-5089-49ef-836d-36a90a836ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f636542-1769-4a14-b2c7-fb196bc84f8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## approach - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854da9b6-5a11-42d1-a56f-3dbc5c3b3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "import panel as pn\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "pn.extension(design=\"material\")\n",
    "\n",
    "db = SQLDatabase(eng)\n",
    "# toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0))\n",
    "\n",
    "llm = OpenAI(temperature=0, streaming=True)\n",
    "# tools = load_tools([\"ddg-search\"])\n",
    "# tools = [generate_sql]\n",
    "# agent = initialize_agent(\n",
    "#     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True,\n",
    "# )\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", '''You will play the role of a mortgage adviser working with a client to create a mortgage quote. I will play the role of the client.'''),\n",
    "                    \n",
    "            (\"system\", '''As a mortgage advisor for the bank you have direct access to the banks data for me\n",
    "                    generate the SQL to get the details  like Balance, Income, CreditScore, etc. if user don't know the answer or Do not enter the answer.'''),\n",
    "                    \n",
    "            (\"system\", '''This information should compliment any answers you gain from the me. \n",
    "                    Ask a series of questions, no more than eight, sequentially - pausing between each question to wait for a response before proceeding to the next. '''),\n",
    "\n",
    "            (\"system\", '''Here's the first question: 'Income: What is your total annual income, including any additional sources of income?' '''),\n",
    "\n",
    "            (\"system\", '''And then follow this format for each subsequent question. Again, wait for my response before moving on to the next. '''),\n",
    "\n",
    "            (\"system\", '''Start the chat with greeting and ask the questions to user as mentioned above. '''),\n",
    "\n",
    "            (\"system\", '''For example:\n",
    "                    First question: What is your total annual income?\n",
    "                    Second question: Do you have any additional income?\n",
    "                    \n",
    "                    If user don't enter answer or say I don't know the call a function \"generate_sql\" \n",
    "                    and generate the SQL query\n",
    "                    \n",
    "                    '''),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "# --------------\n",
    "\n",
    "# llm = OpenAI(temperature=0, verbose=True)\n",
    "# db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "# db_chain.run(\"How many employees are there?\")\n",
    "\n",
    "# ---------------\n",
    "\n",
    "agent = AgentExecutor(agent=chain, tools=tools, verbose=False, memory=memory)\n",
    "# agent_kwargs = {\n",
    "#     \"system_message\": SystemMessage(content=\"You will play the role of a mortgage adviser working with a client to create a mortgage quote. I will play the role of the client. As a mortgage advisor for the bank you have direct access to the banks data for me                     generate the SQL to get the details  like Balance, Income, CreditScore, etc. if user don't know the answer or Do not enter the answer.\")\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# agent = create_sql_agent(\n",
    "#     llm=OpenAI(temperature=0),\n",
    "#     toolkit=toolkit,\n",
    "#     verbose=True,\n",
    "#     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     agent_kwargs=agent_kwargs,\n",
    "# )\n",
    "\n",
    "async def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(instance)\n",
    "    await agent.arun(contents, callbacks=[callback_handler])\n",
    "\n",
    "pn.chat.ChatInterface(callback=callback).servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94675fe4-a63d-4f85-88e7-2ae7d36a2e79",
   "metadata": {},
   "source": [
    "## Ask_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635924f-32f9-4838-8160-390c6af8c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj.qa({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c23171-2c78-4e4e-898f-8e599f632951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ai():\n",
    "    flag = True\n",
    "    obj = cbfs(tools)\n",
    "    obj.memory.clear()\n",
    "    while flag: \n",
    "        query = input(\"What do you want to ask? \")\n",
    "        \n",
    "        # stop asking questions if user type stop\n",
    "        if \"stop\" in query:\n",
    "            flag = False\n",
    "            break\n",
    "        result = obj.qa({\"input\": query})\n",
    "        # display(Markdown(f\"Response: <b>{result['output']}</b>\"))\n",
    "        response_template(query, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f62d4e-fa5c-4089-ad02-b2e4be59d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8dc76-f6bf-49fd-aa98-4bd1b1b99acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c4580-12b2-4520-b374-ec34c6394453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84653416-8c7b-47a6-8f37-089616b0c498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62835a-8163-484d-8b52-75f40f573c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df82c07-05d0-4630-ba74-7bd6b77955a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c78807-8f01-41fa-9a96-6b4b84dc3280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a65e8-9586-499f-be9f-47f650af5fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702f759-c410-416d-92e1-dcf115582e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80fe022-ae73-4ad9-9af3-bce2c61d0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "import panel as pn\n",
    "\n",
    "pn.extension(design=\"material\")\n",
    "\n",
    "llm = OpenAI(temperature=0, streaming=True)\n",
    "tools = load_tools([\"ddg-search\"])\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True,\n",
    ")\n",
    "\n",
    "async def callback(contents, user, instance):\n",
    "    callback_handler = pn.chat.langchain.PanelCallbackHandler(instance)\n",
    "    await agent.arun(contents, callbacks=[callback_handler])\n",
    "\n",
    "pn.chat.ChatInterface(callback=callback).servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4f055-502a-424f-b3e0-6ed64ba2476e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a6d31-e334-4c08-88b4-9752fce652e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50b8b817-c0ff-42b3-a651-c8268ac40942",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>5. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacb87c-0d81-40f0-8754-608d47e602cc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b398148-9486-4535-8c06-d3f77669bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_MarketingCamp');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4cbe-1beb-4c65-ac60-ffd6250668a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae971f90-bef1-4228-bcbf-c333e9843284",
   "metadata": {},
   "source": [
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Dataset:</b>\n",
    "\n",
    "- `customer_id`: Unique row customer id\n",
    "- `age`: customer age (numeric)\n",
    "- `profession` : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")\n",
    "- `marital` : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" meansdivorced or widowed)\n",
    "- `education` customer eduction (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
    "- `city`: city of customer (categorical: 'New York','Los Angeles','Chicago','Houston','Phoenix','Philadelphia','San Antonio','San Diego','Dallas','San Jose')\n",
    "- `monthly_income_in_thousand`: customer's monthly income, in dollar (numeric)\n",
    "- `family_members`: number of family members (numeric)\n",
    "- `communication_type`: communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n",
    "- `last_contact_day`: last contact day of the month (numeric)\n",
    "- `last_contact_month`: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "- `credit_card`: does customer have a credit card? (binary: 'yes','no')\n",
    "- `num_of_cars`: number of cars (numeric)\n",
    "- `last_contact_duration`: last contact duration, in seconds (numeric)\n",
    "- `campaign`: number of contacts performed during this campaign and for this client (categorical,includes last contact)\n",
    "- `days_from_last_contact`: number of days that passed by after the client was last contacted from a previouscampaign (numeric, -1 means client was not previously contacted)\n",
    "- `prev_contacts_performed`: number of contacts performed before this campaign and for this client (numeric)\n",
    "- `prev_campaign_outcome`: outcome of the previous marketing campaign (categorical:\"unknown\",\"other\",\"failure\",\"success\")\n",
    "- `payment_method`: payment method use by customer (categorical: 'cash','credit_card','debit_card','ewallets', 'payment_links', 'QRcodes')\n",
    "- `purchase_frequency`: how frequently customer is purchasing (categorical: 'daily','weekly','biweekly','monthly','quarterly','yearly')\n",
    "- `gender`: gender of customer? (binary: 'male','female')\n",
    "- `recency`: number of days since the last purchase (numeric)\n",
    "\n",
    "\n",
    "Output variable (desired target):\n",
    "- `purchased`: does customer did a purchase - target column (binary: 'yes','no')\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/search/all?query=Python+Package+User+Guide&content-lang=en-US'>here</a></li>\n",
    "    <li>Langchain Python reference: <a href='https://python.langchain.com/docs/get_started/introduction.html'>here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5463848-592f-4321-b852-287e133872dd",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Copyright © Teradata Corporation - 2023. All Rights Reserved.</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
