{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d484b2-fd8a-4e18-a341-715cac5ab0ea",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Chat Powered Product Recommendation based on Search Output\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6596b-33d1-4883-b9fe-6e205584d45c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#00233c'><b>Introduction:</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The combination of <b>FlagEmbedding</b> from HuggingFace and <b>Vantage in the db_function</b> assists consumers in receiving product recommendations while generating delicious recepies from the recommendations system using this generative AI demo.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo, we will build a product recommendation system using <b>FlagEmbedding</b> and Vantage in the db_function <b>VectorDistance</b>. Recommendation systems are a type of information filtering system that seeks to predict the rating or preference that a user would give to an item. They are often used on e-commerce websites to recommend products to users based on their past purchase history, browsing behavior, and other factors. In this demo, we use product-to-product recommendations based on embedding distances. The <b>VectorDistance</b> function will return the closest products from the databases as a recommendations.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The following diagram illustrates the architecture.</p>\n",
    "\n",
    "<center><img src=\"images/chat_powered_v5.png\" alt=\"Product_search_architecture\" width=1000 height=800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa766ed-f02a-41e9-92cd-1a0228352a4a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Before going any farther, let's get a better understanding of Cosine similarity(distance measure method) and Embeddings</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a68893-8b18-4015-b0a6-1840a6a39230",
   "metadata": {},
   "source": [
    "<ul style = 'font-size:16px;font-family:Arial;color:#00233C'><li> <b>Cosine similarity:</b></li></ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> &emsp;  &emsp; In natural language processing (NLP), a vector is a way of representing a word or phrase as a set of numbers. These numbers represent the meaning of the word or phrase in a way that can be understood by computers.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Cosine distance is a way of <b>measuring the similarity between two vectors</b>. It works by calculating the cosine of the angle between the two vectors. The cosine of an angle is a number between -1 and 1, where 0 means that the vectors are perpendicular, 1 means that they are pointing in the same direction and -1 means that they are pointing in the opposite directions</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>So, if you have two vectors that are very similar, the cosine of the angle between them will be close to 1. And if you have two vectors that are very different, the cosine of the angle between them will be close to 0.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Imagine you have a bunch of products, and you want to know how similar they are to each other. You could represent each product as a vector of numbers, where each number represents a different feature of the product. For example, you could have a vector for <b>cheese</b> that looks like this: <b>[0.6, -0.2, 0.8, 0.9, -0.1, -0.7]</b> Once you have represented each product as a vector, you can use cosine similarity to measure how similar they are.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example, the <b>The cosine of an angle would be close to 1 </b> between <b>cheese</b> and <b>butter, </b> because they have many similar features and they both are dairy products. However, the <b>The cosine of an angle would be close to 0 or less than 0</b> between <b>cheese and eggs</b>, because they are not as similar.</p>\n",
    "\n",
    "<center><img src=\"images/cosine.png\" alt=\"cosine\" width=1000 height=800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa706c-8370-4b0d-9c18-e89cc26fd3b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial;color:#00233C'><li> <b>Embeddings:</b></li></ul>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> &emsp;  &emsp; Embeddings are the A.I-native way to represent any kind of data, making them the perfect fit for working with all kinds of A.I-powered tools and algorithms. They can represent text, images, and soon audio and video. There are many options for creating embeddings, whether locally using an installed library, or by calling an API.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Imagine you have a bunch of words, and you want to find a way to represent them in a way that captures their meaning. One way to do this is to create a word embedding. A word embedding is a vector of numbers that represents the meaning of a word. The numbers in the vector are chosen so that words that are similar in meaning have similar vectors.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example, the word \"cheese\", \"butter\", \"chocolate\" and \"sauce\" might have a vector that looks like below:</p>\n",
    "\n",
    "<center><img src=\"images/word_embeddings.png\" alt=\"word_embeddings\"  width=1000 height=800/></center>\n",
    "\n",
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The numbers in this vector don't have any special meaning by themselves. They just represent the way that the word \"cheese\" is related to other words in the vocabulary.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can use word embeddings to find the similarity between words. For example, we can calculate the cosine similarity between the vector for \"cheese\" and the vector for \"butter\". The cosine similarity is a measure of how similar two vectors are, and it ranges from 0 to 1. A cosine similarity of 1 means that the two vectors are perfectly aligned, and a cosine similarity of 0 means that the two vectors are completely unrelated.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this case, the cosine similarity between the vector for \"cheese\" and the vector for \"butter\" would be very high. This is because the words \"cheese\" and \"butter\" are very similar in meaning. They are both foods that are made from milk, and they are both often used in cooking.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can also use word embeddings to find related words. For example, we can find all of the words that are similar in meaning to \"cheese\". This would include words like \"milk\", \"cream\", \"yogurt\", and \"feta\".</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Word embeddings are a powerful tool for natural language processing. They can be used for a variety of tasks, such as sentiment analysis, machine translation, and question answering.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Above is a visual representation of how word embeddings work</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Imagine a bunch of points in a high-dimensional space. Each point represents a word, and the position of the point in space represents the meaning of the word. Words that are similar in meaning will be close together in space, and words that are different in meaning will be far apart.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Now, imagine that we take a slice through this high-dimensional space. This slice will be a two-dimensional space, and the points in the two-dimensional space will represent the word embeddings. The distance between two points in the two-dimensional space will be a measure of the similarity between the two words.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this way, word embeddings can be used to represent the meaning of words in a way that is both compact and informative.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddd5cb-dbc3-4c9f-bb70-7e2ae7aa2c01",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233c'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>Generate the embeddings</li>\n",
    "    <li>Load the existing embeddings to DB</li>\n",
    "    <li>Calculate the VectorDistance using Teradata Vantage in-DB function</li>\n",
    "    <li>Display the recommended products for the users</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31aa4b-2c4d-4217-9961-4aa732c5bf7c",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>1. Configuring the environment</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534bc96-0f28-4c96-80a9-86cc814af478",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note:</b>The installation of the required libraries will take approximately <b>4 to 5 minutes</b> for the first-time installation. However, if the libraries are already installed, the execution will complete within 5 seconds.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements_opensource.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba04f9-32f6-461c-b951-badc91513178",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <i>The above statements will install the required libraries to run this demo. To gain access to installed libraries after running this, restart the kernel.</i></p>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note:</b> The above statements may need to be uncommented if you run the notebooks on a platform other than ClearScape Analytics Experience that does not have the libraries installed. If you uncomment those installs, be sure to restart the kernel after executing those lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b>0 0</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122293f-cd07-4b86-9285-0ce04744cb9d",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fe319-3920-440f-bcb7-e955170766d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm.notebook import *\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# teradata lib\n",
    "from teradataml import *\n",
    "import teradataml\n",
    "from teradataml import KMeans\n",
    "from teradataml.analytics.valib import *\n",
    "from teradataml import configure\n",
    "configure.val_install_location = \"val\"\n",
    "configure.byom_install_location = \"byom\"\n",
    "\n",
    "# helper functions\n",
    "from utils.sql_helper_func import *\n",
    "from utils.bedorck_helper_func import *\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "display.max_rows = 5\n",
    "\n",
    "display.print_sqlmr_query = False\n",
    "display.suppress_vantage_runtime_warnings = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d962be-1952-4b6c-9512-07a3a6ffbb77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i>The code above will download the necessary models to generate the embeddings required to run this demo. The initial download may take approximately 50-60 seconds minutes if you are running this demo for the first time in this environment. However, subsequent runs will be much faster since the models will already be available locally.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0c1ee-451d-4e6e-95dd-c8571cef9f1b",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>2. Connect to Vantage</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab749c23-3c8c-43c4-90df-d2c435a3b33a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f2af9-4af2-498c-ae00-4c0e1c96f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "execute_sql('''SET query_band='DEMO= Chat_powered_product_recommendation_based_on_search_output.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d529a9-f743-4bfd-bf95-24cdb00225ab",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4e45f-3f13-41b1-9e4e-2c8e0a61ebd3",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>2.3 Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have provided data for this demo on cloud storage. You can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. You may switch which mode you choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581b789-ed6b-4b68-a03c-ca3aad854e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_Grocery_Data_cloud');\"        # Takes 1 minute\n",
    "# %run -i ../run_procedure.py \"call get_data('DEMO_Grocery_Data_local');\"        # Takes 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc348393-5b07-4f58-b622-bbd4657a6716",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Adding Customer and Customer Transaction Tables</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {'customers': 'Customer_ID', 'customer_transaction': 'Transaction_ID'}\n",
    "for file in files_dict:\n",
    "    print(file, '|', files_dict[file])\n",
    "    df = pd.read_csv(os.path.join('./data/', f'{file}.csv'))\n",
    "    print('file: ', file)\n",
    "    print(df.shape)\n",
    "    copy_to_sql(df, table_name=file, primary_index=files_dict[file], if_exists='replace')\n",
    "    print('--'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b46836-bc67-4711-b9b3-21031adcf757",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Next is an optional step – if you want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78ebbd-0ef9-4474-8677-3ad35fdb8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e18f3d-a27c-4712-9c7a-5b0d0322993a",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>3. Data Exploration</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Product recommendation systems are a type of recommender system that suggests products to users based on the recipe what they asked for in the search box and their previous order history. To recommend products to users, we will use FlagEmbedding from HuggingFace and Vantage in db_function.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The data for this demo comes from the products table of Instacart. There are also a few other tables, such as orders, aisles, departments, and order_products_prior. However, for this demo, we will only use the products table.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The products table contains information about all of the products that are available on Instacart. This includes the product id, product name, etc. The table also includes the product's department and aisle, which can be used to group products together.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The other tables in the Instacart dataset contain additional information about orders, aisles, departments, and product purchases. However, for this demo, we will only focus on the products table.<p/>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Each row is a snapshot of data taken from the products table, Below are the list of columns in the product table:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> \n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li>product_id</li>\n",
    "    <li>product_name</li>\n",
    "    <li>aisle_id</li>\n",
    "   <li>department_id</li>\n",
    "\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The source data from <a href=\"https://www.kaggle.com/competitions/instacart-market-basket-analysis/data\">kaggle</a> is loaded in Vantage with table named <i>Products</i>.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b><i>*Please scroll down to the end of the notebook for detailed column descriptions of the dataset.</i></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928d357-c163-406d-91ea-f080951cb2e9",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>3.1 Examine the Products table</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Let's look at the sample data in the Products table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4a8ad-0d8f-4acd-a063-01f6d1305b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(in_schema(\"DEMO_Grocery_Data\", \"products\"))\n",
    "print(\"Data information: \\n\", tdf.shape)\n",
    "tdf.sort(\"product_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ee595-4cbc-4c6e-af53-b0a0d93e5651",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>There are approx 50K records in all, and there are 4 variables. Products are listed from different departments. We shall recommend the products to the user when user is searching for some items from the page.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a871a80-800b-4e70-9d3d-43cbcff71ec8",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>3.1.1 Analyze Number of products per aisle.</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Now, let's do some data exploration with aisle and number of products. \n",
    "A histogram of the number of products per aisle is a useful tool for understanding the distribution of products in a store. It can be used to identify aisles with a high or low number of products, as well as aisles with a wide or narrow range of products.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aff544-69f0-4335-b7f3-21543a452101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(df, x, y, color, title, x_title, y_title, width=1200, height=500):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    colors = px.colors.qualitative.Plotly[: len(df)]  # Generate colors for categories\n",
    "    # print(df.columns)\n",
    "    for i in range(1, len(df) + 1):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[df[\"aisle\"][i]],\n",
    "                y=[df[\"no_products\"][i]],  # Use the count of each category as y-value\n",
    "                name=df[\"aisle\"][i],\n",
    "                marker_color=colors[i - 1],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=x_title,\n",
    "        yaxis_title=y_title,\n",
    "        hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"),\n",
    "        hovermode=\"x unified\",\n",
    "        autosize=False,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        yaxis={\"categoryorder\": \"total ascending\"},\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93609c-a042-43f2-a1f3-e8a01f1f3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with cte1 as (\n",
    "SELECT aisle_id,\n",
    "       Count(product_id) AS no_products\n",
    "FROM   DEMO_Grocery_Data.products\n",
    "GROUP  BY 1)\n",
    "\n",
    "sel top 11 a.aisle_id, a.no_products, b.aisle from cte1 a join DEMO_Grocery_Data.aisles b on a.aisle_id = b.aisle_id\n",
    "ORDER  BY 2 desc\n",
    "\"\"\"\n",
    "df_aisle_id_products = DataFrame.from_query(query)\n",
    "df_aisle_id_products = df_aisle_id_products.to_pandas()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efcc77-3526-4013-a00d-843500c5044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histogram(\n",
    "    df_aisle_id_products,\n",
    "    x=\"aisle\",\n",
    "    y=\"no_products\",\n",
    "    color=\"aisle\",\n",
    "\n",
    "    title=\"Number of products by aisle\",\n",
    "    x_title=\"aisle\",\n",
    "    y_title=\"No of Products\",\n",
    "\n",
    "    width=1300,\n",
    "    height=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45abf1-1b43-4e86-8bf1-ec9c2f4662a2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Based on the graph presented above, it is evident that <b>Candy chocolate</b> boasts the highest number of products, with a total of 1,246 items. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e3ac8-250b-49ca-9b63-bfb2ba730d1f",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>3.1.2 Sample the data.</b></p>   \n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo will use <b>250 Samples from all the Departments</b>. This will allow us to test the system quickly. Once we have validated the system, we can then consider expanding it to include more products. Here, a quick look into the samples</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fd812-6af2-4985-a3c8-a08fe3f3b88d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>First we find out the number of departments from the entire dataset. This information helps us with the sampling later. As the results show, we have 21 Departments in total</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b7dad-5305-4ec8-8370-deb77dd98e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data from every department for 250 samples across various aisles\n",
    "tdf_sample = tdf.loc[tdf[\"department_id\"] == 0].head(10)\n",
    "print(tdf_sample.shape)\n",
    "print(\"Please Wait, Process Not Completed Yet\")\n",
    "\n",
    "for dept_id in range(1, 22):\n",
    "    tdf_sample1 = tdf.loc[tdf[\"department_id\"] == dept_id] # get the department\n",
    "    tdf_sample2 = tdf_sample1.sample(n=250, seed=10, id_column = \"product_id\") # sample 250 entries\n",
    "    tdf_sample2 = tdf_sample2.drop(['sampleid'], axis=1) # drop sampleid column from the sample result\n",
    "    tdf_sample = tdf_sample.concat(tdf_sample2) # concatinate the results (append)\n",
    "    \n",
    "print(tdf_sample.shape)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf969d6-9d96-4d68-bbb6-b9ecd10fa3e5",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>3.2 Do you want to generate the embeddings?</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have already generated embeddings for the snacks department and stored them in files.</p>\n",
    "\n",
    "<center><img src=\"images/decision_emb_gen.png\" alt=\"embeddings_decision\" width=300 height=300/></center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note: If you would like to skip the embedding generation step and move on to the next section, pleasse click  <a href=\"#section50\">here</a> to skip.</b></i></p>\n",
    "</div>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To save time, you can move to the already generated embeddings section. However, if you would like to see how we generate the embeddings, or if you need to generate the embeddings for a different dataset, then continue to the following section.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d40316-4837-4909-bbc4-9257feae93c5",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<a id='section4'></a>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>4. Generate the embeddings </b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.1 Generate the embeddings for product table</b></p>    \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To generate embeddings for a product table, we will use the FlagEmbedding repo from HuggingFace, which uses the <code>BAAI/bge-large-en-v1.5</code> model to generate embeddings. FlagEmbedding are a type of word embedding that can be used to represent products in a way that captures their semantic meaning. This means that the embeddings will reflect the meaning of the product name, not just the words that are used. Please refer to the <a href=\"https://huggingface.co/BAAI/bge-large-en-v1.5\"> Embeddings documentation</a> for more information about embeddings.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The FlagEmbedding takes a text string as input and returns a vector of numbers that represent the embedding. The length of the vector depends on the model that you are using. For example, the <code>BAAI/bge-large-en-v1.5</code> model returns a vector of 1024 numbers.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo, we will use <code>BAAI/bge-large-en-v1.5</code> as the model which is rank 12th in <a href=\"https://huggingface.co/spaces/mteb/leaderboard\"> MTEB leaderboard</a> (At the Time of writing)</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6115921-56bb-4717-88d7-9389ae805a26",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To generate the embeddings, we will call the <b>get_embeddings()</b> function. This function will convert the Teradata DataFrame to a Pandas DataFrame and generate the embeddings. Once the embeddings are generated, we will store them in separate columns so that we can pass them to the <b>VectorDistance()</b> function later on.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note: The embedding generation step is estimated to take approximately 1 hour to complete. If you prefer to skip this step and proceed to the next section, please click  <a href=\"#section50\">here</a> to skip.</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2565eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_emb_generator(table_name, file_name, chunksize=100):\n",
    "    wallclock_time_start = timeit.default_timer()\n",
    "\n",
    "    # delete the records\n",
    "    delete_emb_from_sql(table_name, eng)\n",
    "\n",
    "    # Read the data in chunks of 1000 rows\n",
    "    temp_df = pd.read_csv(file_name, chunksize=chunksize)\n",
    "\n",
    "    # Iterate over the chunks\n",
    "    for chunk in tqdm(\n",
    "        temp_df,\n",
    "        desc=\"Overall progress \",\n",
    "    ):\n",
    "        start = timeit.default_timer()\n",
    "        print(\"Data size in current chunk: \", chunk.shape)\n",
    "        df_chunk = get_embeddings_bedrock(df=chunk, txt_col_name='product_name')\n",
    "\n",
    "        copy_emb_to_sql(table_name=table_name, tdf=df_chunk)\n",
    "        print(f\"{df_chunk.shape[0]} products saved to sql \\n\")\n",
    "        end = timeit.default_timer()\n",
    "        print(f\"time taken for {df_chunk.shape[0]} products: {end - start}\")\n",
    "\n",
    "    wallclock_time_end = timeit.default_timer()\n",
    "    wallclock_time = wallclock_time_end - wallclock_time_start\n",
    "    print(\"wallclock time:\\t\", wallclock_time)\n",
    "    print(\"-\" * 50, \" complete \", \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1937bb1-814e-404a-ae60-8f7a16d20738",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note: Please be patient. It will take the code below about 1 hour to generate the embeddings for more than 6000 goods. Products are being passed in batches of 100, and after that, the embeddings are created and stored in Vantage. Therefore, getting through will take time. </b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048894e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_products = tdf_sample.to_pandas()\n",
    "# df_products.to_csv(\"df_products.csv\", index=False)\n",
    "\n",
    "# recursive_emb_generator(\n",
    "#     table_name=\"product_embeddings\", file_name=\"df_products.csv\", chunksize=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b317622-12d6-480c-9a27-66ab9e6a8fbf",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<a id='section42'></a>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.2 Display the product embeddings</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6961d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings = DataFrame(in_schema(\"demo_user\", \"product_embeddings\"))\n",
    "print(\"Data information: \\n\", product_embeddings.shape)\n",
    "product_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745d1d1-f7ed-46d5-b3db-55d63c6f90bf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can see that generated embeddings for all of the products are in vector of 1024 columns. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example: The generated embeddings for product name: <b>Chocolate Sandwich Cookies</b> consists of 1024 numbers and looks like:<br>\n",
    "<code>-0.008196\t0.012901\t0.008759\t-0.002950\t-0.019805\t-0.010412</code></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7470ae-ce98-4b66-9206-c2ba2c90150d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Now, we have generated the embeddings from the product names and saved the product embeddings dataframe into a vantage table named <b>product_embeddings_os</b> to use it further.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Exbeddings and convert to parquet file\n",
    "\n",
    "# product_embeddings_os.to_csv(\"product_Embeddings.csv\")\n",
    "# df = pd.read_csv('product_Embeddings.csv')\n",
    "# df.to_parquet('product_Embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f728c-de97-4fda-8f1f-46da7a4ddf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee006f2-f551-47a0-b055-940cbde8b0c3",
   "metadata": {},
   "source": [
    "<a id='section50'></a>\n",
    "\n",
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>5. Load the existing embeddings to DB</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>5.1 Load the products and searched products embeddings</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo, we will load existing embeddings from files to a database. This will allow us to perform further processing on the embeddings.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note</b>: If you have already executed the Generate the embeddings section, then below code will be skipped automatically.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49fd6e1-c637-4091-b522-54fa5591c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_section4_executed = False\n",
    "try:\n",
    "    is_section4_executed = (\n",
    "        DataFrame.from_query(\n",
    "            \"select count(*) as emb_cnt from product_embeddings\"\n",
    "        ).get_values()[0][0]\n",
    "        > 0\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21851d5a-561b-4c84-8e55-dd7da1c2137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def get_section5_desc_start():\n",
    "    return \"\"\"<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The code above first reads the data from the files. The files contain information about the product embeddings and the customer's searched product embeddings. The code then loads the data into a permanent table in SQL. Once the data is loaded, we will use the Vantage in-database function <code>VectorDistance</code> to calculate the distance between the product embeddings and the customer's searched product embeddings. The data contains product embeddings, which are lists of numerical values, or vectors.</p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>The embeddings file contains over 6,000 records, each with 1,028 numerical features. This means that the file is quite large and it may take some time to load it into SQL.</p>\n",
    "    <div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note</b>: Please be patient. The code above is loading data from files and copying it to SQL. This process may take 80-100 seconds.</i></p>\n",
    "</div>\"\"\"\n",
    "\n",
    "\n",
    "def get_section5_desc_end():\n",
    "    return \"\"\"<a id='section52'></a><p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>5.2 Display the product embeddings</b></p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>To give you a better idea of what the embeddings look like, here are the first five rows of the product embeddings:</p>\"\"\"\n",
    "\n",
    "\n",
    "def get_section5_desc_sample():\n",
    "    return \"\"\"<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can see that generated embeddings for all of the products are in vector of 1024 columns. </p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example: The generated embeddings for product name: <b>Chocolate Sandwich Cookies</b> consists of 1024 numbers and looks like:<br>\n",
    "    <code>-0.00819\t0.01290\t0.00875\t-0.00294\t-0.01980\t-0.01041</code></p>\"\"\"\n",
    "\n",
    "\n",
    "def load_the_emb():\n",
    "    is_section5_executed = False\n",
    "\n",
    "    if not is_section4_executed:\n",
    "        is_section5_executed = True\n",
    "        start = timeit.default_timer()\n",
    "        display(Markdown(get_section5_desc_start()))\n",
    "\n",
    "        # load product_embeddings to sql\n",
    "        df_product_embeddings_prq = pd.read_parquet(\n",
    "            \"./embeddings/product_embeddings.parquet\"\n",
    "        )\n",
    "        delete_and_copy_embeddings(\n",
    "            table_name=\"product_embeddings\",\n",
    "            tdf=df_product_embeddings_prq,\n",
    "            eng=eng,\n",
    "        )\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        load_time = end - start\n",
    "        print(f\"embeddings load time:\\t\", load_time)\n",
    "\n",
    "        display(Markdown(get_section5_desc_end()))\n",
    "        product_embeddings = DataFrame(\n",
    "            in_schema(\"demo_user\", \"product_embeddings\")\n",
    "        )\n",
    "        # display(Markdown(get_section5_desc_sample()))\n",
    "        return product_embeddings, is_section5_executed\n",
    "    else:\n",
    "        # print(\"Section 4: Generate the embeddings is already executed!\")\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"<br><div class=\"alert alert-block alert-success\">\n",
    "        <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i>Section 4: Generate the embeddings is already executed! So, skipping the execution of above code.</i></p></div>\"\"\"\n",
    "            )\n",
    "        )\n",
    "        return None, is_section5_executed\n",
    "\n",
    "\n",
    "product_embeddings, flag = load_the_emb()\n",
    "product_embeddings.sort(\"product_id\") if product_embeddings is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321d666-69ce-4c7b-a130-f5bc37aa6892",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The code below will not run if Section 5 has already been skipped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306b908-6305-41a6-bf58-2802a2ec12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(get_section5_desc_sample())) if flag else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a2f57-442d-4712-9145-4bcfd2c2d526",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>6. Calculate the K-Mean Clusters using Teradata Vantage in-DB function</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2adb4-f705-47f6-ba51-ba36edbd5156",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>gist -  Take the product embeddings and remove the followinf columns for clustering</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292ab96-0920-4117-96cd-1eb005a33ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_column = product_embeddings\n",
    "embedding_column_list = embedding_column.drop(columns=['product_id', 'product_name', 'aisle_id', 'department_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c6f93-2db7-45de-911e-26db319ef5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76832e4b-cbb4-45d0-b45d-d27e7dc853a7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "The below model table will be used in Kmeans predict after the chatbot gives us the ingredients</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1d3ae-ddef-45bf-999c-9f45dbedf638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Table\n",
    "query = f\"\"\"\n",
    "    SELECT * from TD_KMeans (\n",
    "    ON product_embeddings as InputTable\n",
    "    OUT TABLE ModelTable(KMeans_Model)\n",
    "    USING\n",
    "        IdColumn('product_id')\n",
    "        TargetColumns{tuple(embedding_column_list.columns)}\n",
    "        NumClusters(13)\n",
    "        OutputClusterAssignment('true')\n",
    ") AS dt;\n",
    "    \"\"\"\n",
    "execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50efb8-892b-460a-9557-0273fdfe8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_Model = DataFrame(in_schema(\"demo_user\", \"KMeans_Model\"))\n",
    "print(\"Data information: \\n\", KMeans_Model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a570cb-6ae3-4a94-8efe-0e118ffd02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Cluster info, used later for filtering\n",
    "# update cluster value from elbow method(code down below) \n",
    "query1 = f\"\"\"\n",
    "    SELECT * from TD_KMeans (\n",
    "    ON product_embeddings as InputTable\n",
    "    USING\n",
    "        IdColumn('product_id')\n",
    "        TargetColumns{tuple(embedding_column_list.columns) } \n",
    "        NumClusters(13)\n",
    "        OutputClusterAssignment('true')\n",
    ") AS dt;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcbee6-16bf-4d7d-b0c0-180730a7e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_out_sql = DataFrame.from_query(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23178c-33cb-458d-aae7-906716f4224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_embeddings_os.td_clusterid_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896344d-6fcc-4abf-bb54-2b8c8dfce0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings_os_cluster = product_embeddings.join(other=kmeans_out_sql, how=\"full\", on=\"product_id=product_id\", lprefix= \"L_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147a8f5-92c5-46ab-bd5b-828ca5b3c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings_os_cluster.td_clusterid_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ddb1f5-1889-46bf-b286-3bb1cbbdcbda",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>3.2 Dataframes for Customers and Customer_transactions</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f89cc-086d-485f-afe5-2a2b8ca973b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_customer = DataFrame(in_schema(\"demo_user\", \"customers\"))\n",
    "\n",
    "print(\"Data information: \\n\", tdf_customer.shape)\n",
    "tdf_customer.sort(\"Customer_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419a73b-7165-480d-9f55-e8f93de716eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_customer_transaction = DataFrame(in_schema(\"demo_user\", \"customer_transaction\"))\n",
    "\n",
    "print(\"Data information: \\n\", tdf_customer_transaction.shape)\n",
    "tdf_customer_transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc972656-e202-44b6-81ee-301f23589e59",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>7. Setup LLM </b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.1 Connect to database using SQLAlchemy and Initialize the Large Language Model</b></p>    \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Under the hood, LangChain uses SQLAlchemy to connect to SQL database. The SQLDatabaseToolkit can therefore be used with any SQL dialect supported by SQLAlchemy, such as Teradata Vantage, MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. Please refer to the <a href=\"https://docs.sqlalchemy.org/en/20/\"> SQLAlchemy documentation</a> for more information about requirements for connecting to your database. The SQLDatabaseToolkit builds off of SQLDatabaseChain and is designed to answer more general questions about a database, as well as recover from errors.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Important:</b> The code below establishes a database connection for data sources and Large Language Models. Please note that the solution will only work if the database connection for your sources is defined in the cell below. In addition to this, to use OpenAI models, we have to set OpenAI API key to environment variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c8094-dd6b-4c7a-b6ab-13e89b736927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "import boto3\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ae7ad-5a5a-43f5-8045-142782875f89",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo we will be using Google's <code>Gemini 1.0 pro</code> model as LLM. To view list of available models on Google <a href='https://ai.google.dev/models'>click here</a></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In Google's language models, the <b>temperature</b> parameter controls the randomness of the generated text. It affects the diversity and creativity of the model's responses. It is always a number between 0 and 1. A temperature of 0 means the responses will be very straightforward, almost deterministic (meaning you almost always get the same response to a given prompt). A temperature of 1 means the responses can vary wildly.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>A higher temperature value, such as 1.0, increases the randomness and diversity of the generated output. This can lead to more varied and surprising responses, but it may also result in less coherence and occasional nonsensical outputs. A higher temperature means that the model might select a word with slightly lower probability, leading to more variation, randomness and creativity. A very high temperature therefore increases the risk of <b>hallucination</b>, meaning that the model starts selecting words that will make no sense or be offt-opic.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>On the other hand, a lower temperature value, such as 0.2 or below, reduces randomness and makes the model's output more focused and deterministic. The generated text is likely to be more conservative, sticking closely to patterns observed in the training data. A temperature of 0 means roughly that the model will always select the highest probability word.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Choosing an appropriate temperature value depends on the desired output. Higher temperatures can be useful for creative tasks or brainstorming, while lower temperatures are preferred when you need more control over the output, such as when generating specific responses or following a particular style.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acafbf7a-6d31-4a1f-8b8e-9b7b38b8693d",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>7.2 Setup SQLAgent</b></p> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Before jumping into the chatbot, let us first understand what is an agent and why it might be preferred over a simple SQLChain.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>An agent is a component that has access to a suite of tools, including a Large Language Model (LLM). Its distinguishing characteristic lies in its ability to make informed decisions based on user input, utilizing the appropriate tools until it achieves a satisfactory answer. For example in the context of text-to-SQL, the LangChain SQLAgent will not give up if there is an error in executing the generated SQL. Instead, it will attempt to recover by interpreting the error in a subsequent LLM call and rectify the issue. Therefore, in theory, SQLAgent should outperform SQLChain in productivity and accuracy.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can think of agents as enabling tools for LLMs. Like how a human would use a calculator for maths or perform a Google search for information — agents allow an LLM to do the same thing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28179842-c298-4012-baa5-9ad5862e20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bedrock Clients\n",
    "bedrock=boto3.client(service_name=\"bedrock-runtime\")\n",
    "bedrock_embeddings=BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock)\n",
    "\n",
    "def get_llm():\n",
    "    # llama2 - meta.llama2-70b-chat-v1\n",
    "    llm=Bedrock(model_id=\"ai21.j2-mid-v1\",client=bedrock,\n",
    "                model_kwargs={'temperature': 0.1, 'maxTokens': 1500, \"stopSequences\":[\"##\"],\"countPenalty\":{\"scale\":0},\"presencePenalty\":{\"scale\":0}})\n",
    "    \n",
    "    return llm\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a0d81-0da0-456b-91df-ed7da881ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "custom_criterion = {\n",
    "    \"recipe\": \"Does the output contain ingredients and steps?\"\n",
    "}\n",
    "\n",
    "def check_for_recipe(str):\n",
    "    \"\"\"Check if the Input is a recipe or Not\"\"\"\n",
    "    evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=custom_criterion, llm=llm)\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=str,\n",
    "        input=\"Does this look like a recipe?\")\n",
    "    print(eval_result)\n",
    "    return eval_result[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5bfcf-6858-420e-9816-c0826b93363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingredient_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", '''\n",
    "#         You are a helpful AI bot to extract the ingredients from given receipe:\n",
    "#         Looking at the Input section 1 and only extract the ingredients. Do not try to undestand anything apart from ingredients.\n",
    "        \n",
    "#         Give the response in following Format\n",
    "#         Item 1, Item2, Item3, Item4, ....\n",
    "#             '''),\n",
    "#     (\"user\", \"{input}\"),\n",
    "# ])\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "ingredient_prompt = PromptTemplate.from_template(\n",
    "    '''You are a helpful AI bot to extract the ingredients from given receipe's Section 1: {input}. \n",
    "    Do not give quantity and Do not repeat the ingredient in the response.'''\n",
    ")\n",
    "\n",
    "ingredient_chain = ingredient_prompt | llm\n",
    "\n",
    "def get_ingredients(str):\n",
    "    \"\"\"Get the Ingredients from the Recipie Provided\"\"\"\n",
    "    answer1 = ingredient_chain.invoke({\"input\" : str})\n",
    "    ingredients_answer = Convert(answer1)\n",
    "    print(ingredients_answer)\n",
    "    return ingredients_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eaf186-6b20-4109-88a0-e30416c4e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''\n",
    "\n",
    "System:\n",
    "In Section 1:\n",
    "* Eggs: 2\n",
    "* Wine: 100 ml\n",
    "* Cheese: 50 g\n",
    "\n",
    "In Section 2:\n",
    "1. Preheat the oven to 180 degrees Celsius.\n",
    "2. In a mixing bowl, crack the eggs and whisk well.\n",
    "3. Add the cheese and wine to the eggs and mix well.\n",
    "4. Grease a baking dish and pour the egg mixture into the dish.\n",
    "5. Place the baking dish in the oven and bake for 20-25 minutes, or until the eggs are set.\n",
    "6. Serve the baked eggs warm, garnished with fresh parsley if desired.\n",
    "\n",
    "Enjoy your meal!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c69363-c4e6-4364-829a-9f0253331c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_ingredients(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa5300-4aba-4c61-88b1-2a46c3bb9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1a466-4a75-4021-bd53-70273222d7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a98af9-b063-4e1c-a526-bbe21a8976c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(string):\n",
    "    li = list(string.split(\", \"))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b43d17-53ae-42dc-9228-5db562188275",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the code above, we're initializing SQLAgent by passing in LLM, SQLDatabaseToolkit, database, and agent type as ZeroShotReactAgent. Agents use an LLM to determine the best course of action and execute it. An action can either be using a tool and observing its output, or responding to the user. <a href='https://python.langchain.com/docs/modules/agents/agent_types/'>Here</a> are the agents available in LangChain.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The agent used here is a <b>zero-shot-react-description</b> agent. Zero-shot means the agent functions on the current action only — it has no memory. It uses the <a href='https://arxiv.org/pdf/2210.03629.pdf'>ReAct</a> framework to decide which tool to use, based solely on the tool’s description.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28c983-783a-4318-84ac-3f023b7f6387",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>8. Launch the Chatbot</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo we are using ChatOpenAI model with Memory. This advanced technology allows us to store and recall conversations, enabling our chatbot to provide more personalized and informed responses.As a mortgage advisor, our chatbot is trained to assist with a wide range of loan-related inquiries. Whether you're looking to purchase a new home, refinance an existing loan, or simply have questions about the mortgage process, our chatbot is here to help.To begin, we'll set the prompt for our chatbot to work as a mortgage advisor. This will enable it to provide tailored advice and guidance based on your unique needs and circumstances.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1888c8-7835-4a87-b505-26bc3f56ce59",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note:</b>Please note that our chatbot is specifically designed to address questions related to mortgage loans. If you have a question outside of this scope, we kindly ask that you refrain from asking it. This will help us provide you with the most accurate and efficient assistance possible.</i></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb4baf-972f-47c4-80ba-a15d9046b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory import (\n",
    "    ConversationSummaryMemory,\n",
    "    ChatMessageHistory,\n",
    "    ConversationSummaryBufferMemory,\n",
    ")\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''\n",
    "            As a seasoned culinary expert, I am delighted to offer my assistance in providing delicious recipe suggestions \n",
    "            along with detailed cooking instructions. Upon your request, I will promptly provide a list of fresh and flavorful \n",
    "            ingredients, as well as easy-to-follow very Detailed steps with all the details for creating delectable dishes that are sure to impress even the most \n",
    "            discerning palates. Let's get started!\n",
    "            My response comes in the format of \n",
    "            # Recipe Name\n",
    "            ## Section 1: Ingredients with Quantity\n",
    "                        - Ingredient 1: Quantity\n",
    "                        - Ingredient 2: Quantity\n",
    "                        - Ingredient 3: Quantity\n",
    "                        - ...\n",
    "            ## Section 2: Cooking Instructions\n",
    "                        1. Step 1 of the cooking process.\n",
    "                        2. Step 2 of the cooking process.\n",
    "                        3. Step 3 of the cooking process.\n",
    "                        ...\n",
    "            In Section 1: All the Ingredients with their respective quantities\n",
    "            In Section 2: Cooking recipe with excruciating detail\n",
    "            \n",
    "            I have Expertise Only in the Following Domain: Culinary Knowledge, Food Science, Cooking Tips and Nutritional Knowledge\n",
    "            I will not answer any other question outside of my domain and reply with \"I cannot Help you with this query, Please ask another question related to Culinary, Food Science, Cooking Tips and Nutritional Knowledge\"\n",
    "            \n",
    "            Do not include \"System:\" in the response.\n",
    "            What kind of dish would you like to prepare today?\n",
    "            '''),\n",
    "            (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = ( prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500785c-dba8-4f3b-b173-bbe7d60c00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "from panel.chat import ChatMessage\n",
    "pn.extension(design=\"material\")\n",
    "\n",
    "#clear the memory\n",
    "# memory.clear()\n",
    "\n",
    "# Starting Greeting message \"Hi, I am a recipe bot\"\n",
    "# Only keep section 2 with quantities and remove section 1 from response template\n",
    "\n",
    "def send_checkboxes(content):\n",
    "    global ingredients\n",
    "    ingredients = get_ingredients(content)\n",
    "    global checkbox_group\n",
    "    checkbox_group = pn.widgets.CheckBoxGroup(\n",
    "                name='Checkbox Group', options=ingredients,\n",
    "                    inline=False)\n",
    "    chat_interface.send(checkbox_group, user=\"System\", respond=False)\n",
    "\n",
    "def submit_values(instance, event):\n",
    "    global values_checkbox\n",
    "    values_checkbox = checkbox_group.value\n",
    "    print(values_checkbox)\n",
    "    instance.send(\"Values Submitted, Please Continue with the Notebook\", respond=False, user=\"System\")\n",
    "\n",
    "\n",
    "def callback(contents, user, instance):\n",
    "    answer = chain.invoke({\"input\": contents})\n",
    "    if check_for_recipe(answer):\n",
    "        send_checkboxes(answer)\n",
    "        return answer\n",
    "    else :\n",
    "        return answer\n",
    "\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=callback,\n",
    "    show_rerun=False,\n",
    "    show_undo=False,\n",
    "    show_clear=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    callback_exception='verbose',\n",
    "    button_properties={\"Submit\": {\"callback\": submit_values, \"icon\": \"help\"}}\n",
    ")\n",
    "chat_interface.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3915d8d-3e1c-47da-8606-4a15a3b4b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_checkbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9cb1f-31b1-4341-b74b-6d0d76e0a4c1",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>8. Calculate the VectorDistance using Teradata Vantage in-DB function</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6d416-76c4-43d2-bdd0-2259feb42f61",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The TD_VectorDistance function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The function computes the distance between the target pair and the reference pair from the same table if you provide only one table as the input.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91765b5-e0af-435c-8173-ff09bbb2528c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The VectorDistance function calculates the distance between a target vector and a reference vector. We use the cosine distance metric, which measures the similarity between two vectors. The function can return the maximum of 1 to 100 closest reference vectors to include in the output table for each target vector. In this demo, we want the top 2 closest reference vectors to the target vector.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The VectorDistance function have a parameter <b>distance_measure</b>. You can pass anyone from the below list. Default value is cosine.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li><b>Cosine distance measures</b> the similarity between two vectors by calculating the cosine of the angle between them. It is a good measure of similarity for high-dimensional data, as it is not affected by the magnitude of the vectors.</li>\n",
    "    <li><b>Euclidean distance measures</b> the distance between two points in a Euclidean space. It is the most common distance measure, and it is a good measure of similarity for low-dimensional data.</li>\n",
    "    <li><b>Manhattan distance measures</b> the distance between two points in a Manhattan space. It is similar to Euclidean distance, but it uses the absolute value of the difference between the coordinates instead of the square of the difference.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51aa88e-40ed-45ec-8f14-ecd8835f14d4",
   "metadata": {},
   "source": [
    "<center><img src=\"images/distance_measure.png\" alt=\"distance_measure\"  width=600 height=600/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc073c1-a651-4177-b035-d9a1b106d96e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The below function, <code>TD_VECTORDISTANCE</code>, will take the target table, reference table, embedding column names, and number of recommendations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59072271-8a8c-4dff-89ac-6809ce141985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_distance2(target_table, reference_table, emb_column_names, topk):\n",
    "    start = timeit.default_timer()\n",
    "    VectorDistance_out = VectorDistance(target_id_column=\"product_id\",\n",
    "                                        target_feature_columns=emb_column_names,\n",
    "                                        ref_id_column=\"product_id\",\n",
    "                                        ref_feature_columns=emb_column_names,\n",
    "                                        distance_measure=['Cosine'],\n",
    "                                        topk=topk,\n",
    "                                        target_data=target_table,\n",
    "                                        reference_data=reference_table)\n",
    "\n",
    "    print(f'vector-distance calcuatation time:\\t',  timeit.default_timer() - start)\n",
    "    return VectorDistance_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d001ae-cced-4e9f-9d03-0886638524ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note</b>: by default, we suggest 3 recommendations for each searched product. If you want to change this, you can update the value of the <code>number_of_recommendations</code> variable.</i></p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Please be aware that calculating vector distances may take approximately 80-100 seconds. This is due to the fact that our platform is still small and we are employing advanced mathematical algorithms to determine the cosine distance between products. This process can be computationally intensive, resulting in a slightly longer processing time.</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7700b9-8fc7-4b6a-a9c3-b7cae7496ae2",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b> 8.3 Get the embedding for the selected Items</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbbe6e-aad7-4b91-8cd2-347427c69966",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Let's take <b>the selected products</b> to check their recommended products from our database. To do this, we need to follow the same process as before: generate the embeddings for the products and store them back to the Vantage table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41acefc-ab50-4e03-abbd-64a1673d3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from pandas to teradataml \n",
    "import pandas as pd\n",
    "Search_products = pd.DataFrame(values_checkbox)\n",
    "Search_products = Search_products.rename(columns={0:'product_name'})\n",
    "Search_products.insert(0, 'product_id', range(1, 1 + len(Search_products)))\n",
    "Search_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7810a8-1c62-43ad-af13-48b75eb5fc08",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The get_embeddings() function uses the FlagEmbedding's bge-large-en-v1.5 model to generate the embeddings.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fceb7-36a8-4639-957c-529b425f9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "df_search_products = get_embeddings_bedrock(Search_products, txt_col_name='product_name')\n",
    "end = timeit.default_timer()\n",
    "load_time = end - start\n",
    "print(\n",
    "    f\"generate the embeddings for {df_search_products.shape[0]} search products:\\t\",\n",
    "    load_time,\n",
    ")\n",
    "print(\"----- complete -----\")\n",
    "\n",
    "# Print the DataFrame.\n",
    "df_search_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1a92d-f406-4f3c-aa73-d472eb9a020c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Since the product names were searched, we have now generated the embeddings. The product embeddings dataframe must therefore be saved into a new table called <b>search_product_embeddings_os</b> before we can utilize it further..</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d352f26-1523-49e7-a0f9-c21fe8b238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql helper function \n",
    "copy_to_sql(df_search_products, table_name=\"search_product_embeddings\",primary_index='product_name', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a68e4-c575-416e-a4cc-2ce9990375a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_product_embeddings_os = DataFrame(in_schema(\"demo_user\", \"search_product_embeddings\"))\n",
    "\n",
    "print(\"Data information: \\n\", search_product_embeddings_os.shape)\n",
    "search_product_embeddings_os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2668e3-d5e1-4e9b-acb4-da7f3a0c1924",
   "metadata": {},
   "source": [
    "## Figure out the clusters for the selected items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabaab3-b991-4500-9d25-8b1c0d3e64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEans Predict\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM TD_KMeansPredict (\n",
    "        ON search_product_embeddings AS InputTable\n",
    "        ON KMeans_Model AS ModelTable DIMENSION\n",
    "        USING\n",
    "            OutputDistance('true')\n",
    "            Accumulate('product_name')\n",
    "        ) AS dt;\n",
    "    \"\"\"\n",
    "# execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301cab8-d2db-4965-8ff3-2c30aa7853b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_predict_out_sql = DataFrame.from_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f69e00-304e-4bd2-abe1-8cf6f15f22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_predict_out_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc1b4c-29c0-42df-a1cd-129d22e1adb8",
   "metadata": {},
   "source": [
    "## Filter data only from the selected clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6772f-c43c-4ec2-8e3a-1a7c4be6bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = kmeans_predict_out_sql.get_values()[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ff23d-37c8-420a-a434-91d020219a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_list = vals.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0dad05-ec19-4698-8ee1-78495aa9010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1bcde-bb19-4190-a9a8-799f033a4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_embeddings_os_filtered = product_embeddings_os_cluster.loc[vals in \"td_clusterid_kmeans\",:]\n",
    "product_embeddings_os_cluster_filtered = product_embeddings_os_cluster.loc[product_embeddings_os_cluster['td_clusterid_kmeans'].isin(vals_to_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc17ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings_os_cluster_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(product_embeddings_os_cluster_filtered, table_name=\"product_embeddings_os_cluster_filtered\",primary_index='product_name', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27437b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_product_embeddings_os_cluster_filtered = DataFrame(in_schema(\"demo_user\", \"product_embeddings_os_cluster_filtered\"))\n",
    "\n",
    "print(\"Data information: \\n\", tdf_product_embeddings_os_cluster_filtered.shape)\n",
    "tdf_product_embeddings_os_cluster_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80e169-e0df-4511-a7b1-16c7cc4ec1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_product_embeddings_os_cluster_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399d9fa-3a36-4a60-ad3c-28e299a020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_column_names = DataFrame(\n",
    "    in_schema(\"demo_user\", \"search_product_embeddings_os\")\n",
    ").columns[4:]\n",
    "\n",
    "# select top matching\n",
    "number_of_recommendations = 3\n",
    "\n",
    "vector_distance_df = calculate_vector_distance2(\n",
    "    target_table= search_product_embeddings_os,\n",
    "    reference_table = tdf_product_embeddings_os_cluster_filtered,\n",
    "    emb_column_names=emb_column_names,\n",
    "\n",
    "    topk=number_of_recommendations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25939903-ac22-4c4d-becb-1c87ba5ceaa9",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>9. Display the recommended products for the users.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96ef74-4d19-4b5b-ba26-b1459d35204f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To view the recommendations, we need to join two tables together. First, we will join the vector distance result table with the product embeddings table. This will give us a table that contains the vector distance scores for each product, as well as the product embeddings. Then, we will join this table with the search products table. This will give us a final table that contains the recommendations for the search products.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_recommendations2(\n",
    "    vector_distance_df, product_embeddings_df, search_product_embeddings_df\n",
    "):\n",
    "    product_embeddings_df_selected_columns = (product_embeddings_df.select([\"product_id\", \"product_name\"]))\n",
    "\n",
    "    # join vector-distance results and products\n",
    "    vec_prod_join_result = vector_distance_df.merge(right = product_embeddings_df_selected_columns, left_on = \"reference_id\", right_on=\"product_id\", lsuffix = \"t1\", rsuffix = \"t2\")\n",
    "\n",
    "\n",
    "    # join the above joined table with search products\n",
    "    vec_prod_join_result_selected = vec_prod_join_result[[\"product_id\", \"product_name\", \"target_id\", \"distancetype\", \"distance\"]]\n",
    "\n",
    "\n",
    "    # join_result_sorted_selected\n",
    "    df_search_products_selected = (search_product_embeddings_df.select([\"product_id\", \"product_name\"]))\n",
    "\n",
    "    # recommandation results\n",
    "    df_recommandations = df_search_products_selected.merge(right=vec_prod_join_result_selected,\n",
    "        left_on=\"product_id\",\n",
    "        right_on=\"target_id\",\n",
    "        how=\"inner\",\n",
    "        lsuffix=\"_search\", rsuffix = \"_recommended\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # filter with extact match\n",
    "    df_recommandations = df_recommandations[df_recommandations.distance > 0.001]\n",
    "    \n",
    "    # sort by distance\n",
    "    df_recommandations = df_recommandations.sort([\"_search_product_id\", \"distance\"], ascending=True)\n",
    "\n",
    "    return df_recommandations[\n",
    "        [\n",
    "            \"_search_product_id\",\n",
    "            \"_search_product_name\",\n",
    "            \"_recommended_product_id\",\n",
    "            \"_recommended_product_name\",\n",
    "            \"distance\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9e238-893e-4e57-ad0e-096ee3062e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings_df = DataFrame(in_schema('demo_user', 'product_embeddings_os_cluster_filtered'))\n",
    "search_product_embeddings_df = DataFrame(in_schema('demo_user', 'search_product_embeddings_os'))\n",
    "\n",
    "# get topk final recommendations for each searched products\n",
    "df_recommandations = get_final_recommendations2(\n",
    "    vector_distance_df, product_embeddings_df, search_product_embeddings_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8e588-2aa8-47a8-8da3-01a87cab119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommandations.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e20b6-b4be-4088-81d2-0146b1f1cca2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the table above, we can see the recommendations for the products searched by the user. The cosine distance between the searched and recommended products is also shown. Note that a few products have a cosine distance of zero. This is because the cosine distance is calculated by comparing the vectors of the two products. If the two products are the same, then the cosine distance will be zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19a522-0b72-44f4-8510-3a11a969382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me chicken bhiryani recipe\n",
    "# I have rice, tomatoes and onion what can i make\n",
    "# Give me the ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e181ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. K-Means Clustering (Finding Cluster Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df3fe0-8598-4378-847c-829b7ed443be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn import KMeans\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388de7b-0671-4db1-8534-b270fe0867ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_column_list = embedding_column.drop(columns=['product_id', 'product_name', 'aisle_id', 'department_id'])\n",
    "visualizer.fit(embedding_column_list.to_pandas())        # Fit the data to the visualizer\n",
    "visualizer.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d4bff-cd8e-4749-aa93-abb1396d7f5e",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fe831-ace9-43d4-82cd-bd7e63767a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1870a43-fb5e-4b98-8964-02b7f4200713",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_drop_table(table_name = 'product_embeddings_os')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccc51e-cc36-4635-88f5-3173d3996c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO be updated with all the table names\n",
    "\n",
    "tables = [\n",
    "    \"product_embeddings\",\n",
    "    \"customers\",\n",
    "    \"customer_transactions\",\n",
    "    \"clusters\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a9ff0-1100-484d-b97d-cc3f8a50b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_Grocery_Data');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4c18d-82be-4314-adde-7bc387f2328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a0c26-03e2-4115-b44f-d85531600b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
