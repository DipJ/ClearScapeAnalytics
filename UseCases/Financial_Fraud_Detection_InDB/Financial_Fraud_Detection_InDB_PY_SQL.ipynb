{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Financial Fraud Detection with In-Database Machine Learning</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Introduction:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    <b>ABC Bank</b> is a global bank that offers financial services to millions of customers worldwide. They have been experiencing a significant increase in fraud incidents, resulting in financial losses and customer dissatisfaction. As a result, <b>ABC Bank</b> has decided to seek a solution to improve their fraud detection capabilities.\n",
    "<br>\n",
    "<br>\n",
    "After conducting a thorough evaluation of their current fraud detection system and analyzing historical transaction data, <b>ABC Bank</b> has identified the need for a robust and accurate fraud detection solution. They approach us for assistance in developing a solution using machine learning techniques.\n",
    "    <br>\n",
    "    <br>\n",
    "    This notebook provides a demonstration of \"data science workflow\" that illustrates how to leverage <b>teradataml</b> package to build, validate and score a model at scale in Vantage without moving the data. Users can perform large-scale operations such as feature analysis, data transformation, Model training and ML Model Scoring in the Vantage environment without moving data.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>Data Preparation</li>\n",
    "    <li>In-Database Machine Learning</li>\n",
    "    <li>In-Database Model Scoring</li>\n",
    "    <li>Visualize the results</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>1. Configuring the Environment</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "\n",
    "# Data Manipulation and Visualization Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Teradata Libraries\n",
    "from teradataml import *\n",
    "\n",
    "# Machine Learning Metrics and Visualizations\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Configuration\n",
    "display.max_rows = 5\n",
    "configure.val_install_location = 'val'\n",
    "\n",
    "# Suppress Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Magic Command for Inline Plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>2. Connect to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, then use down arrow to go to next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username = 'demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=Financial_Fraud_Detection_InDB_PY_SQL.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. You can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. You may switch which mode you choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('demo_glm_fraud_cloud');\"        # Takes 1 minute\n",
    "%run -i ../run_procedure.py \"call get_data('demo_glm_fraud_local');\"        # Takes 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step â€“ if you want to see status of databases/tables created and space used. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>3. Data Exploration</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The data from <a href = 'https://www.kaggle.com/datasets/ealaxi/paysim1'>https://www.kaggle.com/datasets/ealaxi/paysim1</a> is loaded in Vantage in a table named \"transaction_data\". Check the data size and print sample rows: 63k rows and 12 columns.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b><i>*Please scroll down to the end of the notebook for detailed column descriptions of the dataset.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_data = DataFrame(in_schema('DEMO_GLM_Fraud','transaction_data'))\n",
    "\n",
    "print(txn_data.shape)\n",
    "txn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.1 Renaming columns</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Here we rename a misspelt column without moving the data out of Vantage. We are renaming <b>oldbalanceOrg</b> to <b>oldbalanceOrig</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_data = txn_data.assign(oldbalanceOrig = txn_data.oldbalanceOrg).drop(['oldbalanceOrg'], axis=1)\n",
    "\n",
    "txn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Fraudulent agents inside a simulation make these transactions. In this specific dataset, the fraudulent behaviour of the agents aims to profit by taking control or customers' accounts and trying to empty the funds by transferring them to another account and then cashing out of the system.</p>\n",
    "<!-- <p style = 'font-size:16px;font-family:Arial'><b>Below are some insights about the dataset:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>There are 92 fraud transactions i.e. 0.14% of fraud transactions in the dataset.</li>\n",
    "    <li>From these 92 fraud transactions, 47 are of type TRANSFER and 45 are of type CASH_OUT.</li>\n",
    "    <li>97.83% of fraud transations have transaction amount equal to oldbalanceOrig i.e. account cleanout.</li>\n",
    "    <li>71.74% of fraud transactions have recipient's old balance as zero.</li>\n",
    "    <li>isFlaggedFraud is correct only two times among the 92 fraud transactions.</li>\n",
    "</ol> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.2 How many fraudulent transactions do we have in our dataset?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 92 fraud transactions i.e. 0.14% of fraud transactions in the dataset.\n",
    "print(\"No of fraud transactions: %d\\nPercentage of fraud transactions: %.2f%%\"%(\n",
    "    txn_data.loc[txn_data.isFraud == 1].shape[0],\n",
    "    txn_data.loc[txn_data.isFraud == 1].shape[0]/txn_data.shape[0]*100)\n",
    ")\n",
    "\n",
    "# Calculate percentage of fraud transactions\n",
    "fraud_transactions_count = txn_data.loc[txn_data.isFraud == 1].shape[0]\n",
    "total_transactions_count = txn_data.shape[0]\n",
    "fraud_percentage = fraud_transactions_count / total_transactions_count * 100\n",
    "\n",
    "# Create a pie chart with Plotly\n",
    "fig = px.pie(values = [fraud_percentage, 100-fraud_percentage],\n",
    "             labels = [\"Fraud Transactions\", \"Non-Fraud Transactions\"],\n",
    "             names = [\"Fraud Transactions\", \"Non-Fraud Transactions\"],\n",
    "             color_discrete_sequence = ['lightgreen', 'red'],\n",
    "             hover_name = [\"Fraud Transactions\", \"Non-Fraud Transactions\"],\n",
    "             hole = 0.6)\n",
    "\n",
    "# Update layout\n",
    "fig.update_traces(textposition = 'inside', textinfo = 'percent+label')\n",
    "fig.update_layout(title_text = 'Percentage of Fraud Transactions')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.3 How many fraudulent transactions do we have group by transaction type?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for fraud transactions and group by 'type'\n",
    "fraud_transactions_by_type = txn_data.loc[txn_data.isFraud == 1].groupby('type').count().get(['type','count_step']).to_pandas()\n",
    "\n",
    "\n",
    "# Sort by 'count_step' column in descending order\n",
    "fraud_transactions_by_type = fraud_transactions_by_type.sort_values('count_step', ascending = False)\n",
    "\n",
    "# Create a bar chart with Plotly\n",
    "fig = px.bar(data_frame = fraud_transactions_by_type,\n",
    "             x = 'type',\n",
    "             y = 'count_step',\n",
    "             color = 'type',\n",
    "             color_discrete_sequence = px.colors.qualitative.Pastel,\n",
    "             hover_name = 'type',\n",
    "             text = 'count_step')\n",
    "\n",
    "# Update layout\n",
    "fig.update_traces(textposition = 'inside')\n",
    "fig.update_layout(title_text = 'Distribution of Fraud Transactions by Type',\n",
    "                  xaxis_title = 'Transaction Type',\n",
    "                  yaxis_title = 'Count')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>3.4 What percentage of fraudulent transactions do we have where transaction amount is equal to old balance in the origin account?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This might be the case where the fraudster emptied the account of the victim.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of cleanout fraud transactions: %d\\nPercentage of cleanout fraud transactions: %.2f%%\"%(\n",
    "    txn_data.loc[txn_data['amount'] == txn_data.oldbalanceOrig].loc[txn_data['isFraud'] == 1].shape[0],\n",
    "    txn_data.loc[txn_data['amount'] == txn_data.oldbalanceOrig].loc[txn_data['isFraud'] == 1].shape[0] / txn_data.loc[txn_data.isFraud == 1].shape[0]*100)\n",
    ")\n",
    "\n",
    "# Calculate percentage of fraud transactions\n",
    "cleanout_fraud_transactions_count = txn_data.loc[(txn_data.isFraud == 1) and (txn_data.amount == txn_data.oldbalanceOrig)].shape[0]\n",
    "fraud_transactions_coun = txn_data.loc[(txn_data.isFraud == 1)].shape[0]\n",
    "fraud_percentage = cleanout_fraud_transactions_count / fraud_transactions_coun * 100\n",
    "\n",
    "# Create a pie chart with Plotly\n",
    "fig = px.pie(values = [fraud_percentage, 100-fraud_percentage],\n",
    "             labels = [\"Cleanout Fraud Transactions\", \"Non-Cleanout Fraud Transactions\"],\n",
    "             names = [\"Cleanout Fraud Transactions\", \"Non-Cleanout Fraud Transactions\"],\n",
    "             color_discrete_sequence = ['pink', 'red'],\n",
    "             hover_name = [\"Cleanout Fraud Transactions\", \"Non-Cleanout Fraud Transactions\"],\n",
    "             hole = 0.6)\n",
    "\n",
    "# Update layout\n",
    "fig.update_traces(textposition = 'inside', textinfo = 'percent+label')\n",
    "fig.update_layout(title_text = 'Percentage of Cleanout Fraud Transactions')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>4. Data Preparation</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>We'll perform the following steps:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>One-hot encoding categorical \"type\" column</li>\n",
    "    <li>Feature scaling using ScaleFit and ScaleTransform on numerical columns.</li>\n",
    "    <li>Splitting the data in training and testing datasets (80:20 split)</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>ScaleFit outputs a table of statistics used as an input to ScaleTransform, which scales specified input table columns. ScaleTransform scales specified input table columns using ScaleFit output.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Feature scaling is performed during data pre-processing to handle highly varying magnitudes, values, or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values higher and consider smaller values as lower ones, regardless of the unit of the values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>4.1 Drop unnecessary columns</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>nameDest and nameOrigin are not required as we have txn_id to uniquely identify each transaction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_data = txn_data.drop(['nameDest', 'nameOrig'], axis = 1)\n",
    "txn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>4.2 One-hot encoding</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we are one-hot encoding the \"type\" column. One-hot encoding is necessary in many cases to represent categorical variables as binary values, enable numerical processing, ensure feature independence, handle non-numeric data, and improve the performance and interpretability of machine learning models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_type_encoder = OneHotEncoder(values = [\"CASH_IN\", \"CASH_OUT\", \"DEBIT\", \"PAYMENT\", \"TRANSFER\"], columns = \"type\")\n",
    "\n",
    "retain = Retain(columns = ['step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig', 'isFlaggedFraud', 'isFraud'])\n",
    "\n",
    "obj = valib.Transform(data = txn_data,\n",
    "                      one_hot_encode = txn_type_encoder,\n",
    "                      retain = retain,\n",
    "                      index_columns = 'txn_id')\n",
    "txn_trans = obj.result\n",
    "txn_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>4.3 Feature Scaling</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we are using ScaleFit and ScaleTransform for scaling the numerical columns using Standard Deviation as scale method.\n",
    "<br><br>\n",
    "Feature scaling is important in machine learning to avoid numerical instability, ensure fair comparison of features, improve model performance, enhance interpretability, and handle distance-based algorithms.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import ScaleFit, ScaleTransform\n",
    "\n",
    "sf_fit = ScaleFit(data = txn_trans, scale_method = 'STD',\n",
    "                     target_columns = ['step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig'])\n",
    "\n",
    "sf_trns = ScaleTransform(data = txn_trans, object = sf_fit.output, accumulate = [\"txn_id\", \"isFraud\", 'isFlaggedFraud',\n",
    "                                                                                 'CASH_IN_type', 'CASH_OUT_type', 'DEBIT_type',\n",
    "                                                                                 'PAYMENT_type', 'TRANSFER_type'])\n",
    "\n",
    "copy_to_sql(sf_trns.result, table_name = 'clean_data', if_exists = 'replace')\n",
    "sf_trns.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above output shows that the data has transformed into a scaled dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>4.4 Train-test split</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Splitting the data into train-test datasets in 80:20 ratio.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CREATE MULTISET TABLE TrainTestSplit_output AS (\n",
    "    SELECT * FROM TD_TrainTestSplit(\n",
    "        ON clean_data AS InputTable\n",
    "        USING\n",
    "        IDColumn('txn_id')\n",
    "        trainSize(0.75)\n",
    "        testSize(0.25)\n",
    "        Seed(7)\n",
    "    ) AS dt\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('TrainTestSplit_output')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CREATE MULTISET TABLE clean_data_train AS (\n",
    "    SELECT * FROM TrainTestSplit_output WHERE TD_IsTrainRow = 1\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('clean_data_train')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CREATE MULTISET TABLE clean_data_test AS (\n",
    "    SELECT * FROM TrainTestSplit_output WHERE TD_IsTrainRow = 0\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('clean_data_test')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame('clean_data_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>5. In-Database Machine Learning</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>5.1 Generalized Linear Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The GLM function is a generalized linear model (GLM) that performs regression and classification analysis on data sets, where the response follows an exponential or binomial family distribution.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Due to gradient-based learning, the function is highly sensitive to feature scaling. Input features should be standardized, such as using ScaleFit, and ScaleTransform, before using them in the function. The function takes only numeric features. We must convert the categorical features to numeric values before training. The function skips the rows with missing (null) values during training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CREATE MULTISET TABLE glm_model AS (\n",
    "    SELECT * FROM TD_GLM (\n",
    "        ON clean_data_train\n",
    "        OUT TABLE MetaInformationTable(glm_out) \n",
    "        USING\n",
    "            InputColumns('[3:13]')\n",
    "            ResponseColumn('isFraud')\n",
    "            Family('Binomial')\n",
    "            BatchSize(10)\n",
    "            MaxIterNum(300)\n",
    "            RegularizationLambda(0.02)\n",
    "            Alpha(0.15)\n",
    "            IterNumNoChange(50)\n",
    "            Intercept('true')\n",
    "            LearningRate('optimal')\n",
    "            InitialEta(0.05)\n",
    "            Momentum(0)\n",
    "            Nesterov('false')\n",
    "            LocalSGDIterations(0)\n",
    "    ) AS dt\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    # Drop the tables and try again if the table already exists\n",
    "    db_drop_table('glm_model')\n",
    "    db_drop_table('glm_out')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the next cell, we extract the feature importances and plot them. Remember to consider absolute value of the feature importances.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model_out = DataFrame('glm_model').to_pandas().reset_index()\n",
    "feat_imp = glm_model_out[glm_model_out['attribute'] > 0].sort_values(by = 'estimate', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify figure size\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "# Use ax.barh() for horizontal bar chart\n",
    "ax.barh(feat_imp['predictor'], feat_imp['estimate'], edgecolor = 'red')\n",
    "\n",
    "# Add text labels on right of the bars\n",
    "for x, y in zip(feat_imp['estimate'], feat_imp['predictor']):\n",
    "    ax.text(x, y, str(round(x, 2)), ha = 'left', va = 'center')\n",
    "\n",
    "# Set y-axis label\n",
    "ax.set_xlabel('Estimate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above plot shows that GLM model considers the transaction types: PAYMENT, CASH_OUT and CASH_IN as important.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_GLM output is a trained GLM model, which can be input to the TDGLMPredict function for prediction. The model also contains model statistics of MSE, Loglikelihood, AIC, and BIC.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b><i>*Please scroll down to the end of the notebook for detailed output explanation.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model_out[glm_model_out['attribute'] < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>5.2 XGBoost</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_XGBoost function, also known as eXtreme Gradient Boosting, is an implementation of the gradient boosted decision tree designed for speed and performance. It has recently been dominating applied machine learning.\n",
    "<br>\n",
    "<br>\n",
    "In gradient boosting, each iteration fits a model to the residuals (errors) of the previous iteration to correct the errors made by existing models. The predicted residual is multiplied by this learning rate and then added to the previous prediction. Models are added sequentially until no further improvements can be made. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might take upto few minutes to run\n",
    "\n",
    "query = '''\n",
    "CREATE MULTISET TABLE xgb_model AS (\n",
    "    SELECT * FROM TD_XGBoost(\n",
    "        ON clean_data_train PARTITION BY ANY\n",
    "        OUT TABLE MetaInformationTable(xgb_out) \n",
    "        USING\n",
    "            ResponseColumn('isFraud')\n",
    "            InputColumns('[3:13]')\n",
    "            MaxDepth(10)\n",
    "            NumBoostedTrees(100)\n",
    "            ModelType('CLASSIFICATION')\n",
    "            Seed(2)\n",
    "            ShrinkageFactor(0.1)\n",
    "            IterNum(10)\n",
    "            ColumnSampling(1.0) \n",
    "    ) AS dt\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    # Drop the tables and try again if the table already exists\n",
    "    db_drop_table('xgb_model')\n",
    "    db_drop_table('xgb_out')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>6. In-Database Model Scoring</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>6.1 Generalized Linear Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TDGLMPredict function predicts target values (regression) and class labels (classification) for test data using a GLM model trained by the GLM function.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Similar to GLM, input features should be standardized, such as using ScaleFit, and ScaleTransform, before using them in the function. The function takes only numeric features. We must convert the categorical features to numeric values before prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "CREATE MULTISET TABLE glm_predict_out AS (\n",
    "    SELECT * FROM TD_GLMPredict(\n",
    "        ON \"clean_data_test\" AS inputtable\n",
    "        PARTITION BY ANY \n",
    "        ON glm_model AS ModelTable\n",
    "        DIMENSION\n",
    "        USING\n",
    "            IDColumn('txn_id')\n",
    "            Accumulate('isFraud')\n",
    "            OutputProb('True')\n",
    "            Responses('0','1')\n",
    "    ) AS dt\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except Exception as e:\n",
    "    db_drop_table('glm_predict_out')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the GLM model's performance using TD_CLASSIFICATIONEVALUATOR\n",
    "\n",
    "# Check if the necessary tables exist before executing the query\n",
    "if not get_connection().dialect.has_table(get_connection(), 'glm_predict_out'):\n",
    "    print('Error: glm_predict_out table does not exist.')\n",
    "    sys.exit(1)\n",
    "\n",
    "query = '''\n",
    "SELECT * from TD_CLASSIFICATIONEVALUATOR(\n",
    "    ON (\n",
    "        SELECT\n",
    "            CAST(\"isFraud\" AS INTEGER) AS \"isFraud\",\n",
    "            CAST(prediction AS INTEGER) as prediction\n",
    "        FROM glm_predict_out\n",
    "    ) AS InputTable\n",
    "    OUT TABLE OutputTable(additional_metrics_glm)\n",
    "    USING\n",
    "        Labels(0, 1)\n",
    "        ObservationColumn('isFraud')\n",
    "        PredictionColumn('Prediction')\n",
    ") AS dt1\n",
    "ORDER BY 1, 2, 3;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('additional_metrics_glm')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_glm = DataFrame('additional_metrics_glm').to_pandas()\n",
    "metrics_glm['Metric'] = metrics_glm['Metric'].str.strip('\\x00')\n",
    "metrics_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_result = DataFrame('glm_predict_out').to_pandas()\n",
    "glm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output above shows prob_1, i.e. transaction is fraud and prob_0, i.e. transaction is not a fraud. The prediction column uses these probabilities to give a class label, i.e. prediction column.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>6.2 XGBoost</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CREATE MULTISET TABLE xgb_predict_out AS (\n",
    "    SELECT * FROM TD_XGBoostPredict(\n",
    "        ON clean_data_test AS inputtable PARTITION BY ANY\n",
    "        ON xgb_model AS modeltable DIMENSION ORDER BY task_index, tree_num, iter, class_num, tree_order\n",
    "        USING\n",
    "            IdColumn('txn_id')\n",
    "            OutputProb('True')\n",
    "            Responses('0','1')\n",
    "            ModelType('classification')\n",
    "            Accumulate('isFraud')\n",
    "    ) AS dt\n",
    ") WITH DATA;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except Exception as e:\n",
    "    db_drop_table('xgb_predict_out')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the XGBoost model's performance using TD_CLASSIFICATIONEVALUATOR\n",
    "\n",
    "# Check if the necessary tables exist before executing the query\n",
    "if not get_connection().dialect.has_table(get_connection(), 'xgb_predict_out'):\n",
    "    print('Error: xgb_predict_out table does not exist.')\n",
    "    sys.exit(1)\n",
    "\n",
    "query = '''\n",
    "SELECT * from TD_CLASSIFICATIONEVALUATOR(\n",
    "    ON (\n",
    "        SELECT CAST(\"isFraud\" AS INTEGER) AS \"isFraud\", prediction FROM xgb_predict_out\n",
    "    ) AS InputTable\n",
    "    OUT TABLE OutputTable(additional_metrics_xgb)\n",
    "    USING\n",
    "        Labels(0, 1)\n",
    "        ObservationColumn('isFraud')\n",
    "        PredictionColumn('Prediction')\n",
    ") AS dt1\n",
    "ORDER BY 1, 2, 3;\n",
    "'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('additional_metrics_xgb')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_xgb = DataFrame('additional_metrics_xgb').to_pandas()\n",
    "metrics_xgb['Metric'] = metrics_xgb['Metric'].str.strip('\\x00')\n",
    "metrics_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result = DataFrame('xgb_predict_out').to_pandas().reset_index()\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output above shows prob_1, i.e. transaction is fraud and prob_0, i.e. transaction is not a fraud. The prediction column uses these probabilities to give a class label, i.e. prediction column.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>7. Visualize the results</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.1 Comparing the metrics</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis values for the bars\n",
    "ind = np.arange(len(metrics_glm['Metric']))\n",
    "\n",
    "# Create bar graph with increased size\n",
    "fig, ax = plt.subplots(figsize = (10, 6))  # Set figsize to (width, height) in inches\n",
    "\n",
    "# Define soothing colors for the bars\n",
    "color1 = '#2C7BB6'  # blue\n",
    "color2 = '#ABD18A'  # green\n",
    "\n",
    "# Plotting bars for metrics_glm with soothing blue color\n",
    "ax.bar(ind, metrics_glm['MetricValue'], bar_width, label = 'metrics_glm', color = color1)\n",
    "\n",
    "# Plotting bars for metrics_xgb with soothing green color\n",
    "ax.bar(ind + bar_width, metrics_xgb['MetricValue'], bar_width, label = 'metrics_xgb', color = color2)\n",
    "\n",
    "ax.set_ylabel('MetricValue')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_title('Comparison of MetricValues between metrics_glm and metrics_xgb', fontsize = 14)\n",
    "ax.set_xticks(ind + bar_width / 2)\n",
    "ax.set_xticklabels(metrics_glm['Metric'], rotation = 45, ha = 'right', fontsize = 12)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()  # Add padding between subplots and prevent overlapping labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In this specific use case, minimizing Type-2 error or False Negatives is crucial as we want to avoid fraud transactions being misclassified as non-fraud. The bar graph presented above further confirms that the xgboost model outperforms the glm model in handling Type-2 errors.\n",
    "    <br>\n",
    "    <br>\n",
    "This is supported by the higher sensitivity or recall of the xgboost model in identifying fraud cases, as evident from the comparison of macro-precision, macro-f1, and macro-recall values. By minimizing Type-2 errors, we can reduce the risk of financial losses or reputational damage associated with undetected fraud transactions, making the xgboost model a more reliable choice for fraud detection in this scenario.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.2 Confusion Matrix</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for GLM\n",
    "cm_glm = confusion_matrix(glm_result['isFraud'], glm_result['prediction'])\n",
    "\n",
    "# Calculate confusion matrix for XGBoost\n",
    "cm_xgb = confusion_matrix(xgb_result['isFraud'], xgb_result['Prediction'])\n",
    "\n",
    "# Create figure and axes objects\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16, 8))\n",
    "\n",
    "# Plot GLM confusion matrix\n",
    "disp_glm = ConfusionMatrixDisplay(confusion_matrix = cm_glm, display_labels = ['No Fraud', 'Fraud'])\n",
    "disp_glm.plot(ax = ax1, cmap = 'Blues', colorbar = False)\n",
    "ax1.set_title('GLM Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_xticklabels(['No Fraud', 'Fraud'])\n",
    "ax1.set_yticklabels(['No Fraud', 'Fraud'])\n",
    "\n",
    "# Add text to the plot to show the actual values of the confusion matrix\n",
    "for i in range(cm_glm.shape[0]):\n",
    "    for j in range(cm_glm.shape[1]):\n",
    "        ax1.text(j, i, f'{cm_glm[i, j]}', ha = 'center', va = 'center', color = 'white' if cm_glm[i, j] > cm_glm.max() / 2 else 'black')\n",
    "\n",
    "# Plot XGBoost confusion matrix\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix = cm_xgb, display_labels = ['No Fraud', 'Fraud'])\n",
    "disp_xgb.plot(ax = ax2, cmap = 'Blues', colorbar = False)\n",
    "ax2.set_title('XGBoost Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_yticks([0, 1])\n",
    "ax2.set_xticklabels(['No Fraud', 'Fraud'])\n",
    "ax2.set_yticklabels(['No Fraud', 'Fraud'])\n",
    "\n",
    "# Add text to the plot to show the actual values of the confusion matrix\n",
    "for i in range(cm_xgb.shape[0]):\n",
    "    for j in range(cm_xgb.shape[1]):\n",
    "        ax2.text(j, i, f'{cm_xgb[i, j]}', ha = 'center', va = 'center', color = 'white' if cm_xgb[i, j] > cm_xgb.max() / 2 else 'black')\n",
    "\n",
    "# Adjust layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The comparison of the confusion matrices reveals that the xgboost model demonstrates superior performance in detecting fraud cases compared to the glm model. While the xgboost model may misclassify some non-fraud cases as fraud, it is important to note that the primary objective is to minimize false negatives, or type-2 errors. This underscores the significance of prioritizing high recall or sensitivity in fraud detection scenarios, where the consequences of missed fraud cases can be substantial.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>7.3 ROC-AUC</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ROC curve is a graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity) for different classification thresholds. AUC measures the overall performance of a classification model, where a higher value indicates better performance. AUC above 0.75 is generally considered decent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1\n",
    "AUC_glm = roc_auc_score(glm_result['isFraud'], glm_result['prob_1'])\n",
    "fpr_glm, tpr_glm, thresholds_glm = roc_curve(glm_result['isFraud'], glm_result['prob_1'])\n",
    "plt.plot(fpr_glm, tpr_glm, color = 'orange', label='GLM ROC. AUC = {}'.format(str(round(AUC_glm, 4))))\n",
    "\n",
    "# Plot 2\n",
    "AUC_xgb = roc_auc_score(xgb_result['isFraud'], xgb_result['Prob_1'])\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(xgb_result['isFraud'], xgb_result['Prob_1'])\n",
    "plt.plot(fpr_xgb, tpr_xgb, color = 'green', label = 'XGB ROC. AUC = {}'.format(str(round(AUC_xgb, 4))))\n",
    "\n",
    "# Plot the diagonal dashed line\n",
    "plt.plot([0, 1], [0, 1], color = 'darkblue', linestyle = '--')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Based on the ROC (Receiver Operating Characteristic) curve and AUC (Area Under the Curve) analysis, it is evident that the xgboost model performs better compared to the glm model in this use case.\n",
    "<br>\n",
    "<br>\n",
    "The ROC curve of the xgboost model consistently shows higher true positive rates at various false positive rates compared to the glm model. This indicates that the xgboost model has better sensitivity or ability to correctly identify true positive cases (fraud transactions) while maintaining a lower false positive rate or misclassification of non-fraud cases.\n",
    "<br>\n",
    "<br>\n",
    "In conclusion, both the ROC curve and AUC analysis suggest that the xgboost model performs better than the glm model in accurately classifying fraud and non-fraud transactions, and minimizing both false negatives and false positives.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>8. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['clean_data', 'clean_data_train', 'clean_data_test', 'glm_model',\n",
    "          'glm_out', 'xgb_model', 'xgb_out', 'glm_predict_out',\n",
    "          'additional_metrics_glm', 'xgb_predict_out', 'additional_metrics_xgb']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name = table)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('demo_glm_fraud');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = 'font-size:24px;font-family:Arial;color:#E37C4D'>Dataset:</b>\n",
    "\n",
    "- `txn_id`: transaction id\n",
    "- `step`: maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (31 days simulation).\n",
    "- `type`: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n",
    "- `amount`: amount of the transaction in local currency\n",
    "- `nameOrig`: customer who started the transaction\n",
    "- `oldbalanceOrig`: customer's balance before the transaction\n",
    "- `newbalanceOrig`: customer's balance after the transaction\n",
    "- `nameDest`: customer who is the recipient of the transaction\n",
    "- `oldbalanceDest`: recipient's balance before the transaction\n",
    "- `newbalanceDest`: recipient's balance after the transaction\n",
    "- `isFraud`: identifies a fraudulent transaction (1) and non fraudulent (0)\n",
    "- `isFlaggedFraud`: flags illegal attempts to transfer more than 200,000 in a single transaction\n",
    "\n",
    "<b style = 'font-size:24px;font-family:Arial;color:#E37C4D'>Model Output Explanations:</b>\n",
    "<br>\n",
    "<b style = 'font-size:16px;font-family:Arial;color:#E37C4D'>Model Configuration</b>\n",
    "- `Loss Function (LOG)`: The logarithmic loss function measures the difference between the predicted probabilities and the actual outcomes. It is commonly used in binary classification problems.\n",
    "- `Regularization (Enabled, 0.02)`: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The value 0.02 indicates the strength of this penalty.\n",
    "- `Alpha (Elasticnet, 0.15)`: Alpha is a parameter in the Elastic Net regularization method, which combines L1 (Lasso) and L2 (Ridge) regularization. A value of 0 represents pure L2 regularization, while a value of 1 represents pure L1 regularization. In this case, the model uses a combination of both (0.15).\n",
    "\n",
    "<b style = 'font-size:16px;font-family:Arial;color:#E37C4D'>Optimization Algorithm</b>\n",
    "- `Learning Rate (Initial, 0.05; Final, 0.388190)`: The learning rate determines the step size the optimization algorithm takes during each iteration. The initial learning rate is set to 0.05, and it reaches a final value of 0.388190.\n",
    "- `Momentum (0.0)`: Momentum is a technique used in optimization algorithms to speed up convergence by adding a fraction of the previous update to the current update. A momentum value of 0.0 means it's not being used in this case.\n",
    "- `Nesterov (FALSE)`: Nesterov momentum is a variation of the standard momentum method. This parameter being set to FALSE indicates it is not being used.\n",
    "\n",
    "<b style = 'font-size:16px;font-family:Arial;color:#E37C4D'>Model Performance</b>\n",
    "- `Log-likelihood (-0.013004)`: Log-likelihood is a measure of how well the model fits the observed data. Higher values indicate a better fit.\n",
    "- `Akaike Information Criterion (AIC, 26.026008)`: AIC is used to compare models with different numbers of parameters, balancing goodness of fit with model complexity. Lower values indicate a better model.\n",
    "- `Bayesian Information Criterion (BIC, 140.9153)`: BIC is similar to AIC but places a stronger penalty on model complexity. Lower values indicate a better model.\n",
    "\n",
    "<b style = 'font-size:16px;font-family:Arial;color:#E37C4D'>Convergence</b>\n",
    "- `Number of Observations (50,901)`: The total number of data points used in the model.\n",
    "- `Number of Iterations (111, Converged)`: The number of iterations the optimization algorithm took to converge, indicating the algorithm has found an optimal solution.\n",
    "- `Intercept (-4.113711)`: The intercept (or bias term) is the value of the output when all input features are set to zero.\n",
    "- `LocalSGD Iterations (0)`: The number of iterations used in the Local Stochastic Gradient Descent (SGD) method. A value of 0 indicates it's not being used in this case.\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Dataset source: <a href = 'https://www.kaggle.com/datasets/ealaxi/paysim1'>https://www.kaggle.com/datasets/ealaxi/paysim1</a></li>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/search/all?query=Python+Package+User+Guide&content-lang=en-US'>here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Â©2023 Teradata. All Rights Reserved</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
