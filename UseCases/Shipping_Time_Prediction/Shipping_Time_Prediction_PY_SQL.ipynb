{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4b9f6c-0c0c-4180-8688-d09cdfbaa8dc",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Shipping Time Prediction Using Vantage InDB Analytic Functions</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcd9c9-a417-46c9-913f-4937369ecf47",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Introduction</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>eBay, as an online marketplace, faces the challenge of accurately estimating delivery dates for shipments from various sellers. The current estimation process, based on seller handling time and carrier transit time, often leads to inconsistent and inaccurate predictions. This results in customer dissatisfaction and potential erosion of trust in the platform. Therefore, there is a need to develop a robust system that can reliably estimate delivery dates, accounting for handling time, transit time, and other relevant variables affecting the actual delivery timeframe. This system should leverage historical data, predictive modeling techniques, and machine learning algorithms to improve accuracy. The successful implementation of an improved delivery time estimation system will enhance customer satisfaction, increase buyer trust, boost sales, and improve seller engagement on the platform.</p>\n",
    "\n",
    "  \n",
    "<p style = 'font-size:16px;font-family:Arial'> To address the problem in estimating delivery dates for eBay packages, we propose leveraging Teradata's in-database capabilities. By using Teradata's data cleaning and machine learning functionalities, we can develop a robust model to predict delivery dates. This involves collecting relevant data, cleaning it for accuracy, performing feature engineering, developing a predictive model,and validating and optimizing it. The implementation of this solution can lead to improved customer satisfaction, increased trust, higher sales, and enhanced seller engagement.</p>\n",
    "   \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "The implemented functions are from the following documentation:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'> <a href='https://www.docs.teradata.com/r/Teradata-VantageTM-Analytics-Database-Analytic-Functions-17.20'>Advanced SQL Engine 17.20 Functions</a></li>       \n",
    "<li style = 'font-size:16px;font-family:Arial'> <a href= 'https://docs.teradata.com/r/Enterprise_IntelliFlex_Lake_VMware/Vantage-Analytics-Library-User-Guide/Welcome-to-Vantage-Analytics-Library'>Vantage Analytics Library</a></li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> <a href= 'https://docs.teradata.com/r/Teradata-VantageTM-Unbounded-Array-Framework-Time-Series-Reference'>UAF Time-Series 17.20 Functions</a></li>    \n",
    "<br> \n",
    "       \n",
    "    \n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Data</b></p>\n",
    "      \n",
    "<p style = 'font-size:16px;font-family:Arial'>The data was collected from open source <a href= 'https://www.kaggle.com/datasets/armanaanand/ebay-delivery-date-prediction'>Kaggle</a> with following description</p>\n",
    "    \n",
    "<img src='images/DataSet.png'>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426d0ca-3471-4e50-9472-113ebf16dd19",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>1. Connect to Vantage.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca611c2-906f-4adc-a7f0-dacf65576edb",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the section, we import the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbc783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:42:10.698948Z",
     "start_time": "2023-07-31T07:42:06.990321Z"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import time\n",
    "import pandas as pd\n",
    "import teradataml as tdml\n",
    "from teradataml import *\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import event\n",
    "import csv\n",
    "# from teradataml.dataframe.data_transfer import read_csv\n",
    "from teradatasqlalchemy.types import *\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from teradataml import *\n",
    "configure.val_install_location = \"val\"\n",
    "\n",
    "import plotly.express as px\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "display.max_rows=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e793f3-bc05-4191-8b4a-ab4373eb95d1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, then use down arrow to go to next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65006e51-2f7d-4c88-a1fa-bb336b1a15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e20f29-a223-4479-a4d6-f2b6ec349bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=Shipping_Time_Prediction_PY_SQL.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84961f42-47f7-4d55-b804-aae5cdd080dc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'> <b>2. Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo only locally.  Since we are altering the table and creating new columns for various calculations we have only created the local version of the database and tables. The data is stored on cloud and we will insert the data in a local table and then do the processing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1319ca-0fe6-4a14-a59d-9020cf86c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_ShippingTimePrediction_local');\"\n",
    " # Takes about 1 minute 30 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a934c0a-ff68-4b45-8fa1-1106f91461e5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae82294-7180-446e-ba6c-33addf882f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5541ee-0cf7-4b79-a4d7-95ad52b855b0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>3. Analyze the raw data set</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The dataset is shipping dataset with data containing 110,000 rows. A more detailed description of the features is already mentioned in the introduction of the Data.\n",
    "\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a DataFrame to get the data from the table created.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb465f6-e446-499d-bcb4-3e477520dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df=DataFrame(in_schema('DEMO_ShipTimePred', 'Delivery_date_data'))\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84bb202-24f5-4852-a0c0-e811239579b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=raw_df.to_pandas(all_rows=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0146e-4b44-4093-b0b4-3856421f62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Distribution by channel\n",
    "conversions=df[['record_number','b2c_c2c']]\n",
    "conversions = conversions.groupby(['b2c_c2c'], as_index=False).count()\n",
    "fig = px.bar(conversions, x='b2c_c2c', y='record_number', color='b2c_c2c')\n",
    "\n",
    "fig.update_layout(title='Channel Distribution',\n",
    "                   xaxis_title='Channel',\n",
    "                   yaxis_title='Distributions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441badd-f69b-436b-9125-6f61a53b0c94",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above chart shows the distribution of the shipments based on the channel use B2C(Business to Customer) and C2C(Customer to Customer).\n",
    "\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We can also try analyzing the shipments by Shipment Methods. Since the data is sample data for the purpose of this demo the shipment methods used are not specified and are using just numbers to categorize the shipment methods used and are depicted as shipment methods ids.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27672f97-ffdf-40bf-b21e-a7a9d8a722ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Distribution by Shipment Method\n",
    "shipments=df[['record_number','shipment_method_id']]\n",
    "shipments = shipments.groupby(['shipment_method_id'], as_index=False).count()\n",
    "fig = px.bar(shipments, x='shipment_method_id', y='record_number', color='shipment_method_id')\n",
    "\n",
    "fig.update_layout(title='Shipment Method Distribution',\n",
    "                   xaxis_title='Shipment Method',\n",
    "                   yaxis_title='Distributions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13f4ed-5113-4651-bb86-92060d62a924",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above chart shows the distribution of the shipments based on different shipment method. As seen in the chart most of the shipments are using the Shipment Method Id(0)\n",
    "\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We can also try analyzing the shipments by Categories. Similar to the Shipment Methods the categories are not specified and are using just numbers to categorize the shipments. The categories are defined using numbers and are depicted as Category IDs. \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84374ee-3971-44e3-a474-a85927d96f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Distribution by category\n",
    "categories=df[['record_number','category_id']]\n",
    "categories = categories.groupby(['category_id'], as_index=False).count()\n",
    "fig = px.bar(categories, x='category_id', y='record_number', color='category_id')\n",
    "\n",
    "fig.update_layout(title='Category Distribution',\n",
    "                   xaxis_title='Category',\n",
    "                   yaxis_title='Distributions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372cb1e-b524-4c58-a41c-8ed7e485b77f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above chart shows how the distribution of the shipments based on the categories. Most of the shipments are for categories with Category Ids between 0-5.\n",
    "\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Below we try to check the Shipment Fees for various shipments.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63272461-4e9c-45c0-ac0b-53cded5d8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "ShipFees=df[['record_number','shipping_fee','weight_units']]\n",
    "ax=px.scatter(ShipFees, x=\"record_number\", y=\"shipping_fee\",\n",
    "              size=\"shipping_fee\",size_max = 70,color=\"shipping_fee\",hover_data=['shipping_fee'],\n",
    "              width=900, height=400, \n",
    "              # color_discrete_map = {'Online Display': '#E15759','Online Video': '#76B7B2','Facebook': '#4E79A7','Instagram': '#F28E2B' ,'Paid Search': '#59A14F'},\n",
    "             labels={\n",
    "                     \"shipping_fee\": \"Shipping Fees\",\n",
    "                     \"record_number\": \"Record Number\"\n",
    "        }\n",
    "             )\n",
    "ax.update_layout(showlegend=False)\n",
    "ax.update_layout(title_text='Shipping Fees Distribution', title_x=0.5)\n",
    "ax.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2f68f-358a-4ee8-80a5-64bc1cba3429",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above chart shows how the distribution of the shipments based on the shipment fees. The size of the bubble is dependant on the fees, larger the size of the bubble, larger the fees. The colors also depict the increasing fees as shown in the legend at the side. The least starts from dark purple and the maximum fees is yellow i.e. the largets yellow bubble at the left top corner has the maximum shipping fees.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce4e8-56d6-4294-9259-2775e54795cc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>4. Data Preprocessing and Cleaning</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a62675-2050-4f67-a56c-0fb80d4fa103",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Data Preprocessing:</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>New column 'distance' is added to the table which will store distance between the item location and the buyer location.</p>\n",
    "<p style = 'font-size:18px;font-family:Arial'>The geospatial function <b>ST_SPHERICALDISTANCE</b> in Vantage is used to calculate the distance using the latitude and longitude columns of the item and buyer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87dcbd-3cd9-4ece-baca-ddc3ce889663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:23:49.856645Z",
     "start_time": "2023-07-31T06:23:43.201506Z"
    }
   },
   "outputs": [],
   "source": [
    "qry='''ALTER TABLE DEMO_ShipTimePred_db.Delivery_Date_Data\n",
    "ADD distance FLOAT;'''\n",
    "\n",
    "execute_sql(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af992b-dae0-478f-94f8-780862922b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:24:06.581344Z",
     "start_time": "2023-07-31T06:23:49.858878Z"
    }
   },
   "outputs": [],
   "source": [
    "qry='''UPDATE DEMO_ShipTimePred_db.Delivery_Date_Data\n",
    "SET distance = NEW ST_Geometry('ST_Point', item_long, item_lat).ST_SPHERICALDISTANCE(NEW ST_Geometry('ST_Point', \n",
    "buyer_long, buyer_lat))/1000;'''\n",
    "\n",
    "execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4483cf6-abb6-4a64-8d6a-c1d96a25b887",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Checking and handling missing values:</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We create a new table with this available data so that we maintain the copy of the original data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fad591-1d18-4295-9e6c-30cfb01d170b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:43:02.457075Z",
     "start_time": "2023-07-31T06:42:57.716270Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qry='''CREATE multiset TABLE delivery_date_complete_dataset AS (\n",
    "        SELECT *\n",
    "        FROM DEMO_ShipTimePred_db.Delivery_Date_Data\n",
    "    ) WITH DATA PRIMARY INDEX (record_number);'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('delivery_date_complete_dataset')\n",
    "    execute_sql(qry)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed0165-afd1-410c-9c10-2a0ffaf736ee",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Get Rows With Missing Values</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>TD_GetRowsWithMissingValues used on the table delivery_date_complete_dataset will select rows from the table where at least one of the first 19 columns has missing values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c91b67-e032-40d3-b301-f44e2e9f8664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:43:25.732520Z",
     "start_time": "2023-07-31T06:43:18.816347Z"
    },
    "code_folding": [],
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "MissingVal_df = DataFrame.from_query(\"\"\"\n",
    "SELECT * FROM TD_GetRowsWithMissingValues ( \n",
    "    ON delivery_date_complete_dataset  AS InputTable\n",
    "    USING\n",
    "    TargetColumns ('[0:23]')\n",
    ") AS dt;\n",
    "\"\"\")\n",
    "MissingVal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2875448-ff85-4688-8e10-3d186cc9040f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Replace Missing Values</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We create a reusable function to replace missing values for various columns which is used below to calculate missing values for declared_handling_days, weight, carrier_min_estimate and carrier_max_estimate. Below is the logic used for replacing missing values:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>It calculates the average value (AvgVal) of a specified column (avgColumn) grouped by another column (groupCol) in the delivery_date_complete_dataset table. Only non-null values are considered, and the result is grouped by the specified column.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Updates the delivery_date_complete_dataset table by filling in missing values in the avgColumn with either the corresponding value from AverageData based on the matching groupCol, or with the overall average value if no match is found.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd2430-eee4-446d-8bcc-507b5f8241fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:42:27.070406Z",
     "start_time": "2023-07-31T07:42:27.067048Z"
    }
   },
   "outputs": [],
   "source": [
    "def temp_col(col):\n",
    "    execute_sql(\"\"\"\n",
    "    ALTER TABLE delivery_date_complete_dataset\n",
    "    ADD \"{0}_varchar\" VARCHAR(50);\"\"\".format(col))\n",
    "    \n",
    "    execute_sql(\"\"\"\n",
    "    UPDATE delivery_date_complete_dataset\n",
    "    SET \"{0}_varchar\" = CAST({0} AS VARCHAR(50));\"\"\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b8a92-c9c9-44fc-bc47-ff5c2613b468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:42:27.064473Z",
     "start_time": "2023-07-31T07:42:27.059223Z"
    }
   },
   "outputs": [],
   "source": [
    "def handleMissingData(avgColumn, groupCol):\n",
    "    temp_col(avgColumn)\n",
    "    \n",
    "    try:\n",
    "        execute_sql(\"\"\"DROP TABLE AVERAGEDATA\"\"\")\n",
    "        print(\"DROPPING TABLE AVERAGEDATA\")\n",
    "    except:\n",
    "        print(\"[Teradata Database] [Info] Object 'AVERAGEDATA' does not exist.\")\n",
    "        \n",
    "    execute_sql(\"\"\"\n",
    "        CREATE VOLATILE TABLE AverageData AS (\n",
    "            SELECT DISTINCT AVG(\"{0}\") as AvgVal, \"{1}\" as \"{1}\"\n",
    "            FROM delivery_date_complete_dataset\n",
    "            WHERE \"{0}_varchar\" <> '**********************'\n",
    "            GROUP BY \"{1}\"\n",
    "        )\n",
    "        WITH DATA\n",
    "        ON COMMIT PRESERVE ROWS;\n",
    "    \"\"\".format(avgColumn, groupCol))\n",
    "    \n",
    "    execute_sql(\"\"\"\n",
    "        UPDATE delivery_date_complete_dataset AS E\n",
    "    SET \"{0}\" = \n",
    "        CASE\n",
    "            WHEN E.\"{0}_varchar\" = '**********************'\n",
    "                THEN COALESCE(\n",
    "                    (SELECT AvgVal FROM AverageData AS D WHERE E.\"{1}\" = D.\"{1}\"),\n",
    "                    (SELECT AVG(AvgVal) FROM AverageData)\n",
    "                )\n",
    "            ELSE \"{0}\"\n",
    "        END;\n",
    "    \"\"\".format(avgColumn, groupCol))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042ed64-476a-4faa-a93f-c1eb56d0bbf7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above code is used to get the missing values for columns 'declared_handling_days', 'weight','carrier_min_estimate' and 'carrier_max_estimate' </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd389c-db07-49ff-ade6-0a253e1f9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "handleMissingData(\"declared_handling_days\", \"seller_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be892ac7-06ae-411f-9f2e-92e5d72dc2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:44:59.160770Z",
     "start_time": "2023-07-31T06:44:15.110157Z"
    }
   },
   "outputs": [],
   "source": [
    "handleMissingData(\"weight\", \"category_id\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea854b-6a3f-46d1-9c77-66144429d907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:45:32.796528Z",
     "start_time": "2023-07-31T06:44:59.163069Z"
    }
   },
   "outputs": [],
   "source": [
    "handleMissingData(\"carrier_min_estimate\", \"shipment_method_id\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d388844-6b29-4548-b377-d12e2cd118d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:46:22.767415Z",
     "start_time": "2023-07-31T06:45:32.798934Z"
    }
   },
   "outputs": [],
   "source": [
    "handleMissingData(\"carrier_max_estimate\", \"shipment_method_id\") # Handle missing carrier_max_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b27d2e-be8b-40bc-8388-ef4e536b1112",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The below code is used to get the missing values for column package size based on weight and average package size.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbafc63-b461-4f3f-890b-e5c68393462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry='''CREATE VOLATILE TABLE pkg_averagedata AS (\n",
    "            SELECT DISTINCT AVG(weight) as AvgVal, package_size\n",
    "            FROM delivery_date_complete_dataset\n",
    "            WHERE package_size <> 'NONE'\n",
    "            GROUP BY package_size\n",
    "        )\n",
    "        WITH DATA\n",
    "        ON COMMIT PRESERVE ROWS;'''\n",
    "execute_sql(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2324f-7928-473b-a318-1fd28c2fbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry='''CREATE VOLATILE TABLE temp_table AS (\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT\n",
    "                delivery_date_complete_dataset.*,\n",
    "                pkg_averagedata.package_size AS new_package_size,\n",
    "                ABS(delivery_date_complete_dataset.weight - pkg_averagedata.AvgVal) AS difference,\n",
    "                ROW_NUMBER() OVER (PARTITION BY delivery_date_complete_dataset.record_number ORDER BY ABS(delivery_date_complete_dataset.weight - pkg_averagedata.AvgVal)) AS rn\n",
    "            FROM delivery_date_complete_dataset\n",
    "            CROSS JOIN pkg_averagedata\n",
    "            WHERE delivery_date_complete_dataset.package_size = 'NONE'\n",
    "        ) AS subquery\n",
    "        WHERE rn = 1\n",
    "    ) WITH DATA PRIMARY INDEX(record_number) ON COMMIT PRESERVE ROWS;'''\n",
    "\n",
    "execute_sql(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4104066-1647-43f9-a001-07200af05254",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry='''UPDATE delivery_date_complete_dataset\n",
    "    FROM temp_table\n",
    "    SET package_size = new_package_size\n",
    "    WHERE delivery_date_complete_dataset.record_number = temp_table.record_number;'''\n",
    "\n",
    "execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601f8b6-3f7b-4c3a-921a-1e028e6c28b8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will drop the varchar columns which were generated when handling missing values to get the data in shape.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993252a-0567-47f2-ba1a-226d865d0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry='''ALTER TABLE delivery_date_complete_dataset\n",
    "DROP declared_handling_days_varchar,\n",
    "DROP weight_varchar,\n",
    "DROP carrier_min_estimate_varchar,\n",
    "DROP carrier_max_estimate_varchar;'''\n",
    "\n",
    "execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253fad3-56ce-476a-8dea-d28483669091",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will standardize all the date and timestamp columns to timestamp(0).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788be1c9-1e52-4a1a-9a22-fdcfba5de4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sql('''ALTER TABLE delivery_date_complete_dataset\n",
    "    ADD payment_datetime_temp TIMESTAMP(0),\n",
    "    ADD acceptance_scan_timestamp_temp TIMESTAMP(0),\n",
    "    ADD delivery_date_temp TIMESTAMP(0);''')\n",
    "    \n",
    "execute_sql('''UPDATE delivery_date_complete_dataset\n",
    "    SET payment_datetime_temp = CAST(SUBSTRING(cast(payment_datetime as VARCHAR(30)), 1, 19) AS TIMESTAMP(0)),\n",
    "        acceptance_scan_timestamp_temp = CAST(SUBSTRING(cast(acceptance_scan_timestamp as VARCHAR(30)), 1, 19) AS TIMESTAMP(0)),\n",
    "        delivery_date_temp = cast(cast(cast(delivery_date as date format 'yyyy-mm-dd') as varchar(10)) || ' 00:00:00' as timestamp(0));''')\n",
    "\n",
    "execute_sql('''ALTER TABLE delivery_date_complete_dataset\n",
    "    DROP payment_datetime,\n",
    "    DROP acceptance_scan_timestamp,\n",
    "    DROP delivery_date;''')\n",
    "    \n",
    "execute_sql('''ALTER TABLE delivery_date_complete_dataset\n",
    "    RENAME payment_datetime_temp TO payment_datetime,\n",
    "    RENAME acceptance_scan_timestamp_temp TO acceptance_scan_timestamp,\n",
    "    RENAME delivery_date_temp TO delivery_date;''')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d11f0-6014-446f-a43b-15a70956de30",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Calculate number of shipping days, handling days and delivery days based on the above timestamp values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab54cfd-67b7-4595-9b76-74928e9b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sql('''ALTER TABLE delivery_date_complete_dataset\n",
    "    ADD handling_days INTEGER,\n",
    "    ADD shipping_days INTEGER,\n",
    "    ADD delivery_days INTEGER;''')\n",
    "    \n",
    "execute_sql('''UPDATE delivery_date_complete_dataset\n",
    "    SET handling_days = CAST(acceptance_scan_timestamp AS DATE) - CAST(payment_datetime AS DATE),\n",
    "        shipping_days = CAST(delivery_date AS DATE) - CAST(acceptance_scan_timestamp AS DATE),\n",
    "        delivery_days = CAST(delivery_date AS DATE) - CAST(payment_datetime AS DATE);''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31b054-112f-4ece-9b7e-0e26784996d6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Round off the declared_handling_days and distance and then Delete rows where distance , weight or item price are zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b9504-fd2d-4b08-a073-706070a45150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:51:55.306348Z",
     "start_time": "2023-07-31T06:51:49.684743Z"
    }
   },
   "outputs": [],
   "source": [
    "execute_sql('''UPDATE delivery_date_complete_dataset\n",
    "SET declared_handling_days = CAST(ROUND(declared_handling_days, 0) AS INTEGER)\n",
    ",distance = CAST(ROUND(distance, 0) AS INTEGER);''')\n",
    "\n",
    "execute_sql('''DELETE FROM delivery_date_complete_dataset\n",
    "WHERE distance = 0.0 OR weight = 0 OR item_price = 0.0;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1d562-ca65-4796-8c79-1bc77932300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_= DataFrame.from_query(\"\"\"\n",
    "    SELECT * FROM delivery_date_complete_dataset\n",
    "\"\"\")\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc1058-675a-42fb-af89-175e351f7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final=df_.select([\"b2c_c2c\",\"shipping_fee\",\"item_price\",\"quantity\", \"weight\",\"package_size\",\"record_number\"\n",
    "#                     ,\"distance\",\"shipment_method_id\",\"category_id\",])\n",
    "df_final=df_.drop([\"seller_id\",\"declared_handling_days\", \"carrier_min_estimate\", \"carrier_max_estimate\",\n",
    "                \"item_zip\",\"buyer_zip\", \"weight_units\", \"item_lat\",\"item_long\",\"buyer_lat\",\"buyer_long\",\n",
    "                   \"payment_datetime\", \"acceptance_scan_timestamp\", \"delivery_date\"], axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8deb7-602f-41c7-b7eb-929287053e8f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>5. Creation of final analytic dataset </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have datasets in which different columns have different units . If we feed these features to the model as is, there is every chance that one feature will influence the result more due to its value than the others. But this doesn’t necessarily mean it is more important as a predictor. So, to give importance to all the features we need feature scaling.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we apply the Standard scale and transform functions which are ScaleFit and ScaleTransform functions in Vantage. ScaleFit() function outputs statistics to input to ScaleTransform() function, which scales specified input DataFrame columns.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2fd80-26f4-4dea-a96b-34aa1e7f0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import ScaleFit , ScaleTransform\n",
    "scaler = ScaleFit(\n",
    "                    data=df_final,\n",
    "                    target_columns=[\"shipping_fee\",\"item_price\", \"quantity\", \"weight\", \"distance\"],\n",
    "                    scale_method=\"STD\",\n",
    "                    global_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3ce9f-fc6e-4c48-bfc3-0c6f4e9f3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS_scaled = ScaleTransform(data=df_final,\n",
    "                         object=scaler.output,\n",
    "                         accumulate=[\"record_number\",\"b2c_c2c\", \"package_size\", \"delivery_days\"\n",
    "                                     ,\"shipment_method_id\",\"category_id\"]\n",
    "                           ).result\n",
    "ADS_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be10134-baee-40f7-883a-bb344fb012e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(ADS_scaled, table_name = 'delivery_date_dataset_final', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbdbf1-7b00-4313-b1a3-51fcfa0faf3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:53:42.903738Z",
     "start_time": "2023-07-31T06:53:40.357201Z"
    }
   },
   "outputs": [],
   "source": [
    "qry=  '''Create multiset table ohe_fit as ( \n",
    "        SELECT * FROM TD_OneHotEncodingFit( \n",
    "        ON (select record_number,b2c_c2c, package_size,cast(shipment_method_id as VARCHAR(5)) as shipment_method_id, \n",
    "        cast(category_id as VARCHAR(5)) as category_id, shipping_fee ,item_price, quantity ,weight, distance,delivery_days\n",
    "        from delivery_date_dataset_final) AS INPUTTABLE \n",
    "        USING \n",
    "        TargetColumn('b2c_c2c','shipment_method_id','category_id','package_size') \n",
    "        OtherColumnName('other') \n",
    "        IsInputDense('true') \n",
    "        CategoryCounts(2,23,33,6) \n",
    "        Approach('Auto') \n",
    "        ) AS dt  \n",
    "        )with data;'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('ohe_fit')\n",
    "    execute_sql(qry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001bec5-a892-4a1b-b206-fc06cd764de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:53:54.462621Z",
     "start_time": "2023-07-31T06:53:47.922467Z"
    }
   },
   "outputs": [],
   "source": [
    "qry='''CREATE MULTISET TABLE ohe_dataset AS (\n",
    "    SELECT * FROM TD_OneHotEncodingTransform ( \n",
    "    ON (select record_number,b2c_c2c, package_size,cast(shipment_method_id as VARCHAR(5)) as shipment_method_id, \n",
    "        cast(category_id as VARCHAR(5)) as category_id, shipping_fee ,item_price, quantity ,weight, distance,delivery_days \n",
    "        from delivery_date_dataset_final) AS InputTable \n",
    "    ON ohe_fit AS FitTable Dimension \n",
    "    USING \n",
    "    IsInputDense('True') \n",
    "    ) AS dt)\n",
    "    WITH DATA;'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('ohe_dataset')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06703833-deb1-45b6-830f-39acff7e027d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>6. Creation of Train and Test data.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TrainTestSplit() function simulates how a model would perform on new data. The function divides the dataset into train and test subsets to evaluate machine learning algorithms and validate processes. The first subset is used to train the model. The second subset is used to make predictions and compare the predictions to actual values.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfc808-f3f5-4a3a-b96e-8b64ae26dce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:54:18.436648Z",
     "start_time": "2023-07-31T06:54:10.289949Z"
    }
   },
   "outputs": [],
   "source": [
    "qry= '''CREATE MULTISET TABLE delivery_date_ads AS (\n",
    "    SELECT * FROM TD_TrainTestSplit( \n",
    "        ON ohe_dataset AS InputTable \n",
    "        USING \n",
    "        IDColumn('record_number') \n",
    "        trainSize(0.75) \n",
    "        testSize(0.25) \n",
    "        Seed(42)\n",
    "    ) AS dt\n",
    ") WITH DATA PRIMARY INDEX(record_number);'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('delivery_date_ads')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2b13f-c70d-43c4-9570-3580f5cc89e5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Creating delivery_date_train_dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef48ab-fb8f-4b5b-8ad3-17b6c34d9c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:54:48.423928Z",
     "start_time": "2023-07-31T06:54:44.197116Z"
    }
   },
   "outputs": [],
   "source": [
    "qry = '''create multiset table delivery_date_train_dataset AS(\n",
    "select * FROM delivery_date_ads\n",
    "    WHERE TD_IsTrainRow in (1)\n",
    ")WITH DATA PRIMARY INDEX(record_number);'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('delivery_date_train_dataset')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03850fc4-81ff-46f7-b264-fba5d5d57ab0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Creating delivery_date_test_dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b74c46-d274-4fb2-882e-190406f046ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T06:54:51.032611Z",
     "start_time": "2023-07-31T06:54:48.848727Z"
    }
   },
   "outputs": [],
   "source": [
    "qry = '''create table delivery_date_test_dataset AS(\n",
    "select * FROM delivery_date_ads\n",
    "    WHERE TD_IsTrainRow in (0)\n",
    ")WITH DATA PRIMARY INDEX(record_number);'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('delivery_date_test_dataset')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a627aea-099c-43ed-b614-913c97f865b3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>7. Feature Selection using Elastic Net Regularization.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Feature selection is a crucial step in building predictive models as it helps identify the most relevant and informative features from a potentially large set of variables. In this context, elastic net regularization is a powerful technique that can be employed to effectively filter out features and improve model performance.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Elastic net regularization combines the L1 (Lasso) and L2 (Ridge) regularization techniques, offering a balanced approach to feature selection. It applies a penalty term to the model's objective function, encouraging sparsity in the coefficient estimates and promoting the selection of a subset of important features while shrinking the coefficients of less relevant or redundant features.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For more information on **Regularization**: <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Database-Analytic-Functions/Model-Training-Functions/TD_GLM/TD_GLM-Syntax-Elements'>[Link]</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921782c-fe37-4579-bc8b-6ab6034d19e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:42:42.532288Z",
     "start_time": "2023-07-31T07:42:39.546605Z"
    }
   },
   "outputs": [],
   "source": [
    "qry1 = '''CREATE MULTISET TABLE td_glm_cal_ex AS (\n",
    "    SELECT * from TD_GLM (\n",
    "        ON delivery_date_train_dataset\n",
    "        USING\n",
    "        InputColumns('[2:74]')\n",
    "        ResponseColumn('delivery_days')\n",
    "        Family('Gaussian')\n",
    "        RegularizationLambda(0.00175)\n",
    "        LearningRate('optimal')\n",
    "        IterNumNoChange(100)\n",
    "    ) as dt\n",
    ") WITH DATA;'''\n",
    "\n",
    "qry='''CREATE MULTISET TABLE td_glm_cal_ex AS (\n",
    "    SELECT * from TD_GLM (\n",
    "        ON delivery_date_train_dataset\n",
    "        USING\n",
    "        InputColumns('[2:74]')\n",
    "        ResponseColumn('delivery_days')\n",
    "        Family('Gaussian')\n",
    "    BatchSize(500)\n",
    "    MaxIterNum(300)\n",
    "    RegularizationLambda(0.02)\n",
    "    Alpha(0.3)\n",
    "    IterNumNoChange(70)\n",
    "    Tolerance(0.008)\n",
    "    Intercept('true')\n",
    "    LearningRate('adaptive')\n",
    "    InitialEta(0.015)\n",
    "    Momentum(0.8)\n",
    "    LocalSGDIterations(10)) as dt\n",
    ") WITH DATA;'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('td_glm_cal_ex')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63126e-dd43-4d9c-afdb-39a977341ab4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output of the TD_GLM function provides attributes where the index of the predictors have positive values and the estimate column has the predictor weights. For feature selection we consider all columns which are the predictors and have weights >0 i.e. estimate > 0. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'> In the for loop we create a list of all such columns and create a table with only the columns which have weightage as predictors for the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f52a8-363b-4467-8de1-bbb7dd0be667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:22.680096Z",
     "start_time": "2023-07-31T07:43:19.627616Z"
    }
   },
   "outputs": [],
   "source": [
    "features_output = pd.read_sql(\"\"\"SELECT predictor FROM td_glm_cal_ex where attribute >= 0 and estimate not in (0)\"\"\", eng)\n",
    "Str = \"\"\n",
    "train_values = pd.read_sql(\"\"\"SELECT top 1 * FROM delivery_date_train_dataset\"\"\", eng)\n",
    "\n",
    "for i in range(len(features_output)):\n",
    "    if(features_output['predictor'][i] in train_values.columns.tolist()):\n",
    "        Str += '\"{}\"'.format(features_output['predictor'][i])\n",
    "        if i != (len(features_output) - 1):\n",
    "            Str += \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26426ef8-311f-4685-aef4-c57b73a81d35",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We create the train and test datasets with only these features(columns) to be used in the model for predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c867318-e618-42ff-bc64-836a8283ea1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:26.727362Z",
     "start_time": "2023-07-31T07:43:23.168611Z"
    }
   },
   "outputs": [],
   "source": [
    "qry= '''CREATE MULTISET TABLE selected_ddp_features_train AS (\n",
    "    select \"record_number\",{0},\"delivery_days\" from delivery_date_train_dataset\n",
    ") WITH DATA PRIMARY INDEX(record_number)'''.format(Str)\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('selected_ddp_features_train')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84986d8b-cfa9-4cdd-a56a-c2024587edf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:28.339715Z",
     "start_time": "2023-07-31T07:43:27.156936Z"
    }
   },
   "outputs": [],
   "source": [
    "qry = '''\n",
    "CREATE MULTISET TABLE selected_ddp_features_test AS (\n",
    "    select \"record_number\",{0},\"delivery_days\" from delivery_date_test_dataset\n",
    ") WITH DATA\n",
    "PRIMARY INDEX(record_number)\n",
    "'''.format(Str)\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('selected_ddp_features_test')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbd479-1402-45b4-8cf4-f6184b30b038",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>8. Generalized Linear Model (GLM) in Teradata </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_GLM function is a generalized linear model (GLM) that performs regression and classification analysis on data sets, where the response follows an exponential family distribution and supports the following models:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Regression (Gaussian family): The loss function is squared error.\n",
    "<li style = 'font-size:16px;font-family:Arial'>Binary Classification (Binomial family): The loss function is logistic and implements logistic regression. The only response values are 0 or 1.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The function uses the Minibatch Stochastic Gradient Descent (SGD) algorithm. The algorithm estimates the gradient of loss in minibatches, which is defined by the BatchSize argument and updates the model with a learning rate using the LearningRate argument.</p>\n",
    "    <p style = 'font-size:16px;font-family:Arial'>Here we are using Regression</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6d51e-c061-469a-91f0-7caf02d330d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:38.573236Z",
     "start_time": "2023-07-31T07:43:35.605344Z"
    }
   },
   "outputs": [],
   "source": [
    "qry1= '''CREATE MULTISET TABLE td_glm_cal_ex AS (\n",
    "    SELECT * from TD_GLM (\n",
    "        ON selected_ddp_features_train\n",
    "        USING\n",
    "        InputColumns('[1:51]')\n",
    "        ResponseColumn('delivery_days')\n",
    "        Family('Gaussian')\n",
    "BatchSize(800)\n",
    "MaxIterNum(300)\n",
    "RegularizationLambda(0.0175)\n",
    "Alpha(0.2)\n",
    "IterNumNoChange(200)\n",
    "Tolerance(0.001)\n",
    "Intercept('true')\n",
    "LearningRate('adaptive')\n",
    "InitialEta(0.02)\n",
    "Momentum(0.85)\n",
    "LocalSGDIterations(20)\n",
    ") as dt\n",
    ") WITH DATA;'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry1)\n",
    "except:\n",
    "    db_drop_table('td_glm_cal_ex')\n",
    "    execute_sql(qry1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6855f-56a5-49d7-a720-05f57d295458",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>TD_GLMPredict </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_GLMPredict function predicts target values (regression) and class labels (classification) for test data using a GLM model of the TD_GLM function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0022e68-af50-4358-8591-bc87769c63cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:40.094442Z",
     "start_time": "2023-07-31T07:43:38.998839Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "qry = '''CREATE MULTISET TABLE glm_predict_cal_ex AS (\n",
    "    SELECT * from TD_GLMPredict (\n",
    "      ON (SELECT * FROM selected_ddp_features_test) AS INPUTTABLE\n",
    "      ON td_glm_cal_ex AS ModelTable DIMENSION\n",
    "      USING\n",
    "      IDColumn ('record_number')\n",
    "      Accumulate('delivery_days')\n",
    "    ) AS dt\n",
    ") WITH DATA;'''\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('glm_predict_cal_ex')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ccaf8-8716-42f6-89e7-61e4f548fca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:44.779094Z",
     "start_time": "2023-07-31T07:43:40.097157Z"
    }
   },
   "outputs": [],
   "source": [
    "df= DataFrame.from_query(\"\"\"SELECT * FROM glm_predict_cal_ex;\"\"\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f6b91-a9eb-4c1f-a795-b702b11a2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "df_plot=df.to_pandas(all_rows=True).reset_index().head(100)\n",
    "x = df_plot['record_number']\n",
    "# Put array of years here\n",
    "y1 = df_plot['delivery_days']\n",
    "y2 = df_plot['prediction']\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.lineplot(data= df_plot ,x=\"record_number\",y=\"delivery_days\",ci=None)\n",
    "sns.lineplot(data= df_plot ,x=\"record_number\",y=\"prediction\",ci=None)\n",
    "plt.grid()\n",
    "# plt.xticks(np.arange(1,60, step=1))\n",
    "plt.legend(['Actual Value', 'Predicted Value'], loc='best', fontsize=16)\n",
    "plt.title('Comparison of Actual vs Predicted Delivery Days', fontsize=20)\n",
    "plt.xlabel('Record Number', fontsize=16)\n",
    "plt.ylabel('Delivery Days', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fceccf6-7022-49f0-9525-3509f0b2d67c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>TD_RegressionEvaluator</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_RegressionEvaluator function computes metrics to evaluate and compare multiple models and summarizes how close predictions are to their expected values.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For more information on **RegressionEvaluator**: <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Database-Analytic-Functions/Model-Evaluation-Functions/TD_RegressionEvaluator'> [Link] </a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c4ae5-38ea-4df3-8e7d-487314ad7446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T07:43:49.543002Z",
     "start_time": "2023-07-31T07:43:44.781650Z"
    }
   },
   "outputs": [],
   "source": [
    "DataFrame.from_query('''SELECT * FROM TD_RegressionEvaluator(\n",
    "ON glm_predict_cal_ex as InputTable\n",
    "USING\n",
    "ObservationColumn('delivery_days')\n",
    "PredictionColumn('prediction')\n",
    "Metrics('RMSE','R2','FSTAT')\n",
    "DegreesOfFreedom(5,48)\n",
    "NUMOFINDEPENDENTVARIABLES(5)\n",
    ") as dt;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33de01-8cb8-418f-b291-9709d4985707",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The Metrics of the regression evaluator has the RMSE, R2 and the F-STAT metrics which are specified in the Metrics.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Root mean squared error (RMSE)The most common metric for evaluating linear regression model performance is called root mean squared error, or RMSE. The basic idea is to measure how bad/erroneous the model’s predictions are when compared to actual observed values. So a high RMSE is “bad” and a low RMSE is “good”.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The coefficient of determination — more commonly known as R² — allows us to measure the strength of the relationship between the response and predictor variables in the model. It’s just the square of the correlation coefficient R, so its values are in the range 0.0–1.0. Higher values of R- Squared is Good.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The metrics specified in the Metrics syntax element are displayed. For FSTAT, the following columns are displayed:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>F_score</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>F_Critcialvalue</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>p_value</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>F_Conclusion.</li></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d67497-6bc3-425e-b3cd-d791b43dd5a2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>9. Decision Forest </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The Decision Forest is a powerful method used for predicting outcomes in both classification and regression problems. It's an improvement on the technique of combining (or \"bagging\") multiple decision trees. Normally, building a decision tree involves assessing the importance of each feature in the data to determine how to divide the information. This method takes a unique approach by only considering a random subset of features at each division point in the tree. This forces each decision tree within the \"forest\" to be different from one another, which ultimately improves the accuracy of the predictions. The function relies on a training dataset to develop a prediction model. Then, the TD_DecisionForestPredict function uses the model built by the TD_DecisionForest function to make predictions. It supports regression, binary, and multi-class classification tasks.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Typically, constructing a decision tree involves evaluating the value for each input feature in the data to select a split point. The function reduces the features to a random subset (that can be considered at each split point); the algorithm can force each decision tree in the forest to be very different to improve prediction accuracy. The function uses a training dataset to create a predictive model. The TD_DecisionForestPredict function uses the model created by the TD_DecisionForest function for making predictions. The function supports regression, binary, and multi-class classification.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5f174-77ca-4c2d-903e-b1b6346d4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''Create table DF_train as (\n",
    "SELECT * FROM TD_DecisionForest (\n",
    "ON selected_ddp_features_train AS INPUTTABLE partition by ANY\n",
    "USING\n",
    "  InputColumns('[1:51]')\n",
    "        ResponseColumn('delivery_days')\n",
    "MaxDepth(32)\n",
    "MinNodeSize(1)\n",
    "NumTrees(4)\n",
    "ModelType('REGRESSION')\n",
    "Seed(4)\n",
    "Mtry(-1)\n",
    "MtrySeed(4)\n",
    ") AS dt\n",
    ") with data;\n",
    "'''\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('DF_train')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db575b-de19-4822-86cd-5a6c67762ebf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>TD_DecisionForestPredict</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>TD_DecisionForestPredict function uses the model output by TD_DecisionForest function to analyze the input data and make predictions. This function outputs the probability that each observation is in the predicted class. Processing times are controlled by the number of trees in the model. When the number of trees is more than what can fit in memory, then the trees are cached in a local spool space.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08aa33-7ca4-4fb0-985c-eab46650c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "Create table DF_Predict as (\n",
    "SELECT * FROM TD_DecisionForestPredict (\n",
    "ON selected_ddp_features_train AS InputTable PARTITION BY ANY\n",
    "ON DF_Train AS ModelTable DIMENSION\n",
    "USING\n",
    "   IDColumn ('record_number')\n",
    "   Accumulate('delivery_days')\n",
    ") AS dt) with data;'''\n",
    "\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('DF_Predict')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca94e71-9330-4fa0-89db-08dbb829f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = DataFrame('DF_Predict')\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff07f03-df45-4e84-9f1f-df80cf096516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "df_plot=df_result_pd=df_result.to_pandas(all_rows=True).reset_index()\n",
    "x = df_plot['record_number'][:300]\n",
    "# Put array of years here\n",
    "y1 = df_plot['delivery_days'][:300]\n",
    "y2 = df_plot['prediction'][:300]\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.lineplot(data= df_plot[:300] ,x=\"record_number\",y=\"delivery_days\",ci=None)\n",
    "sns.lineplot(data= df_plot[:300] ,x=\"record_number\",y=\"prediction\",ci=None)\n",
    "plt.grid()\n",
    "# plt.xticks(np.arange(1,60, step=1))\n",
    "plt.legend(['Actual Value', 'Predicted Value'], loc='best', fontsize=16)\n",
    "plt.title('Comparison of Actual vs Predicted Delivery Days', fontsize=20)\n",
    "plt.xlabel('Record Number', fontsize=16)\n",
    "plt.ylabel('Delivery Days', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cccde-d82f-4cdc-84e6-49915a0bf08f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>TD_RegressionEvaluator</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_RegressionEvaluator function computes metrics to evaluate and compare multiple models and summarizes how close predictions are to their expected values.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For more information on **RegressionEvaluator**: <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Database-Analytic-Functions/Model-Evaluation-Functions/TD_RegressionEvaluator'> [Link] </a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6985ed-63f7-48a4-a34e-8c4b8975e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * FROM TD_RegressionEvaluator(\n",
    "ON DF_Predict as InputTable\n",
    "USING\n",
    "ObservationColumn('delivery_days')\n",
    "PredictionColumn('prediction')\n",
    "Metrics('RMSE','R2','FSTAT')\n",
    "DegreesOfFreedom(5,48)\n",
    "NUMOFINDEPENDENTVARIABLES(5)\n",
    ") as dt;\n",
    "'''\n",
    "\n",
    "DF_eval=DataFrame.from_query(query)\n",
    "DF_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969939d3-fadc-4404-a430-3cd45dc70b00",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The Metrics of the regression evaluator has the RMSE, R2 and the F-STAT metrics which are specified in the Metrics.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Root mean squared error (RMSE)The most common metric for evaluating linear regression model performance is called root mean squared error, or RMSE. The basic idea is to measure how bad/erroneous the model’s predictions are when compared to actual observed values. So a high RMSE is “bad” and a low RMSE is “good”.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The coefficient of determination — more commonly known as R² — allows us to measure the strength of the relationship between the response and predictor variables in the model. It’s just the square of the correlation coefficient R, so its values are in the range 0.0–1.0. Higher values of R- Squared is Good.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The metrics specified in the Metrics syntax element are displayed. For FSTAT, the following columns are displayed:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>F_score</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>F_Critcialvalue</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>p_value</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>F_Conclusion.</li></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c90a7-cc95-4fd5-a633-b5ded163fd7c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>10. Conclusion</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have seen an end-to-end exploration process for Shipping Time Predictions using ClearScape Analytics on Teradata Vantage. We have preprocessed data, created model using the InDB Analytic functions and compared the performance of the 2 models. The data we have used is sample data and so the results may not be accurate. Thanks to the in-database capabilities offered by Teradata Vantage with ClearScape Analytics, we were able to run this exploration with the smallest notebook instance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d384c07-258a-4c93-be28-4b59f8a63010",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>11. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Work Tables</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752650cb-88ee-438f-ba12-56ebf331e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['DF_Predict','DF_train','glm_predict_cal_ex','td_glm_cal_ex',\n",
    "          'selected_ddp_features_train','selected_ddp_features_test','delivery_date_test_dataset', \n",
    "          'delivery_date_train_dataset','delivery_date_ads','ohe_dataset','ohe_fit','delivery_date_dataset_final',\n",
    "          'delivery_date_complete_dataset']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "    except:\n",
    "        pass\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d42f5-5b53-4e7f-b6d1-92f45c6b8f2a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Databases and Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7c0a8-eca9-4b2e-8799-9a767aa51e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_ShippingTimePrediction');\" \n",
    "#Takes 45 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c3f95-9746-4ec0-9bae-cb283397e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c40f5-9b68-4d5b-a043-991a25fc56d3",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Copyright © Teradata Corporation - 2023. All Rights Reserved.</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "394px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
