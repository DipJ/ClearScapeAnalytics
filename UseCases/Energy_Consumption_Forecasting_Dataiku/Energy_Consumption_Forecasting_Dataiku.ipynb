{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acquired-consideration",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Energy Consumption Forecasting using Dataiku</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701a2ac-9d7c-4498-a23c-7ae3955f6a32",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Introduction:</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this business use case, we leverage the power of Dataiku and Teradata Vantage to enhance our machine learning capabilities and enable scalable model scoring. Our goal is to efficiently utilize the strengths of both platforms to streamline our data analysis and decision-making processes.\n",
    "<br>\n",
    "<img src=\"images/logo.jpg\" alt=\"Dataiku X Teradata\">\n",
    "<br>\n",
    "Dataiku serves as a comprehensive data science platform that empowers us to read data from Teradata Vantage, a powerful analytical database. By leveraging Dataiku's seamless integration with Vantage, we can easily extract and analyze large volumes of data stored within the database.\n",
    "<br>\n",
    "<br>\n",
    "Within Dataiku, we harness its rich set of features and functionalities to build and fine-tune multiple machine learning models. With its user-friendly interface and wide array of machine learning algorithms, we can develop models that are tailored to our specific business requirements. Dataiku enables us to handle data preprocessing, feature engineering, model training, and evaluation, providing a complete end-to-end data science workflow.\n",
    "<br>\n",
    "<br>\n",
    "Once the models are trained and validated within Dataiku, we can seamlessly bring them back to Teradata Vantage. Here, we leverage Vantage's advanced functionality known as BYOM (Bring Your Own Model), which allows us to score our machine learning models directly within the Vantage environment. This capability empowers us to perform model scoring at scale, leveraging the high-performance and parallel processing capabilities of Vantage.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The dataset used in this demo represents electricity consumption in Norway from the 1st of January 2016 to the 31st of August 2019. Each line in this dataset reflects consumption for one hour. Apart from electricity consumption, this datamart also reflects additional data: weather from multiple sources, daylight information and labour calendar. We collected all data from open data sources.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>But what if I don't have Dataiku?</b>  Don't worry, we will execute the steps before Dataiku would be used, show you screen shots of what the Dataiku user would be doing, and then we've included the completed model that you will import into Vantage for scoring.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419e9f9-0356-4e40-b59f-34281de410b2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d4207-bc98-428b-93d3-7955c99a34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from jdk4py import JAVA, JAVA_HOME, JAVA_VERSION\n",
    "\n",
    "from teradataml import *\n",
    "from teradataml.dataframe.dataframe import DataFrame, in_schema\n",
    "from teradataml.context.context import create_context, remove_context\n",
    "from teradataml.options.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "display.max_rows = 5\n",
    "\n",
    "# Modify the following to match the specific client environment settings\n",
    "configure.val_install_location = 'val'\n",
    "configure.byom_install_location = 'mldb'\n",
    "os.environ['PATH'] = os.pathsep.join([os.environ['PATH'], str(JAVA_HOME), str(JAVA)[:-5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d75d8d-d58b-4bb8-87e5-cdf1a8add76a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>1. Initiate a connection to Vantage</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'> <b>Let's start by connecting to the Teradata system </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef7d0b-af14-43a8-8725-7952ea3d0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "eng.execute('''SET query_band='DEMO=Energy_Consumption_Forecasting_Dataiku.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0b413-cec4-443f-957d-185172c56e48",
   "metadata": {},
   "source": [
    "<img src=\"images/td-dataiku.png\" alt=\"Teradata and Dataiku Integration\">\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The flow of this notebook is as shown above.</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "  <li>We get the data from cloud into Vantage.</li>\n",
    "  <li>We process the data and build the model in Dataiku.</li>\n",
    "  <li>We export the model back to Vantage in a PMML format file.</li>\n",
    "  <li>We score the model inside Vantage without having to move the data.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0be93a-2c49-4708-a17e-3e9e1401c8fc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20217e-9600-4ef7-adc5-6ba10c76a5cc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. You can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. You may switch which mode you choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a490a6-75b8-4d4d-8986-46b03fc57081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_Energy_cloud');\"        # Takes 15 seconds\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_Energy_local');\"        # Takes 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c1780-0cca-4b25-8fc5-cefbf8bde51b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step â€“ if you want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c657c8-2122-4426-9d38-c5a619f7817a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393eb65-b65d-4ca2-a1e2-1f546512449e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>2. Dataiku</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now that you have got the data in Vantage, let's see how to create a dataiku flow which looks like below.</p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Dataiku Flow:</b></p>\n",
    "<img src=\"images/flow.jpeg\" alt=\"Dataiku Flow\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a03124-b708-4ff9-8873-a31681ecc126",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.1 Create Connection</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a new Connection: Go to the 3x3 icon on the top right corner of the screen, then click Administration > Connections > + New Connection and choose Teradata from the dropdown.\n",
    "<br>\n",
    "<br>\n",
    "Configure the following essential parameters to establish a connection to Teradata:</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Host: You can get hostname from clearscape dashboard</li>\n",
    "    <li>Default Database: demo_user</li>\n",
    "    <li>User: Your username is demo_user</li>\n",
    "    <li>Password: Your password</li>\n",
    "    <li>Advanced JDBC properties: CHARSET -> UTF8, TMODE -> TERA</li>\n",
    "    <li>Tick the checkbox for Autocommit mode in Advanced</li>\n",
    "    <li>Click on CREATE</li>\n",
    "</ol>\n",
    "<img src = 'images/connection.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000091c5-6b3c-49e8-ae65-42a2db0c02d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.2 Import dataset</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a new project and click on <b>+ IMPORT YOUR FIRST DATASET</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Select your connection from the dropdown</li>\n",
    "    <li>Enter Table as <b>consumption</b></li>\n",
    "    <li>Enter Schema as <b>DEMO_Energy_db</b></li>\n",
    "    <li>Click on CREATE</li>\n",
    "</ol>\n",
    "<img src = 'images/data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041578a0-9bcf-4f72-b56e-aba815394962",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.3 Data Preparation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a new python recipe by clicking on <b>Python</b> icon in the right sidebar as shown below.</p>\n",
    "<img src = 'images/fig1.png'>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Enter output dataset name</li>\n",
    "    <li>Select your connection from the dropdown</li>\n",
    "    <li>Click on CREATE DATASET</li>\n",
    "    <li>Click on CREATE RECIPE</li>\n",
    "</ol>\n",
    "<img src = 'images/fig2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af344d9-e109-4f61-8854-cfb02e073032",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You can write your data processing and preparation code here in the recipe created above. Sample code is given below. Use your credentials for creating context.\n",
    "<br>\n",
    "<br>\n",
    "Hit Run button after creating the recipe.</p>\n",
    "\n",
    "Code: [transformation.py](./transformation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034be18-97b6-4c3c-b9eb-9e6ff0a55edc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We'll now use the dataiku function to split the data into train and test dataset. Create a new recipe by clicking on <b>Split</b> icon in the right sidebar as shown below.</p>\n",
    "<img src = 'images/fig3.png'>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Create two output dataset (train_df and test_df)</li>\n",
    "    <li>Select your connection from the dropdown</li>\n",
    "    <li>Click on CREATE DATASET</li>\n",
    "    <li>Click on CREATE RECIPE</li>\n",
    "    <img src = 'images/fig4.png'>\n",
    "    <li>Click on <b>Dispatch percentiles of sorted data</b>. This is because the data is time series data and we do not want to split it randomly.</li>\n",
    "    <img src = 'images/fig5.png'>\n",
    "    <li>Select <b>TD_TIMECODE</b> column from the dropdown. Enter desired ratio to split the data.</li>\n",
    "    <img src = 'images/fig6.png'>\n",
    "</ol>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note:</b>The tables here will be saved as {Project_Name}_train_df and {Project_Name}_test_df. Here in this demo they are stored as DATAIKUBYOM_train_df and DATAIKUBYOM_test_df.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Hit Run button after creating the recipe.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dba319-903b-4dcd-993b-d271d6d8410a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>2.4 Model Training</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a new python recipe by clicking on <b>Python</b> icon in the right sidebar as shown below. We'll use the train_df for model training purpose.</p>\n",
    "<img src = 'images/fig7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fede4a-12d4-49f0-beb2-9b0c752d5770",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You can write your model training and exporting code here in the recipe created above. Sample code is given below. Use your credentials for creating context.\n",
    "<br>\n",
    "<br>\n",
    "Hit Run button after creating the recipe.</p>\n",
    "\n",
    "Code: [model_training.py](./model_training.py) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954082f9-8a77-44ef-acb6-032642396c92",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><p style = 'font-size:16px;font-family:Arial'>The previous step stores the models created in ClearScape on the local machine in a table called <b>dataiku_models</b>. In the next section, we will utilize these trained models to perform scoring within Vantage, eliminating the need to transfer data from Vantage.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Alternatively, you have the option to import the Dataiku project directly into Dataiku platform. You can find a zip file named <b>DATAIKUBYOM.zip</b> in this folder. You can utilize this file to set up the Dataiku project in Dataiku's platform, enabling seamless integration with your existing Dataiku environment.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df57de-c492-460b-a22b-3ce8010d5a91",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"\n",
    "<p style = 'font-size:16px;font-family:Arial'><p style = 'font-size:16px;font-family:Arial'><i><b>Note</b>: If you do not have Dataiku or did not perform the above steps, the following cell will do the required setup to run the remaining notebook.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0ba2-a488-4538-892e-89a57853d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(pd.read_csv('./train_df.csv'), table_name='DATAIKUBYOM_train_df', schema_name = 'demo_user', if_exists = 'replace')\n",
    "copy_to_sql(pd.read_csv('./test_df.csv'), table_name='DATAIKUBYOM_test_df', schema_name = 'demo_user', if_exists = 'replace')\n",
    "\n",
    "# Load the PMML file into Vantage\n",
    "\n",
    "model_ids = ['lr', 'rf']\n",
    "model_files = ['energy_consumption_LR.pmml', 'energy_consumption_RF.pmml']\n",
    "table_name = 'dataiku_models'\n",
    "\n",
    "for model_id, model_file in zip(model_ids, model_files):\n",
    "    try:\n",
    "        save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "    except Exception as e:\n",
    "        # if our model exists, delete and rewrite\n",
    "        if str(e.args).find('TDML_2200') >= 1:\n",
    "            delete_byom(model_id = model_id, table_name = table_name)\n",
    "            save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to save the model '{model_id}' in '{table_name}' due to the following error: {e}\")\n",
    "\n",
    "# Show the bank_models table\n",
    "list_byom(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-poetry",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>3. Model Scoring and Evaluation</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The final step in this process is to test the trained model.  The PMMLPredict function will take the stored pipeline object (including any data preparation and mapping tasks) and execute it against the data on the Vantage Nodes.  Note that we can keep many models in the model table, with versioning, last scored timestamp, or any other management data to allow for the operational management of the process.</p>\n",
    "        <ol style = 'font-size:16px;font-family:Arial'>\n",
    "            <li>Create a pointer to the model in Vantage</li>\n",
    "            <li>Execute the Scoring function using the model against the testing data</li>\n",
    "            <li>Visualize the results</li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3f5de-48ac-40e8-8815-12066e927438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a pointer to the model\n",
    "table_name = 'dataiku_models'\n",
    "model_id = 'lr'\n",
    "model_lr = retrieve_byom(model_id, table_name=table_name, schema_name=\"demo_user\")\n",
    "df_test = DataFrame('DATAIKUBYOM_test_df')\n",
    "\n",
    "result_lr = PMMLPredict(\n",
    "            modeldata = model_lr,\n",
    "            newdata = df_test,\n",
    "            accumulate = ['TD_TIMECODE','consumption'],\n",
    "            ).result.to_pandas(all_rows = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31e0ce-b465-44eb-87ac-42af306d4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4881e-3107-4567-aad6-4daac51b6231",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the above step, we use the PMMLPredict method from teradataml library to score the model in the database. The PMMLPredict function in Teradata allows users to score the PMML model directly on the data in the Vantage system, without having to move the data or the model outside the system. This can help to improve the efficiency and security of the scoring process.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c73a7-4832-408b-a3fb-844612a06aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a pointer to the model\n",
    "table_name = 'dataiku_models'\n",
    "model_id = 'rf'\n",
    "model_rf = retrieve_byom(model_id, table_name=table_name, schema_name=\"demo_user\")\n",
    "df_test = DataFrame('DATAIKUBYOM_test_df')\n",
    "\n",
    "result_rf = PMMLPredict(\n",
    "            modeldata = model_rf,\n",
    "            newdata = df_test,\n",
    "            accumulate = ['TD_TIMECODE','consumption'],\n",
    "            ).result.to_pandas(all_rows = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989b9d6-cd74-4e66-a2c6-c08051f4bcc1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>4. Visualize the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e58fc8-b32a-4e73-93b0-d280381ada63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subplots\n",
    "df_lr = result_lr\n",
    "df_rf = result_rf\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Calculate RMS errors\n",
    "normalize_value = int(DataFrame('DATAIKUBYOM_train_df').to_pandas(all_rows=True).sort_values('TD_TIMECODE').tail(24).mean()['consumption'])\n",
    "rms_lr = mean_squared_error(result_lr['consumption'], result_lr['prediction'].astype(float) + normalize_value, squared=False)\n",
    "rms_rf = mean_squared_error(result_rf['consumption'], result_rf['prediction'].astype(float) + normalize_value, squared=False)\n",
    "\n",
    "# Plot 1: Linear Regression\n",
    "ax1.plot(df_lr.index, df_lr['consumption'], label=f'Actual Consumption', color='#1f77b4', linewidth=2)\n",
    "ax1.plot(df_lr.index, df_lr['prediction'].astype(float) + normalize_value, label=f'Linear Regression (RMS={rms_lr:.2f})', color='#ff7f0e', linestyle='--')\n",
    "ax1.set_ylabel('Energy Consumption')\n",
    "ax1.set_title('Energy Consumption Prediction - Linear Regression')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', linestyle='--')\n",
    "\n",
    "# Plot 2: Random Forest\n",
    "ax2.plot(df_rf.index, df_rf['consumption'], label=f'Actual Consumption', color='#1f77b4', linewidth=2)\n",
    "ax2.plot(df_rf.index, df_rf['prediction'].astype(float) + normalize_value, label=f'Random Forest (RMS={rms_rf:.2f})', color='#ff7f0e', linestyle='--')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Energy Consumption')\n",
    "ax2.set_title('Energy Consumption Prediction - Random Forest')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', linestyle='--')\n",
    "\n",
    "# Add a background color\n",
    "fig.patch.set_facecolor('#f2f2f2')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5e1b8-f01e-490b-a8f9-15f2d0a201b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above graph displays the Root Mean Squared (RMS) error values for both Linear Regression and Random Forest models. The lower the RMS error value, the better the model's performance. As we can see, Random Forest outperforms Linear Regression in predicting energy demand, as it has a lower RMS error value. Therefore, Random Forest is more suitable for proactively predicting energy demand in our use case.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This demonstration has illustrated a simplified - but complete - overview of how a typical machine learning workflow can be improved using Vantage in conjunction with 3rd-party tools and techniques.  This combination allows users to leverage 3rd-party innovation with Vantage's operational scale, power, and stability.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e9e44-3119-4852-8574-9303a40fcd52",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>5. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ce515-d02e-4872-91ae-7735a9db9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_drop_table(table_name='DATAIKUBYOM_train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80323e7-2d7b-40e1-be91-2ed040b795bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_drop_table(table_name='DATAIKUBYOM_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024e65c-581b-4b59-9d09-75995a7d07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_drop_table(table_name='dataiku_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377fef9-b377-4b59-a313-5eb9784dba93",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472c2ca-65d0-4116-9c45-6411fcea52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_Energy');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0b73e-0c50-4110-be9c-05aaf1181960",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Copyright Â© Teradata Corporation - 2023. All Rights Reserved.</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
