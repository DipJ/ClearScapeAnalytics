{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bc532d-c78c-4c89-b191-242da0733f39",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Chat with documents using Vantage as VectorDB and Generative AI\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff71661-19b4-423a-867a-7c815b064c81",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>Introduction:</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the Chat with documentation system using Generative AI demo, the combination of <b>RAG, Langchain, and LLM models</b> allows users to ask queries in layman's terms, retrieve relevant information from the Vector store, and generate accurate and concise answers based on the retrieved data. This integration of retrieval-based and generative-based approaches provides a powerful tool for extracting knowledge from structured sources and delivering user-friendly responses.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo we will build Chatbot using Panel (for chat UI), LangChain, a powerful library for working with LLMs like GPT-3.5, GPT-4, Bloom, etc. and JumpStart in ClearScape notebooks, a system is built where users can ask business questions in natural English and receive answers with data drawn from the relevant databases.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The following diagram illustrates the architecture.</p>\n",
    "\n",
    "<center><img src=\"images/header_chat_td.png\" alt=\"architecture\"  width=800 height=800/></center>\n",
    "\n",
    "\n",
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Before going any farther, let's get a better understanding of RAG, LangChain, and LLM.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'><b><li> Retrieval-Augmented Generation (RAG):</li></b></ol>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> &emsp;  &emsp;RAG is a framework that combines the strengths of retrieval-based and generative-based approaches in question-answering systems.It utilizes both a retrieval model and a generative model to generate high-quality answers to user queries. The retrieval model is responsible for retrieving relevant information from a knowledge source, such as a database or documents. The generative model then takes the retrieved information as input and generates concise and accurate answers in natural language.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>A typical RAG (Retrieval-and-Generation) application has two main components:</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Indexing:</b> a pipeline for ingesting data from a source and indexing it. This usually happens offline. The indexing process involves several steps, including loading the data, splitting it into smaller chunks, and storing and indexing the splits. This is often done using a VectorStore and Embeddings model.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Retrieval and generation:</b> the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The retrieval process involves searching the index for the most relevant data based on the user query, and then passing that data to the model for generation.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The most common full sequence from raw data to answer looks like:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Indexing</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li><b>Load:</b> Load: First we need to load our data. We'll use <code>PyPDFLoader</code> for this.</li>\n",
    "    <li><b>Split:</b> Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't in a model's finite context window. Here, our pdf document will be splits into pages.</li>\n",
    "    <li><b>Store:</b> We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model</li>\n",
    "    </ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The following diagram illustrates the architecture of load, split and store.</p>\n",
    "\n",
    "<center><img src=\"images/rag_load_store.png\" alt=\"rag indexing architecture\"  width=800 height=600/></center>\n",
    "<center>image source: <a href=\"https://python.langchain.com/docs/use_cases/question_answering/\">langchain.com</a></center>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Retrieval and generation</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li><b>Retrieval:</b> During runtime, the user inputs a query. We first generate embeddings for it, which are then passed to the Vantage in the db_function <b>TD_VectorDistance</b> to retrieve similar documents as context. This context is then fed into the LLM model.</li>\n",
    "    <li><b>Generation:</b> Finally, the model generates an answer based on the retrieved data. The answer is then presented to the user.</li>\n",
    "    </ul>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The following diagram illustrates the architecture of retrieval and generation.</p>\n",
    "<center><img src=\"images/rag_retrieval_generation_td.png\" alt=\"retrieval generation architecture\" width=800 height=600/></center>\n",
    "<center>image source: <a href=\"https://python.langchain.com/docs/use_cases/question_answering/\">langchain.com</a></center>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C' start=\"2\"><b><li> Langchain:</li></b></ol>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> &emsp;  &emsp; LangChain is a framework that facilitates the integration and chaining of large language models with other tools and sources to build more sophisticated AI applications. LangChain does not serve its own LLMs; instead, it provides a standard way of communicating with a variety of LLMs, including those from OpenAI and HuggingFace. LangChain accelerates the development of AI applications with building blocks. We learn the leverage the following building blocks in this notebook:</p>\n",
    " \n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li> <b> LLMs</b> – LangChain's <code>llm</code> class is designed to provide a standard interface for all LLM it supports.   </li>\n",
    "    <li> <b> PromptTemplate</b>  - LangChain’s <code>PromptTemplate</code> class are predefined structures for generating prompts for LLM’s. They can be reused across different LLM's.</li>\n",
    "    <li> <b> Chains</b> – When we build complex AI applications, we may need to combine multiple calls to LLM’s and to other components  LangChain’s <code>chain</code> class allows us to link calls to LLM’s and components. The most common type of chaining in any LLM application is combining a prompt template with an LLM and optionally an output parser. </li>\n",
    "</ol>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C' start=\"3\"><b><li> LLM Models (Large Language Models):</li></b></ol>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> &emsp;  &emsp; LLM models refer to the large-scale language models that are trained on vast amounts of text data.\n",
    "These models, such as GPT-3 (Generative Pre-trained Transformer 3),  GPT-3.5, GPT-4, HuggingFace BLOOM, LLaMA, Google's FLAN-T5, etc. are capable of generating human-like text responses. LLM models have been pre-trained on diverse sources of text data, enabling them to learn patterns, grammar, and context from a wide range of topics. They can be fine-tuned for specific tasks, such as question-answering, natural language understanding, and text generation.\n",
    "LLM models have achieved impressive results in various natural language processing tasks and are widely used in AI applications for generating human-like text responses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca49b9d-df0b-400a-acf8-0252ab8a2618",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233c'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>Generate the embeddings</li>\n",
    "    <li>Load the existing embeddings to DB</li>\n",
    "    <li>Calculate the VectorDistance using Teradata Vantage in-DB function</li>\n",
    "    <li>LLM</li>\n",
    "    <li>Chat with documents</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833b5bf-74af-42be-8543-3782e1da95dc",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>1. Configuring the environment</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94f1e8-489b-4084-80c6-c9cff1ef6ee8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>1.1 Install the required libraries</b></p>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note:</b>The installation of the required libraries will take approximately <b>4 to 5 minutes</b> for the first-time installation. However, if the libraries are already installed, the execution will complete within 5 seconds.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6027a7-888d-441f-abc7-a6ea1c45f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b160ce-5ace-4116-86b6-394d6502553b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>The above statements will install the required libraries to run this demo. Be sure to restart the kernel after executing the above lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "    </div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>To ensure that the Chatbot interface reflects the latest changes, please reload the page by clicking the 'Reload' button or pressing F5 on your keyboard for <b>first-time only</b> This will update the notebook with the latest modifications, and you'll be able to interact with the Chatbot using the new libraries.</i></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61067c88-2e9a-4c92-985b-34dc4ab74a13",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50a4aa-1211-44fc-8166-317c35253207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import tqdm\n",
    "from tqdm.notebook import *\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# teradata lib\n",
    "from teradataml import *\n",
    "\n",
    "# helper functions\n",
    "from utils.sql_helper_func import *\n",
    "from utils.opensource_helper_func import *\n",
    "\n",
    "# LLM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11368d-2efb-4906-8e28-d50f0bca6429",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i>The code above will download the necessary models to generate the embeddings required to run this demo. The initial download may take approximately 50-60 seconds minutes if you are running this demo for the first time in this environment. However, subsequent runs will be much faster since the models will already be available locally.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718f8-7af4-4d1a-abc7-a860eb7cbae3",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>2. Connect to Vantage and OpenAI</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83770df6-b923-4cc4-a839-de55c62b32ae",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cfc91-93ed-45b9-98ba-73b91a50c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "execute_sql('''SET query_band='DEMO= Chat_with_docs_VantageDB_GenAI_Python.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153b889-0924-4e0f-acc8-6b0d440c77b3",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>2.2 Get the OpenAI API key</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e10add-4b3c-4fc5-9359-0b44737bba0f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In order to utilize this demo, you will need an OpenAI API key. If you do not have one, please refer to the instructions provided in this guide to obtain your OpenAI API key: </p>\n",
    "\n",
    "[Openai_setup_api_key_guide](..//Openai_setup_api_key/Openai_setup_api_key.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87f56a-894d-4c45-9c79-0fd468336eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# enter your openai api key\n",
    "api_key = getpass.getpass(\"\\n Please Enter OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22db506-84a7-406b-be9f-9fe69d268ba4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f3fe2-ed63-4838-bb19-4c0d0157453d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>3. Data Exploration</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The Chat with documentation demo aims to demonstrate how users can interact with documents such as insurance policy wordings, invoices, and other similar documents through a conversational interface.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The Traveller Easy Single Trip - International insurance policy is a comprehensive travel insurance plan that provides cover for a wide range of risks, including medical expenses, trip cancellation, loss of luggage, and personal accident. The policy is designed to be affordable and flexible, and it can be purchased online or over the phone.<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f35d2-fc39-4a0a-9008-3850cd58e50e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The source data from <a href=\"https://axa-com-my.cdn.axa-contento-118412.eu/axa-com-my/3d2f84a5-42b9-459b-911a-710546df0633_Policy+wording+-+SmartTraveller+Easy+Single+Trip+-+International+%280820%29.pdf\">AXA</a> is loaded in FAISS as Vector Database.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Now, let's use <code>PyPDFLoader</code> library to read the pdf document and split it into pages.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7718aa2-2787-471d-85f4-fd592b151d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/SmartTraveller_International.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6d954-b72f-41c3-9f8c-bc15cf74d672",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>There are 24 pages that describe the policy in detail.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a78f9c-2c05-4221-86d5-3a92ee2dc525",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>3.1 Do you want to generate the embeddings?</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have already generated embeddings for this insurance document and stored them in files.</p>\n",
    "\n",
    "<center><img src=\"images/decision_emb_gen.png\" alt=\"embeddings_decision\"  width=300 height=300/></center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note: If you would like to skip the embedding generation step and move on to the next section, please click  <a href=\"#section50\">here</a> to skip.</b></i></p>\n",
    "</div>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To save time, you can move to the already generated embeddings section. However, if you would like to see how we generate the embeddings, or if you need to generate the embeddings for a different dataset, then continue to the following section.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1626b5-1693-4683-a275-5aa80c862f8d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<a id='section4'></a>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>4. Generate the embeddings </b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.1 Generate the embeddings for product table</b></p>    \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To generate embeddings for a document content, we will use the FlagEmbedding repo from HuggingFace, which uses the <code>BAAI/bge-large-en</code> model to generate embeddings. FlagEmbedding are a type of word embedding that can be used to represent products in a way that captures their semantic meaning. This means that the embeddings will reflect the meaning of the text content, not just the words that are used. Please refer to the <a href=\"https://huggingface.co/BAAI/bge-large-en\"> Embeddings documentation</a> for more information about embeddings.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The FlagEmbedding takes a text string as input and returns a vector of numbers that represent the embedding. The length of the vector depends on the model that you are using. For example, the <code>BAAI/bge-large-en</code> model returns a vector of 1024 numbers.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo, we will use <code>BAAI/bge-large-en</code> as the model which is rank 1st in <a href=\"https://huggingface.co/spaces/mteb/leaderboard\"> MTEB leader board</a></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2386198-83cb-4c17-9eb2-597807b58720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_content(pages):\n",
    "    docs = [p.page_content for p in pages]\n",
    "\n",
    "    # split the page content\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=30,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.create_documents(docs)\n",
    "\n",
    "    texts_data = []\n",
    "    for t in docs:\n",
    "        texts_data.append(t.page_content)\n",
    "\n",
    "    # generate the dataframe\n",
    "    df = pd.DataFrame(data=texts_data, columns=[\"text\"])\n",
    "    df[\"id\"] = range(1090, len(df.index) + 1090)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872a85d-912d-4178-baaf-796fa10553b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To generate the embeddings, we will call the <b>get_embeddings_hf()</b> function. This function will convert the Teradata DataFrame to a Pandas DataFrame and generate the embeddings. Once the embeddings are generated, we will store them in separate columns so that we can pass them to the <b>VectorDistance()</b> function later on.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note: The embedding generation step is estimated to take approximately 1 hour to complete. If you prefer to skip this step and proceed to the next section, please click  <a href=\"#section50\">here</a> to skip.</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4ee5d-d1ea-4dd5-bc4d-ea08c80ee32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_emb_generator(table_name, file_name, chunksize=100):\n",
    "    wallclock_time_start = timeit.default_timer()\n",
    "\n",
    "    # delete the records\n",
    "    delete_emb_from_sql(table_name, eng)\n",
    "\n",
    "    # Read the data in chunks of 1000 rows\n",
    "    temp_df = pd.read_csv(file_name, chunksize=chunksize)\n",
    "\n",
    "    # Iterate over the chunks\n",
    "    for chunk in tqdm(\n",
    "        temp_df,\n",
    "        desc=\"Overall progress \",\n",
    "    ):\n",
    "        print(\"Data size in current chunk: \", chunk.shape)\n",
    "        df_chunk = get_embeddings_hf(chunk)\n",
    "\n",
    "        copy_emb_to_sql(table_name=table_name, tdf=df_chunk)\n",
    "        print(f\"{df_chunk.shape[0]} products saved to sql \\n\")\n",
    "\n",
    "    wallclock_time_end = timeit.default_timer()\n",
    "    wallclock_time = wallclock_time_end - wallclock_time_start\n",
    "    print(\"wallclock time:\\t\", wallclock_time)\n",
    "    print(\"-\" * 50, \" complete \", \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90747cc-4cc2-42bf-a7ac-486f6c3ed2eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Please be patient:</b> Generating embeddings for 600 document contents may take up to 8 to 10 minutes. It is depends on number of APMS in the database. Since the volume of data is large and the machine is small, going through the below code could take up to 10 minutes. </i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf7b0e-4907-40cd-be0e-f828dfe53f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read pdf file content\n",
    "df = read_document_content(pages)\n",
    "print(\"Data information: \\n\", df.shape)\n",
    "\n",
    "df.to_csv(\"df_text.csv\", index=False)\n",
    "\n",
    "# generate the embeddings\n",
    "recursive_emb_generator(\n",
    "    table_name=\"text_embeddings\", file_name=\"df_text.csv\", chunksize=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7fd6f8-e868-4674-9412-4bc2359ad61b",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<a id='section42'></a>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.2 Display the text embeddings</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33438fb9-e0fb-46ea-81fa-9b794db69aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = DataFrame(in_schema(\"demo_user\", \"text_embeddings\"))\n",
    "print(\"Data information: \\n\", text_embeddings.shape)\n",
    "text_embeddings.sort(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668715a-b5f7-4727-a786-62e0f963e4a9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can see that generated embeddings for all of the products are in vector of 1024 columns. </p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example: The generated embeddings for product name: <b>What is the email address of Customer Service Department?</b> consists of 1024 numbers and looks like:<br>\n",
    "    <code>-0.038744\t-0.016937\t-0.017475\t0.003624\t0.00744\t-0.00275\t0.02374</code></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ac502-2983-4b13-958f-18584b870641",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note</b>: If you're generating embeddings for a new document and plan to store it as a file, consider uncommenting the code below. Doing so will significantly speed up the process in future runs by skipping section 4 altogether.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e859f-8f65-4a17-a536-4365692cf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the embeddings if you're generating for new document for speed up in next run\n",
    "# df = text_embeddings.to_pandas().reset_index()\n",
    "# df.to_parquet('./embeddings/df_SmartTraveller_International.parquet.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe9764-4e59-4c49-ac76-85823a910fd0",
   "metadata": {},
   "source": [
    "<a id='section50'></a>\n",
    "\n",
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>5. Load the existing embeddings to DB</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>5.1 Load the products and searched products embeddings</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this demo, we will load existing embeddings from files to a database. This will allow us to perform further processing on the embeddings.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note</b>: If you have already executed the Generate the embeddings section, then below code will be skipped automatically.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46a30a-0dc4-41bd-82e8-0ce9d4884c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_section4_executed = False\n",
    "try:\n",
    "    is_section4_executed = (\n",
    "        DataFrame.from_query(\n",
    "            \"select count(*) as emb_cnt from text_embeddings\"\n",
    "        ).get_values()[0][0]\n",
    "        > 0\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc8e33-08d5-48e7-9293-c6dc78d44b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_section5_desc_start():\n",
    "    return \"\"\"<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The code above first reads the data from the files. The files contain information about the product embeddings and the customer's searched product embeddings. The code then loads the data into a permanent table in SQL. Once the data is loaded, we will use the Vantage in-database function <code>VectorDistance</code> to calculate the distance between the product embeddings and the customer's searched product embeddings. The data contains product embeddings, which are lists of numerical values, or vectors.</p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>The embeddings file contains over 6,000 records, each with 1,028 numerical features. This means that the file is quite large and it may take some time to load it into SQL.</p>\n",
    "    <div class=\"alert alert-block alert-info\" id=\"no-azure\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Note</b>: Please be patient. The code above is loading data from files and copying it to SQL. This process may take 30-50 seconds.</i></p>\n",
    "    </div>\"\"\"\n",
    "\n",
    "def get_section5_desc_end():\n",
    "    return \"\"\"<a id='section52'></a><p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>5.2 Display the product embeddings</b></p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>To give you a better idea of what the embeddings look like, here are the first five rows of the product embeddings:</p>\"\"\"\n",
    "\n",
    "def get_section5_desc_sample():\n",
    "    return \"\"\"<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can see that generated embeddings for all of the products are in vector of 1024 columns. </p>\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example: The generated embeddings for product name: <b>What is the email address of Customer Service Department?</b> consists of 1024 numbers and looks like:<br>\n",
    "    <code>-0.038744\t-0.016937\t-0.017475\t0.003624\t0.00744\t-0.00275\t0.02374</code></p>\"\"\"\n",
    "\n",
    "\n",
    "def load_the_emb():\n",
    "    is_section5_executed = False\n",
    "\n",
    "    if not is_section4_executed:\n",
    "        is_section5_executed = True\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        display(Markdown(get_section5_desc_start()))\n",
    "\n",
    "        # load product_embeddings to sql\n",
    "\n",
    "        text_embeddings_os_prq = pd.read_parquet(\n",
    "            \"./embeddings/df_SmartTraveller_International.parquet.gzip\"\n",
    "        )\n",
    "\n",
    "        print(\"embeddings shape\", text_embeddings_os_prq.shape)\n",
    "\n",
    "        delete_and_copy_embeddings(\n",
    "            table_name=\"text_embeddings\", tdf=text_embeddings_os_prq, eng=eng\n",
    "        )\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "\n",
    "        load_time = end - start\n",
    "\n",
    "        print(f\"embeddings load time:\\t\", load_time)\n",
    "\n",
    "        display(Markdown(get_section5_desc_end()))\n",
    "\n",
    "        product_embeddings_os = DataFrame(in_schema(\"demo_user\", \"text_embeddings\"))\n",
    "\n",
    "        # display(Markdown(get_section5_desc_sample()))\n",
    "\n",
    "        return product_embeddings_os, is_section5_executed\n",
    "    else:\n",
    "        # print(\"Section 4: Generate the embeddings is already executed!\")\n",
    "\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"\"\"<br><div class=\"alert alert-block alert-success\">\n",
    "        <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i>Section 4: Generate the embeddings is already executed! So, skipping the execution of above code.</i></p></div>\"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return None, is_section5_executed\n",
    "\n",
    "\n",
    "sample_embeddings, flag = load_the_emb()\n",
    "\n",
    "sample_embeddings.sort(\"id\") if sample_embeddings is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abf52c-15b3-43fb-95bc-7675f3c4e0cb",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The code below will not run if Section 5 has already been skipped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede1d0a-db0e-4ed4-a021-c8ffc544a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(get_section5_desc_sample())) if flag else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac5492-524e-442e-9696-38d023e02699",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>6. Calculate the VectorDistance using Teradata Vantage in-DB function</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98b12c-276c-41d5-aa7c-abb04cccf5b8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The TD_VectorDistance function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The function computes the distance between the target pair and the reference pair from the same table if you provide only one table as the input.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be3e85-ff43-4b6b-b73c-0ceb0acdbcb6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The VectorDistance function calculates the distance between a target vector and a reference vector. We use the cosine distance metric, which measures the similarity between two vectors. The function can return the maximum of 1 to 100 closest reference vectors to include in the output table for each target vector. In this demo, we want the top 2 closest reference vectors to the target vector.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The VectorDistance function have a parameter <b>distance_measure</b>. You can pass anyone from the below list. Default value is cosine.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li><b>Cosine distance measures</b> the similarity between two vectors by calculating the cosine of the angle between them. It is a good measure of similarity for high-dimensional data, as it is not affected by the magnitude of the vectors.</li>\n",
    "    <li><b>Euclidean distance measures</b> the distance between two points in a Euclidean space. It is the most common distance measure, and it is a good measure of similarity for low-dimensional data.</li>\n",
    "    <li><b>Manhattan distance measures</b> the distance between two points in a Manhattan space. It is similar to Euclidean distance, but it uses the absolute value of the difference between the coordinates instead of the square of the difference.</li>\n",
    "</ol>\n",
    "\n",
    "<center><img src=\"images/distance_measure.png\" alt=\"distance_measure\"  width=600 height=600/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa7006-a178-4f6d-805a-27378ce05aa6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The below function, <code>TD_VECTORDISTANCE</code>, will take the target table, reference table, embedding column names, and number of recommendations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29600ff1-b564-4890-a3ea-c96fd76a63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_distance(target_table, reference_table, emb_column_names, topk):\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "      target_id, \n",
    "      reference_id, \n",
    "      distancetype, \n",
    "      cast(\n",
    "        distance as decimal(36, 8)\n",
    "      ) as distance \n",
    "    FROM \n",
    "      TD_VECTORDISTANCE (\n",
    "        ON {target_table} as TargetTable \n",
    "        ON {reference_table} as ReferenceTable Dimension \n",
    "        USING TargetIDColumn('id') TargetFeatureColumns{tuple(emb_column_names) } \n",
    "        RefIDColumn('id') \n",
    "        RefFeatureColumns{tuple(emb_column_names) } \n",
    "        DistanceMeasure('cosine') \n",
    "        topk({topk})\n",
    "      ) as dt \n",
    "    --order by 3, 1, 2, 4;\n",
    "    \"\"\"\n",
    "\n",
    "    vector_distance_df = DataFrame.from_query(query)\n",
    "\n",
    "    end = timeit.default_timer()\n",
    "    load_time = end - start\n",
    "    print(f\"vector-distance calculation time:\\t\", load_time)\n",
    "    print(\"----- complete -----\")\n",
    "    return vector_distance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6285e-fbe5-4808-accc-ad2a0a460c62",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To view the recommendations, we need to join two tables together. First, we will join the vector distance result table with the document text table. This will give us a table that contains the vector distance scores for each texts, as well as the text embeddings. Then, we will join this table with the query text table. This will give us a final table that contains the distance measures for query texts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5a90c-d8d3-4194-9c83-02ca443b584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_recommendations(\n",
    "    vector_distance_df, product_embeddings_df, search_product_embeddings_df\n",
    "):\n",
    "    product_embeddings_df_selected_columns = (\n",
    "        product_embeddings_df.select([\"id\", \"text\"]).to_pandas().reset_index()\n",
    "    )\n",
    "\n",
    "    # join vector-distance results and products\n",
    "\n",
    "    vec_prod_join_result = pd.merge(\n",
    "        vector_distance_df,\n",
    "        product_embeddings_df_selected_columns,\n",
    "        left_on=\"reference_id\",\n",
    "        right_on=\"id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # join the above joined table with search products\n",
    "\n",
    "    vec_prod_join_result_selected = vec_prod_join_result[\n",
    "        [\"id\", \"text\", \"target_id\", \"distancetype\", \"distance\"]\n",
    "    ]\n",
    "\n",
    "    # join_result_sorted_selected\n",
    "\n",
    "    df_search_products_selected = (\n",
    "        search_product_embeddings_df.select([\"id\", \"text\"]).to_pandas().reset_index()\n",
    "    )\n",
    "\n",
    "    # recommendation results\n",
    "\n",
    "    df_recommendations = pd.merge(\n",
    "        df_search_products_selected,\n",
    "        vec_prod_join_result_selected,\n",
    "        left_on=\"id\",\n",
    "        right_on=\"target_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=[\"_search\", \"_recommended\"],\n",
    "    )\n",
    "\n",
    "    # sort by distance\n",
    "\n",
    "    df_recommendations = df_recommendations.sort_values(\n",
    "        [\"id_search\", \"distance\"], ascending=True\n",
    "    ).reset_index()\n",
    "\n",
    "    return df_recommendations[\n",
    "        [\"id_search\", \"text_search\", \"id_recommended\", \"text_recommended\", \"distance\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c95523",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>7. LLM </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680228a2-3bfa-44f4-8a02-b73fa8bf8e95",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>7.1 Define LLM model</b></p>  \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In OpenAI's language models, the <b>temperature</b> parameter controls the randomness of the generated text. It affects the diversity and creativity of the model's responses. It is always a number between 0 and 1. A temperature of 0 means the responses will be very straightforward, almost deterministic (meaning you almost always get the same response to a given prompt). A temperature of 1 means the responses can vary wildly.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>A higher temperature value, such as 1.0, increases the randomness and diversity of the generated output. This can lead to more varied and surprising responses, but it may also result in less coherence and occasional nonsensical outputs. A higher temperature means that the model might select a word with slightly lower probability, leading to more variation, randomness and creativity. A very high temperature therefore increases the risk of <b>hallucination</b>, meaning that the model starts selecting words that will make no sense or be off topic.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>On the other hand, a lower temperature value, such as 0.2 or below, reduces randomness and makes the model's output more focused and deterministic. The generated text is likely to be more conservative, sticking closely to patterns observed in the training data. A temperature of 0 means roughly that the model will always select the highest probability word.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Choosing an appropriate temperature value depends on the desired output. Higher temperatures can be useful for creative tasks or brainstorming, while lower temperatures are preferred when you need more control over the output, such as when generating specific responses or following a particular style.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbceafb4-793b-4d62-ab22-c9d49ac9a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# OpenAI API\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# call open AI model - api\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32ecf7-575b-460b-ab16-773a7d7b9e8c",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>7.2 Create a Prompt templates and Chain</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><code>PromptTemplate</code> in Language Large Models (LLMs) are pre-defined templates that guide the user in generating prompts for the model. These templates provide a structure for the user to input specific information, such as <b>topic, tone,</b> and <b>style</b>, to help the model generate more accurate and relevant responses. By using prompt templates, users can create more effective prompts and improve the quality of the model's responses. A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40e7af-b23b-456e-be13-e9a3a382d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template_query = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Sorry, I am unable to provide you with an answer. My knowledge is limited to the given document.\" if not sure about the answer.\n",
    "    \n",
    "    Context: Family means the Insured Person including his/her legal spouse and/or a maximum of nine (9) of the Insured Person’s legal Children insured under the same Policy.\n",
    "\n",
    "    Question: What the meaning of Family?\n",
    "    Answer:\n",
    "    \n",
    "    Only use the following context to generate the answer: \\n\n",
    "    Context: {context}\n",
    "    \n",
    "    Write an answer in simple and professional tone for Question: {input}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"context\"], template=prompt_template_query\n",
    ")\n",
    "\n",
    "# defining chain\n",
    "runnable = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ea900-7703-4646-a3dd-fdbf93f85a0f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have created this demo with the goal of enhancing the communication flow between users and Chatbot. </p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You can ask any relevant questions about this insurance document.  \n",
    "    <br>For example:</p>\n",
    "    <ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "        <li><b>Does this policy cover  Loss of or Damage to the Insured’s Articles?</b> </li>\n",
    "        <li><b>What is the reimbursement limit per Baggage?</b></li>  \n",
    "        <li><b>What is the sum insured amount in the case Accidental Death in domestic and international for adult as well as child?</b></li>\n",
    "        <li><b>What documents are required for Rental Car Excess?</b></li>\n",
    "        <li><b>Where can I submit my complaints or feedback?</b></li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe1c6a-191a-4dc7-9b3f-7c282ec94541",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>8. Chat with documents</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Our chatbot is now ready. You can add questions to ask from the insurance document in the chatbot user interface (UI) that opens in the cell below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150c0cb-e9b5-4ec5-a521-19d47819996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "pn.extension(design=\"material\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def get_query_embeddings(query):\n",
    "    start = timeit.default_timer()\n",
    "    df_new_query = pd.DataFrame(data={\"id\": 50001, \"text\": [query]})\n",
    "\n",
    "    df_query_emb = get_embeddings_hf(df_new_query)\n",
    "    end = timeit.default_timer()\n",
    "    load_time = end - start\n",
    "    print(f\"generate the embeddings for {df_query_emb.shape[0]} query:\\t\", load_time)\n",
    "    print(\"----- complete -----\")\n",
    "\n",
    "    # Print the DataFrame.\n",
    "    return df_query_emb\n",
    "\n",
    "\n",
    "def get_context_from_query(user_query):\n",
    "    df_query_emb = get_query_embeddings(user_query)\n",
    "    delete_and_copy_embeddings(table_name=\"query_embeddings\", tdf=df_query_emb, eng=eng)\n",
    "\n",
    "    emb_column_names = DataFrame(in_schema(\"demo_user\", \"query_embeddings\")).columns[2:]\n",
    "\n",
    "    # select top matching\n",
    "    number_of_recommendations = 10\n",
    "\n",
    "    vector_distance_df = calculate_vector_distance(\n",
    "        target_table=\"query_embeddings\",\n",
    "        reference_table=\"text_embeddings\",\n",
    "        emb_column_names=emb_column_names,\n",
    "        topk=number_of_recommendations,\n",
    "    )\n",
    "\n",
    "    text_embeddings_df = DataFrame(in_schema(\"demo_user\", \"text_embeddings\"))\n",
    "    query_embeddings_df = DataFrame(in_schema(\"demo_user\", \"query_embeddings\"))\n",
    "    vector_distance_df = vector_distance_df.to_pandas()\n",
    "\n",
    "    # get topk final recommendations for each searched products\n",
    "    df_recommendations = get_final_recommendations(\n",
    "        vector_distance_df, text_embeddings_df, query_embeddings_df\n",
    "    )\n",
    "\n",
    "    return \",\".join(df_recommendations.text_recommended.unique())\n",
    "\n",
    "\n",
    "# panel callback function\n",
    "def callback(user_query, user, instance):\n",
    "    return runnable.invoke(\n",
    "        {\"input\": user_query, \"context\": get_context_from_query(user_query)}\n",
    "    )\n",
    "\n",
    "\n",
    "pn.chat.ChatInterface(\n",
    "    callback=callback,\n",
    "    show_rerun=False,\n",
    "    show_undo=False,\n",
    "    show_clear=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8490cae-814e-4e83-a615-7e511ac33a33",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'>If the chatbot didn't work when you pressed ENTER, on your first time using this demo on your environment, did you use F5 to reload the site? See instructions at the top of the notebook.\n",
    "If you asked a question and got no response after a few minutes, it is possible that you will need to type 0 0 to restart the kernel and re-run the demo. Questions outside the model seem to confuse the chatbot.</p></div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial;color:#00233C'><i><b>Please be patient:</b> Please note that it may take a few moments to get the response. This is because our platform is still small and we are performing embeddings searching against a database of over 600+ embeddings with 1024 dimensional vector. We are also using complex mathematics to calculate the cosine distance between the products, which can take up to 3 minutes on every question.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867a432-c219-41af-b0da-ebb965cca991",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>When you enter a question in the chat bot above, we will take that question and pass it to a function to generate embeddings. Embeddings are vectors that represent the meaning of a word or phrase. Once we have the embeddings, we will store them in a SQL database.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Next, we will compare the embeddings of the question you entered with the embeddings of all the 600+ embeddings from entire document in our database. We will use a technique called <code>VectorDistance</code> to measure the similarity between the embeddings. In a nutshell, we are comparing the question embeddings against 600+ document embeddings. The closer the embeddings are, the more similar the answer are. We will then select the texts that are the most similar to the question you entered and pass it to the LLM model to generate the answer based on user question.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Finally, it will be display as response into Chatbot.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8b817-c0ff-42b3-a651-c8268ac40942",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#00233c'>9. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8c3d7-3772-419f-b462-3e8ab8cc5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"text_embeddings\", \"query_embeddings\"]\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4cbe-1beb-4c65-ac60-ffd6250668a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5463848-592f-4321-b852-287e133872dd",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2023. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
