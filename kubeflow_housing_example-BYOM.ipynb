{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubeflow is a popular open-source framework for making it easy to develop and deploy machine learning pipelines on Kubernetes. In this tutorial we'll create a three step Kubeflow Pipeline that reads data from Vantage, trains a model and then deploys the model to Vantage.  We will then use that model to create another pipeline that scores new data from a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCP account\n",
    "* Vantage Express installed on GCP with BYOM installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Vantage and loading data\n",
    "\n",
    "1. Setup a Vantage instance - Follow the [Run Vantage Express on GCP](https://quickstarts.teradata.com/vantage.express.gcp.html) how-to to get Vantage setup. Make sure to follow the instructions to open the VM up to the Internet.\n",
    "\n",
    "2. Download sample data - we'll use the Boston Housing dataset which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices?resource=download). Upload the dataset into a Cloud Storage bucket.\n",
    "\n",
    "3. Load training data to Vantage - we'll create a table called `housing`. The example below uses a table in `mldb` database. Make sure you replace `LOCATION('/gs/storage.googleapis.com/vantage_express/boston_housing.csv')` with a string that points to your bucket.\n",
    "\n",
    "    ``` SQL\n",
    "    CREATE FOREIGN TABLE mldb.housing_foreign\n",
    "        USING ( LOCATION('/gs/storage.googleapis.com/vantage_express/boston_housing.csv') );\n",
    "\n",
    "    CREATE MULTISET TABLE\n",
    "        mldb.housing(CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT, MEDV)\n",
    "    AS (\n",
    "        SELECT CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT, MEDV FROM mldb.housing_foreign\n",
    "    ) WITH DATA \n",
    "    NO PRIMARY INDEX;\n",
    "    ```\n",
    "\n",
    "4. Load sample input data - For this tutorial we need a table with some new data that we want to score with our model. Use `teradatasql` to execute the following SQL on your Vantage instance - replace the IP addr with the IP addr of your Vantage instance.\n",
    "\n",
    "    ``` Python\n",
    "    import teradatasql\n",
    "\n",
    "    with teradatasql.connect (host=\"34.71.35.124\", user=\"mldb\", password=\"mldb\") as con:\n",
    "        with con.cursor () as cur:\n",
    "            cur.execute (\"CREATE SET TABLE demo_models (model_id VARCHAR (30), model BLOB) PRIMARY INDEX (model_id);\")\n",
    "            cur.execute (\"CREATE SET TABLE test_housing (ID INTEGER, CRIM FLOAT, ZN FLOAT,INDUS FLOAT,CHAS INTEGER,NOX FLOAT,RM FLOAT, AGE FLOAT,DIS FLOAT, RAD INTEGER,TAX INTEGER,PTRATIO FLOAT,B FLOAT,LSTAT FLOAT) PRIMARY INDEX (CRIM);\")\n",
    "            cur.execute (\"INSERT INTO test_housing (ID, CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT) VALUES (1,.02,0.0,7.07,0,.46,6.4,78.9,4.9,2,242,17.8,396.9,9.14);\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Kubeflow Pipelines on Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your Google Console select `AI Platform Pipelines` and click on `New Instance`, which will redirect you to the `Kubeflow Pipeline` page. On the `Kubeflow Pipeline` page click on `Configure`, which will open up a new page `Deploy Kubernetes Pipelines`.  On the `Deploy Kubernetes Pipelines` page select the region you want to use, you can leave `Network and Subnetwork` as default, and then click on `Create New Cluster` (you don't need to check the `allow access to cloud api` box). This will create a Kubernetes cluster with three VMs in the region you selected.  When the cluster is finished being set up, at the very bottom of the page click on Deploy (for this example you can leave all the other fields as default and not check any of the optional boxes.) You have to wait for the Kubernetes cluster to finish being created - if you click `Deploy` and it fails because the resources are not available, wait and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that your Kubeflow Pipeline is up and running by creating a Kubeflow client.  Go to the `AI Platform Pipelines` and click on the `Settings` and copy the code to connect to the client. Replace the code below with the code copied from the `Pipelines Settings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='https://2475fa1cdaee6bd-dot-us-central1.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the first component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create the components in the pipeline.\n",
    "In this example we will create the following three components:\n",
    "* read_data_from_vantage\n",
    "    * input: ipaddr of the VM hosting Vantage\n",
    "    * output: csv file with the data for training and testing\n",
    "* train_model\n",
    "    * input: csv file with data for training and testing\n",
    "    * output: file containing the model\n",
    "    * output: Metric artifact with model performance\n",
    "* deploy_model\n",
    "    * input: file containing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the Kubeflow Pipeline dsl libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.v2.dsl as dsl\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component reads data from a Vantage warehouse (see above and make sure you have set up Vantage Express in GCP including opening up a firewall to the VM so you can access Vantage from the Internet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The component connects to Vantage using the ipaddr passed as an input parameter, reads the rows from the table Iris.iris in Vantage and then outputs the data to an `Output[Dataset]`, which is essentially a temporary file used to pass data between components (see more about passing data between components [here](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/python-function-components/#passing-artifacts-by-file)).\n",
    "\n",
    "The component uses the `teradatasql` driver. Each component is run in a separate container on Kubernetes so all import statements need to be done within the component. We have created a base image with `teradatasql` already installed. When you pass `base_image='teradata/python-teradatasql'` the component will use that image to create a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='teradata/python-teradatasql')\n",
    "def read_data_from_vantage(\n",
    "    ipaddr: str,\n",
    "    output_file: Output[Dataset]\n",
    "    ):\n",
    "    import teradatasql\n",
    "    \n",
    "    with teradatasql.connect (host=ipaddr, user=\"mldb\", password=\"mldb\") as con:\n",
    "        with con.cursor () as cur:\n",
    "            sFileName = output_file.path\n",
    "            cur.execute (\"{fn teradata_write_csv(\" + sFileName + \")}select * from mldb.housing order by 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train model component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a component to train a model with the training data.\n",
    "\n",
    "The input into this component is the file from the previous component. The output is the file with the trained model using joblib.dump and a file with the test data.\n",
    "\n",
    "The component will use scikit-learn and pandas so we need to pass `packages_to_install=['pandas==1.3.5','scikit-learn']` - this will tell Kubeflow to install the packages when the container is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='teradata/python-sklearn2pmml', packages_to_install=['pandas==1.3.5','scikit-learn','sklearn-pandas==1.5.0'])\n",
    "def train_model(\n",
    "    input_file : Input[Dataset],\n",
    "    output_model: Output[Model],\n",
    "    output_metrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import metrics\n",
    "    from sklearn_pandas import DataFrameMapper\n",
    "    import joblib\n",
    "    from sklearn2pmml.pipeline import PMMLPipeline\n",
    "    from sklearn2pmml import sklearn2pmml\n",
    " \n",
    "    df = pd.read_csv(input_file.path)\n",
    "    \n",
    "    train, test = train_test_split(df, test_size = .33)\n",
    "    train = train.apply(pd.to_numeric, errors='ignore')\n",
    "    test = test.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    features = train.columns.drop('MEDV')\n",
    "    target = 'MEDV'\n",
    "    \n",
    "    pipeline = PMMLPipeline([\n",
    "    (\"mapping\", DataFrameMapper([\n",
    "    (['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], StandardScaler())\n",
    "    ])),\n",
    "    (\"rfc\", RandomForestRegressor(n_estimators = 100, random_state = 0))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(train[features], train[target])\n",
    "    y_pred = pipeline.predict(test[features])\n",
    "\n",
    "    metric_accuracy = metrics.mean_squared_error(y_pred,test[target])\n",
    "    output_metrics.log_metric('accuracy', metric_accuracy)\n",
    "    output_model.metadata['accuracy'] = metric_accuracy\n",
    "        \n",
    "    joblib.dump(pipeline, output_model.path)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create component to deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last component loads the model and tests it on the test data.\n",
    "\n",
    "The `Output[Metrics]` allows us to visualize the model's performance.  We'll use the `output_metrics.log_metric` API to visualize the accuracy of the model in the pipelines run details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='teradata/python-sklearn2pmml')\n",
    "def deploy_model(\n",
    "    input_model : Input[Model],\n",
    "):\n",
    "    \n",
    "    import teradataml as tdml\n",
    "    from teradataml import create_context\n",
    "    import joblib\n",
    "    from sklearn2pmml.pipeline import PMMLPipeline\n",
    "    from sklearn2pmml import sklearn2pmml\n",
    "    \n",
    "    eng = create_context(host = \"104.197.18.6\" , username = 'mldb', password = 'mldb')\n",
    "    \n",
    "    pipeline = joblib.load(input_model.path)\n",
    "    \n",
    "    sklearn2pmml(pipeline, \"test_local.pmml\", with_repr = True)\n",
    "        \n",
    "    model_id = 'housing_rf'\n",
    "    model_file = 'test_local.pmml'\n",
    "    table_name = 'demo_models'\n",
    "    \n",
    "    tdml.configure.byom_install_location = \"mldb\"\n",
    "    \n",
    "    try:\n",
    "        res = tdml.save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "    except Exception as e:\n",
    "        # if our model exists, delete and rewrite\n",
    "        if str(e.args).find('TDML_2200') >= 1:\n",
    "            res = tdml.delete_byom(model_id = model_id, table_name = table_name)\n",
    "            res = tdml.save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create component to use the deployed model to score new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next component uses the `teradataml` driver to retrieve the saved model and score the rows in a table with new data.  The component uses the `PMMLPredict` function to score the newdata and returns a `teradataml` byom output that can be converted to a pandas DataFrame with `result.result.to_pandas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='teradata/python-sklearn2pmml', packages_to_install=['pandas==1.3.5','scikit-learn'])\n",
    "def test_model(\n",
    "):\n",
    "    import teradataml as tdml\n",
    "    from teradataml import create_context\n",
    "    from sklearn2pmml.pipeline import PMMLPipeline\n",
    "    from sklearn2pmml import sklearn2pmml\n",
    "    \n",
    "    eng = create_context(host = \"104.197.18.6\" , username = 'mldb', password = 'mldb')\n",
    "    \n",
    "    #indicate the database that BYOM is using\n",
    "    tdml.configure.byom_install_location = \"mldb\"\n",
    "    \n",
    "    tdf_test = tdml.DataFrame('test_housing')\n",
    "    \n",
    "    modeldata = tdml.retrieve_byom(\"housing_rf\", table_name=\"demo_models\")\n",
    "    \n",
    "    result = tdml.PMMLPredict(\n",
    "            modeldata = modeldata,\n",
    "            newdata = tdf_test,\n",
    "            accumulate = ['ID']\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function for executing the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create a function to execute each component in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='test-vantage-pipeline',\n",
    "   description='An example test pipeline that connects to Vantage.',\n",
    ")\n",
    "def run_vantage_pipeline(\n",
    "   ipaddr: str\n",
    "):\n",
    "    data_file = read_data_from_vantage(ipaddr).output\n",
    "    test_model_data = train_model(data_file)\n",
    "    _deploy_model_op = deploy_model(test_model_data.outputs['output_model'])\n",
    "    test_model().after(_deploy_model_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the Kubeflow client to execute the pipeline.  Replace the `ipaddr` argument with the `ipaddress` of the VM hosting Vantage.\n",
    "When the Experiment starts two links should appear: `Experiment details` and `Run details`. Click on `Run details` to follow the progress of your pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py:79: UserWarning: V2_COMPATIBLE execution mode is at Beta quality. Some pipeline features may not work as expected.\n",
      "  warnings.warn('V2_COMPATIBLE execution mode is at Beta quality.'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://2475fa1cdaee6bd-dot-us-central1.pipelines.googleusercontent.com/#/experiments/details/f16d0405-f3e6-4764-ab9a-a1155f795f56\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://2475fa1cdaee6bd-dot-us-central1.pipelines.googleusercontent.com/#/runs/details/1c4cc82c-44fe-43d7-a2eb-23555a4dd1fb\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=1c4cc82c-44fe-43d7-a2eb-23555a4dd1fb)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments = {'ipaddr' : \"104.197.18.6\" }\n",
    "client.create_run_from_pipeline_func(\n",
    "    run_vantage_pipeline,\n",
    "    arguments=arguments,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the pipeline has completed running (each component in the graph should have a green check mark) .  You can click on each component to see more details. If you click on the train_model component and then click on Visualizations in the side window you will see the accuracy of the model (you can learn more about other metrics you can pass and visulation using the Metrics artifict [here](https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#introduction).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new pipeline to score new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component in the pipeline uses the teradatasql driver to execute a SQL query that retrieves the model from the `demo_model` table and scores the rows in the `test_housing` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='teradata/python-sklearn2pmml', packages_to_install=['pandas==1.3.5','scikit-learn'])\n",
    "def score_new_data(\n",
    "    model_name: str,\n",
    "    model_table: str,\n",
    "    data_table: str,\n",
    "    prediction_table: str  \n",
    "):\n",
    "    import teradataml as tdml\n",
    "    from teradataml import create_context\n",
    "    from sklearn2pmml.pipeline import PMMLPipeline\n",
    "    from sklearn2pmml import sklearn2pmml\n",
    "    import teradatasql\n",
    "        \n",
    "    tdml.configure.byom_install_location = \"mldb\"\n",
    "    \n",
    "    with teradatasql.connect (host=\"34.71.35.124\", user=\"mldb\", password=\"mldb\") as con:\n",
    "        with con.cursor () as cur:\n",
    "            cur.execute (\"CREATE TABLE {fn \" + prediction_table + \"} AS (SELECT * FROM mldb.PMMLPredict ( ON test_housing ON (SELECT * FROM demo_models where model_id='housing_rf') DIMENSION USING Accumulate ('ID')) AS td ) WITH DATA;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_new_data_score` pipeline takes as parameters:\n",
    "- `model_name`: ID of the model\n",
    "- `model_table`: the name of the table storing the model\n",
    "- `data_table`: the name of the table with new data to score\n",
    "- `prediction_table`: the name of the table to store the scoring results\n",
    "When the pipeline is executed the dashboard will provide fields to enter the values you want to use (the values have been hard-coded above with the appropriate values for this tutorial to avoid errors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='new-data-pipeline',\n",
    "   description='An example of a component that scores new data with a saved model.',\n",
    ")\n",
    "def run_new_data_score(\n",
    "    model_name: str,\n",
    "    model_table: str,\n",
    "    data_table: str,\n",
    "    prediction_table: str\n",
    "):\n",
    "    score_new_data(model_name,model_table,data_table,prediction_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the pipeline run the following.  The output will save the package in a yaml file.  Download the yaml file to your local computer.\n",
    "\n",
    "To run the pipeline in Kubeflow, go to the Pipelines and click on `Upload pipeline` in the top right hand corner. Fill out the fields and upload the yaml file.  Go to `Runs`, click on `Create run` and choose the pipeline you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
    "    pipeline_func=run_new_data_score,\n",
    "    package_path='score_new_data_pipeline_sql.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipelines in Vertex AI pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run your Kubeflow pipelines in `Google Vertex AI Pipleline`.  \n",
    "\n",
    "Compile the pipeline with the following code.  The output will save the package in a json file.\n",
    "\n",
    "Go to `Vertex AI Piplelines` and click on `Create run`. Choose the json file, change the pipeline name and run name if you like, and click continue. You'll need a Cloud Storage bucket - enter the path to a Cloud Storage bucket and the pipeline parameters and click on `Submit`.\n",
    "\n",
    "One of the nice benefits of running the pipeline in Vertex AI is that it is serveless - so Vertex AI will take care of creating and deleteing a cluster to run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=run_new_data_score,\n",
    "    package_path='score_new_data_pipeline_sql.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compile the `run_vantage_pipline` above with the three components to load the data, train and deploy, and run that in Vertex AI.  Below is a slightly modified pipeline with the same components we created at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='run-vantage-pipeline',\n",
    "   description='An example test pipeline that connects to Vantage.',\n",
    ")\n",
    "def run_vantage_pipeline_vertex(\n",
    "   ipaddr: str\n",
    "):\n",
    "    data_file = read_data_from_vantage(ipaddr).output\n",
    "    test_model_data = train_model(data_file)\n",
    "    deploy_model(test_model_data.outputs['output_model'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the pipeline with the following code and follow the same instrucations for creating a run in Vertex AI Piplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=run_vantage_pipeline_vertex,\n",
    "    package_path='train_housing_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop incurring charges you need to delete the Vantage Express VM and the Kubeflow Pipeline. Go back to the list of Kubeflow Pipeline instances, select the instance you want to delete and then click on Delete (make sure to check the box to delete the Kubernetes cluster as well, this will delete the cluster's three compute engines.) Delete the Vantage Express VM by going to the list of Compute Engine instances and selecting the instance with Vantage Express and then click on Delete."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
